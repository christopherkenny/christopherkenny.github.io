[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Creating Quarto Journal Article Templates\n\n\n\n\n\nA long walk through creating Quarto templates for journal articles.\n\n\n\n\n\n\nJul 1, 2023\n\n\nChristopher T. Kenny\n\n\n\n\n\n\n  \n\n\n\n\nMaking Maps with feltr\n\n\n\n\n\nA brief introduction to the feltr package.\n\n\n\n\n\n\nJul 7, 2023\n\n\nChristopher T. Kenny\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Packages on CRAN:\n\n\n\n\n\n  \n    \n    \n  \n  \n  \n    \nredist: Simulation Methods for Legislative Redistricting (with Cory McCartan, Ben Fifield, and Kosuke Imai)\n    \nredistmetrics: Redistricting metrics (with Cory McCartan, Ben Fifield, and Kosuke Imai)\n    \ngeomander: Geographic Tools for Studying Gerrymandering\n    \nPL94171: Tabulate P.L. 94-171 Redistricting Data Summary Files (with Cory McCartan)\n    \ncensable: Making Census Data More Usable\n    \ntinytiger: Lightweight Interface to TIGER/Line Shapefiles (with Cory McCartan)\n    \ndots: Dot Density Maps\n    \ncvap: Citizen Voting Age Population\n    \nppmf: Read Census Privacy Protected Microdata Files\n    \ndivseg: Compute Diversity and Segregation Indices\n    \nname: Tools for Working with Names\n    \njot: Jot Down Notes for Later\n    \nggredist: Scales, Palettes, and Extensions of ggplot2 for Redistricting (with Cory McCartan)\n    \ncongress: Access the Congress.gov API\n    \nfeltr: Access the Felt API\n    \ncrayons: Color Palettes from Crayon Boxes\n    \napportion: Apportion Seats\n    \ngptzeror: Identify Text Written by Large Language Models using GPTZero\n  \n  \n  \n\n\n\n\n\n\nPackages on GitHub:\n\n\n\n\n\n  \n    \n    \n  \n  \n  \n    \nei: Ecological Inference (with Shusei Eshima, Gary King, and Molly Roberts)\n    \nalarmdata: Download, Merge, and Process Redistricting Data (with Cory McCartan, Tyler Simko, Michael Zhao, and Kosuke Imai)\n    \nplanscorer: Score Redistricting Plans with PlanScore\n    \nredistverse: Easily Install and Load Redistricting Software"
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html",
    "title": "Creating Quarto Journal Article Templates",
    "section": "",
    "text": "This post is largely written for my future self. If you’re not me and you’ve ended up here, I do hope that this is helpful for you, but it may fall below or above your current experience. I’ll include informative links as I think of them to help with the latter. For the former, this post may be too specific to my process to be helpful.\n\n\nAs with all blog posts, I’m assuming you both know everything and nothing. You have every clue how Quarto, Markdown, LaTeX, pandoc, and Lua work and simultaneously have never used anything but Microsoft Word.\nMore realistically, you need some experience with one of Quarto, RMarkdown, or Markdown and definitely some experience with LaTeX.\nYou needn’t be an expert on the first part, but if you’re new to LaTeX, take some time to write a few documents before trying to build templates that rely on it for pdf-output.\n\n\n\nI’ve made a handful of templates for Quarto and RMarkdown. A few of them are public, but many of them are hacky one-off solutions to some problem. I’m focused here on the more-focused public template, where you want it to work in general for a journal’s set of problems.\nPerhaps most importantly, I’ve got about eight years of self-taught LaTeX experience. That’s good for me and bad for you (or maybe good for you and bad for me). My solutions to problems often (1) produce correct output and (2) are sub-optimal. That’s not the best, but if you care about (1) more, then, hey, we’ve got some work to do.\nFor full context: This post was mostly written while I was making a PNAS template for Quarto. Previously, I used the rticles::pnas_template() but (at the time of writing) I was trying to get a better sense of how Quarto built things. So, I have a sense of what it should look like, but no guarantee of understanding how the details work."
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#background",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#background",
    "title": "Creating Quarto Journal Article Templates",
    "section": "",
    "text": "This post is largely written for my future self. If you’re not me and you’ve ended up here, I do hope that this is helpful for you, but it may fall below or above your current experience. I’ll include informative links as I think of them to help with the latter. For the former, this post may be too specific to my process to be helpful.\n\n\nAs with all blog posts, I’m assuming you both know everything and nothing. You have every clue how Quarto, Markdown, LaTeX, pandoc, and Lua work and simultaneously have never used anything but Microsoft Word.\nMore realistically, you need some experience with one of Quarto, RMarkdown, or Markdown and definitely some experience with LaTeX.\nYou needn’t be an expert on the first part, but if you’re new to LaTeX, take some time to write a few documents before trying to build templates that rely on it for pdf-output.\n\n\n\nI’ve made a handful of templates for Quarto and RMarkdown. A few of them are public, but many of them are hacky one-off solutions to some problem. I’m focused here on the more-focused public template, where you want it to work in general for a journal’s set of problems.\nPerhaps most importantly, I’ve got about eight years of self-taught LaTeX experience. That’s good for me and bad for you (or maybe good for you and bad for me). My solutions to problems often (1) produce correct output and (2) are sub-optimal. That’s not the best, but if you care about (1) more, then, hey, we’ve got some work to do.\nFor full context: This post was mostly written while I was making a PNAS template for Quarto. Previously, I used the rticles::pnas_template() but (at the time of writing) I was trying to get a better sense of how Quarto built things. So, I have a sense of what it should look like, but no guarantee of understanding how the details work."
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#making-the-template",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#making-the-template",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Making the Template",
    "text": "Making the Template\nFirst things first, we need to initialize the template. We can do that in the terminal with:\nquarto create extension journal\nIt’ll prompt you to give it a name. Names tend to be short and normally match a common journal abbrevation. Quarto has a list of templates, as does Awesome Quarto. Some names I’ve used:\n\n\n\njournal\nname\n\n\n\n\nAPSR\napsr\n\n\nPNAS\npnas\n\n\nPA, PSRM, BJPS, …\ncambridge-medium\n\n\nScientific Data\nscientific-data\n\n\n\nAs you can see, some are obvious. The Cambridge medium one tripped me up a bit, as it was intended for at least five journals. As such, I just named it for the template and added text to make it clear what journals it was for.\nNow, once you have a name, it creates a new R project named for the name you provided. You can open that and give the structure a look.\n\n\n\n\n\n\nNote\n\n\n\nFor the rest of this post, I’ll use building a PNAS template as the example. pnas can be replaced with the name you chose above.\n\n\nOn creation, the file structure should look something like this:\n.\n├── bibliography.bib\n├── pnas.Rproj\n├── README.md\n├── template.qmd\n└── _extensions\n    └── pnas\n        ├── header.tex\n        ├── pnas.lua\n        ├── styles.css\n        └── _extension.yml\nFor a journal template, we’re going to have to augment the _extensions/pnas folder. For now some quick bits of information on included files:\n\nREADME.md is the general readme for the whole template. It’ll hold installation instructions, option documentation, and anything else you need users to know.\ntemplate.qmds is where our example template goes.\nbibliography.bib is a placeholder bibtex file.\n_extensions/pnas/_extension.yml will hold the metadata for the template. This tells Quarto which files to use and such.\n\n_extensions/pnas/header.tex is included in the default template and will be inserted in the header of the LaTeX template.\n\n_extensions/pnas/styles.css can probably be deleted at this point, unless you want to make a website version of the template, but you probably aren’t if you’re making a journal template.\n_extensions/pnas/pnas.lua is the default Lua filter. Lua is powerful, but only necessary for more complicated features.\n\n\nThe README\nThe first thing I edit is the README. It can’t be finished yet, but I find it helpful to organize right away.\nUsing RStudio’s find+replace, I change the github organization placeholder to my GitHub account name (christopherkenny). Then, using the same tool, I replace the file name and title with template name from before. (If you now feeling anxious about the name, this is the right time to fix it before doing anything.) Then, I remove the (now complete) TODOs for those.\nIn the “Example” section, I then start to add more info. First, I add some template code to include a screenshot.\n&lt;!-- pdftools::pdf_convert('template.pdf',pages = 1) \n![[template.qmd](template.qmd)](template_1.png) --&gt;\nOnce the template is finished, I’ll run the first line in R and uncomment the second. Including a screenshot is inspired by Cory McCartan’s pre-print template. I think it’s a nice touch.\nThe options section is helpful for the end user, but may not be super clear right now. I think it is best to hold off on that and figure it out later.\nFor now, the last thing I’ll do is add a bit of information on licensing. Odds are good that I’m working off of some official or officially endorsed template. Even if it isn’t official, you want to give credit to the LaTeX wizardry going on behind the scenes. My license section typically takes the form, using {glue} syntax for placeholders:\n## License\nThis modifies the {Owner} {What} Template, available at &lt;{some_link}&gt;.\nThe original template is licensed under the [{license_name}]({license_link}).\n{any_other_required_notes_about_modifications}"
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#adding-latex-files-into-the-template",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#adding-latex-files-into-the-template",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Adding LaTeX Files into the Template",
    "text": "Adding LaTeX Files into the Template\nBefore you dive into building the Quarto part of the template, I’ll next download any relevant files for the LaTeX part of the template. For the PNAS template, I downloaded the source files from Overleaf.\nFrom the template, I extract all of the LaTeX-related files (.bst, .sty, .cls, and in this case .ldf files). Those files go into the _extensions/pnas folder. Any similar files that we want to be available to the template itself should go there.\nAny files that are necessary for demonstrating the template can go into a higher-level folder. For example, in the PNAS template, they use a frog as the placeholder image. I make a folder called figs/ that can hold such things.\nAt this point, my file structure looks like:\n.\n├── bibliography.bib\n├── figs\n│   └── frog.pdf\n├── pnas.Rproj\n├── README.md\n├── template.qmd\n└── _extensions\n    └── pnas\n        ├── header.tex\n        ├── jabbrv-ltwa-all.ldf\n        ├── jabbrv-ltwa-en.ldf\n        ├── jabbrv.sty\n        ├── pnas-new.bst\n        ├── pnas-new.cls\n        ├── pnas.lua\n        ├── pnasresearcharticle.sty\n        ├── styles.css\n        └── _extension.yml\nPutting the files here only does so much. We have to let the template know that we have them. This all happens in _extensions/pnas/_extension.yml\nThere are a pair of lines in the file telling it what to use for PDFs.\n    pdf:\n      include-in-header: header.tex\nWe’ll add some new instructions for the format-resources listing out the new files that we have.\n    pdf:\n      include-in-header: header.tex\n      format-resources:\n        - pnas-new.cls\n        - pnas-new.bst\n        - pnasresearcharticle.sty\n        - jabbrv.sty\n        - jabbrv-ltwa-all.ldf\n        - jabbrv-ltwa-en.ldf\n\nEditing template.qmd (Part 1: The YAML)\nOnce we’ve got all the files in place, it’s time to make some edits to template.qmd so that we can render it and see how we’re doing.\nThe first big part is setting up the Quarto YAML. This is where all of the information that we will later use in the template will go.\nThe first few lines of the document are standard, but we’ll make two changes to the below:\ntitle: Pnas Template\nformat:\n  pnas-pdf:\n    keep-tex: true  \n  pnas-html: default\nFirst, I’ll give it a more fun title, here “Quarto Template for PNAS Submissions”. Second, I’m focused on making the pnas-pdf class, so I’ll remove the pnas-html: default line from line 6 (or so) of the YAML. For journals that need PDFs for submission, the extra type doesn’t seem worth the effort.\n\nAuthors and Affiliations YAML\nNot all templates use all possible information about authors. But, there is a pretty extensive set of possible pieces of information.\nWe need to include every relevant piece of information that the journal template will use. If we don’t include everything, there might be weird empty spaces. If we over include, that’s okay, but some things will get ignored.\nUnlike the rticles templates that RMarkdown used to use, Quarto has tried to build an exhaustive schema for people’s names and their affiliations. The schema are detailed extensively at https://quarto.org/docs/journals/authors.html.\nIf we peek at some recent PNAS articles, we can see what author and affiliation information are used.\nAuthors look something like\n\nAuthor One\\(^{a, c, 1}\\), Author Two\\(^{b, 1, 2}\\), and Author Three^\\(a\\)\n\nwhich is kind of a mess.\n1 indicates that the authors contributed equally. 2 indicates the corresponding author. a,b,c are all affiliations.\nAffiliations themselves are pretty standard here. They should get printed to look like\nUniversity, Department, City, State ZIP\nWith that much information, we can assemble the YAML for an author. To make sure we can see the difference in these, I’ll do set up the affiliations as:\n\na: Harvard University, Department of Government, Cambridge, MA 02138\nb: Yale University, Department of Political Science, New Haven, CT 06511\nc: Harvard University, Department of Statistics, Cambridge, MA 02138\n\nauthor:\n  - name: Author One\n    affiliations:\n      - name: Harvard University\n        id: a\n        department: Department of Government\n        city: Cambridge\n        state: MA\n        postal-code: 02138\n      - name: Harvard University\n        id: c\n        department: Department of Statistics\n        city: Cambridge\n        state: MA\n        postal-code: 02138\n    attributes:\n      equal-contributor: true\nNote here that Author One and Author Two contributed equally and Author Two is the corresponding author.\nWith that, we can build out the other authors.\n  - name: Author Two\n    affiliations:\n      - name: Yale University\n        id: b\n        department: Department of Political Science\n        city: New Haven\n        state: CT\n        postal-code: 06511\n    attributes:\n      equal-contributor: true\n      corresponding: true\n  - name: Author Three\n    affiliations:\n      - ref: a\nBuilding out Author Two’s affiliation is very similar, we just add a corresponding: true under attributes along with their information. For Author Three, we can do a cool thing and reference affiliation a, since it’s the same.\nNow at this point, I know I’ll also need the email for the corresponding author, so I’ll add an email: corresponding@email.com line to the Author Two chunk.\n\n\nCustom YAML Components\nThe rest of the YAML varies immensely by journal. To figure out what we need, we have to look back at the LaTeX template. We want to identify anything where we either need to tell the template about options to use or replace filler text. To that point, I’ve pulled out each thing that I see as something we need to be controllable in the YAML.\n\\templatetype{pnasresearcharticle} % Choose template\n% {pnasresearcharticle} = Template for a two-column research article\n% {pnasmathematics} %= Template for a one-column mathematics article\n% {pnasinvited} %= Template for a PNAS invited submission\nIt looks like we need a template_type argument to the YAML. Later, we can then list the options in the README.\n\npnasresearcharticle: Template for a two-column research article\npnasmathematics: Template for a one-column mathematics article\npnasinvited: Template for a PNAS invited submission\n\nWe should be aware though that these actually have different official templates. This gives an important choice:\n\ntry to support it\nlet it be set but make it clear that the template is optimized for pnasresearcharticle types\ndon’t make it settable and just always use pnasresearcharticle\n\nI tend to go for (2) since it might work really easily without changing anything. Aiming for (1) is commendable, but may open a new can of worms. Option (3) is really safe and might actually be the best option, but if it at least mostly works, then someone else trying to do the other type now has to start from scratch when they didn’t have to. Part of how I’ll approach this is to make it the default in LaTeX if nothing is specified.\n\\leadauthor{Lead author last name}\nThis is a pretty common request, as it’s used in the header of pages. To be consistent with other journal types, such as apsr, I’ll call this field runningauthor. It’s always better to be consistent, since it makes switching easier, but if I didn’t know about the other name, I probably would have called it lead_author.\n\\significancestatement{Authors must submit a 120-word maximum statement about the significance of their research paper written at a level understandable to an undergraduate educated scientist outside their field of speciality. The primary goal of the significance statement is to explain the relevance of the work in broad context to a broad readership. The significance statement appears in the paper itself and is required for all research papers.}\nFor this, we can just give a new argument called significance, since it is just a bunch of text.\n\\authorcontributions{Please provide details of author contributions here.}\nFor this, we can also make a new argument called author_contributions. It’s a bunch of text and readable is best.\n\\authordeclaration{Please declare any competing interests here.}\nI tend to call this conflict_of_interest, as I’ve seen it that way in the old rticles days.\n\\keywords{Keyword 1 $|$ Keyword 2 $|$ Keyword 3 $|$ ...}\nKeywords come up in many templates and are included by default in the automatically generated YAML as keywords.\n\\doi{\\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}\nThis one looked like something that we might want to change, but is probably best to leave for now. With how publications and submissions work for PNAS, you won’t know the doi until after you’re done using the template.\n\\firstpage{12}\n% Use \\firstpage to indicate which paragraph and line will start the second page and subsequent formatting. In this example, there are a total of 11 paragraphs on the first page, counting the first level heading as a paragraph. The value {12} represents the number of the paragraph starting the second page. If a paragraph runs over onto the second page, include a bracket with the paragraph line number starting the second page, followed by the paragraph number in curly brackets, e.g. \"\\firstpage[4]{11}\".\nNow this one presents an interesting question. We could include a first_page argument. Though, the formatting is kinda tricky. We would need to support both \\firstpage{12} and \\firstpage[4]{11}. It seems to me that this would be a good thing to leave out of the YAML but use within the template.qmd, so that it’s obvious it should be used but doesn’t require multi-option handling. That is my preference and I can’t say for sure if this is best practice.\n\\acknow{Please include your acknowledgments here, set in a single paragraph. Please do not include any acknowledgments in the Supporting Information, or anywhere else in the manuscript.}\nWe can give this a name like acknowledgements, again looking to other templates to see what they use as names, since this is a pretty common thing to have.\nOkay now that gives us pretty set of YAML option to add. They’ll look like:\nrunningauthor: \"One, Two, and Three\"\nsignificance: |\n  Authors must submit a 120-word maximum statement about the significance of their research paper written at a level understandable to an undergraduate educated scientist outside their field of speciality. The primary goal of the significance statement is to explain the relevance of the work in broad context to a broad readership. The significance statement appears in the paper itself and is required for all research papers.\nauthor_contributions: \"Please provide details of author contributions here.\"\nconflict_of_interest: \"Please declare any competing interests here.\"\nkeywords: [template, demo]\nacknowledgements: | \n  Please include your acknowledgments here, set in a single paragraph. Please do not include any acknowledgments in the Supporting Information, or anywhere else in the manuscript.\nAs you can see, where applicable, I’m using the instructions as the placeholder text where applicable. This has the benefit of keeping the end-user instructions near the end user without them having to deep dive into the original LaTeX template.\nThe bad news: that was the easy part. The good news: the next part walks you through the hard part.\n\n\n\n\n\n\nUse Git\n\n\n\nIf you aren’t already using Git at this point, now is a good time to run usethis::use_git(), initialize a repo, and commit what you’ve done. This is a good save point so that if you start playing with things for the next part, you can easily remove them if you break anything.\n\n\n\n\n\nUsing the YAML in LaTeX\nAdding the arguments to the YAML doesn’t automatically change anything about the output. It makes variables available to Pandoc’s processing as the name of the YAML set. So, for example, if our YAML looked like:\ntitle: Quarto Template for PNAS Submissions\nthen a variable called title would be available for us to use. We can extract its value with $title$.\nPossibly the most common is to check if a value is set and then use it if it is. For something like title, where we want to pass it to the LaTeX function \\title{}, we can do this with the following pattern.\n$if(title)$\n\\title{$title$}\n$endif$\nThe first and third lines here are how if statements are done. So, if the title is set then in your YAML, then the outputted .tex file will say:\n\\title{Quarto Template for PNAS Submissions}\nFor more details on how these types of variables work, take a look at Template syntax section of the Pandoc manual.\nMost of the templating from here is going to be finding the correct place to evaluate a variable. To find those places, we have to introduce the idea of partials, which are short files that get injected into the bigger template.\n\nLaTeX Partials\nPartials represent little snippets of LaTeX that get joined in a specific order. Quarto is built off of a series of default partials. If you’re rendering a regular document without any of this templating, it passing through those. The defaults are very good: they cover lots of cases and do them in a smart, efficient way. The ideal is that you only have to change a small number of partials. Journals have all sorts of format choices that you want to match, so we will have to replace some.\nAs assumed before, you know some LaTeX. A familiar, stylized LaTeX document then looks something like:\n% first line is the *doc-class*\n\\documentclass{article}\n\n% then we have the header\n% like where we load packages with \\usepackage{}\n\n% This next group of lines are the *title*\n\\title{Some Latex Document}\n\\author{Christopher Kenny}\n\\date{July 2023}\n\n\\begin{document}\n\n% then we have *before-body*\n\\maketitle\n\nThe body of the document\n\n% then we have *after-body*\n\n% then we have the *before-bib*\n\nThe references\n% then we have *biblio* which makes the bibliography\n\n\\end{document}\nEach of the things in *s is a partial and the general place where it goes. There are more partials, like toc, a table of contents partial. There’s also a partial for pandoc, for some things that Pandoc needs for rendering from LaTeX. Odds are good that you don’t have to touch the Pandoc one!\nA full description of Quarto partials is available in the Quarto documentation. This lists out all of the partials and a short description of what they do. The source files for the partials are on GitHub.\nAny partials that we need will live in a partials folder, below where the _extension.yml folder lives. Each partial will be a .tex file. So, they will all be something like: _extensions/pnas/partials/*.tex. Each file that we add to this folder has to be listed in _extension.yml, which I’ll show below.\nWith the general idea of partials down, we can go in order from the top down and see what we need.\n\ndoc-class.tex\nOkay, so first things first is the document class. In the template, it looks like this:\n\\documentclass[9pt,twocolumn,twoside]{pnas-new}\nArguably, we don’t need to change this one out. This whole partial will evaluate to one line. The default looks like this:\n\\documentclass[\n$if(fontsize)$\n  $fontsize$,\n$endif$\n$if(papersize)$\n  $papersize$paper,\n$endif$\n$if(beamer)$\n  ignorenonframetext,\n$if(handout)$\n  handout,\n$endif$\n$if(aspectratio)$\n  aspectratio=$aspectratio$,\n$endif$\n$endif$\n$for(classoption)$\n  $classoption$$sep$,\n$endfor$\n]{$documentclass$}\nI say that we don’t need to necessarily change this one because we could pass the following to the YAML.\nfontsize: \"9pt\"\nclassoption:\n - twocolumn\n - twoside\ndocumentclass: \"pnas-new\"\nThis would do the following:\n\n$if(fontsize)$ is true, so return the fontsize (9pt) and include the ,.\n$if(papersize)$ isn’t specified, so it’s false and nothing happens.\n$if(beamer)$ isn’t specified, so it’s false and nothing happens.\n$if(handout)$ isn’t specified, so it’s false and nothing happens.\n$if(aspectratio)$ isn’t specified, so it’s false and nothing happens.\n\nThen we get a for loop. This works like the if syntax from before. It takes each element of classoption (a length two vector with twocolumn and twoside), returns them followed by the seperator. The $sep$, $endfor$ syntax just says “hey this is the separator ,, a comma followed by a space.”\nFinally, document class would be pnas-new. So that completes it as\n\\documentclass[9pt,twocolumn,twoside]{pnas-new}\nThere are at least three advantages to implementing this simple partial as a custom partial. First, we can hard code the documentclass to be just the class we want to support. This avoids weird errors related to incorrect class specifications by downstream users. Second, if we decide that something is important enough to elevate to an additional YAML argument, we can then do so and add it with a little if syntax. For example, the APSR has a special nonblind argument that is really important for the submission. Knowing that, I could then make it a YAML option here. Third, we can remove some of the arguments that aren’t relevant to our template, like aspectratio.\nWith that in mind, I’ll first trim the options:\n\\documentclass[\n$if(fontsize)$\n  $fontsize$,\n$endif$\n$for(classoption)$\n  $classoption$$sep$,\n$endfor$\n]{$documentclass$}\nThen I’ll hard code the documentclass\n\\documentclass[\n$if(fontsize)$\n  $fontsize$,\n$endif$\n$for(classoption)$\n  $classoption$$sep$,\n$endfor$\n]{pnas-new}\nAt this point, I’ll reopen the template.qmd file and add to the YAML the relevant options for what we were just looking at.\nfontsize: \"9pt\"\nclassoption:\n - twocolumn\n - twoside\nThat’s it for that file. Now we just have to let the _extension.yml file know we have it, like we did for the class files above.\nLook for the lines we edited before and below it we’ll add a template-partials section to indicate that we have this file.\n    pdf:\n      include-in-header: header.tex\n      format-resources:\n        - pnas-new.cls\n        - pnas-new.bst\n        - pnasresearcharticle.sty\n        - jabbrv.sty\n        - jabbrv-ltwa-all.ldf\n        - jabbrv-ltwa-en.ldf\n      template-partials:\n        - \"partials/doc-class.tex\"\n\n\ntitle.tex\nNow, the title block definitely needs work for this example and probably for the majority of cases. The title partial will tell the document how to create the title, identify the authors, and identify anything else important that needs to be specified at the start of the document. For the PNAS template, we need to tell it how to:\n\nSpecify the \\templatetype{}\nCall \\title{}\nIdentify the authors (this one is the hardest!)\nCall \\leadauthor{}\nCall \\significancestatement{}\nCall \\authorcontributions{}\nCall \\authordeclaration{}\nIdentify equal authors\nIdentify corresponding authors\nSpecify the keywords\n\nFor the template type, we want to use an if-else statement. This lets us specify the default option for the case where no type was set. So, since there is an else, we can put it all directly inside the call to \\templatetype{}. This will look something like:\n\\templatetype{\n$if(template_type)$\n$template_type$\n$else$\npnasresearcharticle\n$endif$}\nwhere $else$ is how we specify the else part of the if-else.\nThe next part of this is going to be familiar. If there’s a title, we want to write the title.\n$if(title)$\n\\title{$title$}\n$endif$\nIf we hold onto the author pieces for a second, we can repeat that pattern for each of \\leadauthor{},\\significancestatement{},\\authorcontributions{}, and \\authordeclaration{}.\n$if(runningauthor)$\n\\leadauthor{$runningauthor$}\n$endif$\n\n$if(significance)$\n\\significancestatement{$significance$}\n$endif$\n\n$if(author_contributions)$\n\\authorcontributions{$author_contributions$}\n$endif$\n\n$if(conflict_of_interest)$\n\\authordeclaration{$conflict_of_interest$}\n$endif$\nNow back to the authors and affiliations. This is often the hardest part of making a Quarto template for journals. As a reminder, we want the output to look like:\n\\author[a,c,1]{Author One}\n\\author[b,1,2]{Author Two}\n\\author[a]{Author Three}\n\n\\affil[a]{Affiliation One}\n\\affil[b]{Affiliation Two}\n\\affil[c]{Affiliation Three}\nLet’s break apart what we need for Author One.\n\nA name\nAffiliation with a\n\nNeed to make the \\affil for a\n\nAffiliation with b\n\nNeed to make that \\affil for b too\n\na 1 to indicate joint first authorship\n\nWe’ll fill that part out in a minute.\n\n\nIf we look at the normalized schema for author first, we can figure out how to access it. The relevant part of the YAML looks like:\nauthor:\n  - id: string\n    number: number\n    name:\n      given: string\n      family: string\n      literal: string\nHow do we get the value? We can build it like so\n\nit’s part of the author: $author.\n\nit’s part of the name: name.\n\nwe want the full thing and that’s it literal$\n\n\n\nSo going down the little tree, we get that it is \\(author.name.literal\\). If we needed something like the first name, it would be \\(author.name.given\\).\nSo, we can throw that into:\n\\author[...]{$author.name.literal$}\nleaving the ... for this next step.\nWe need to extract the IDs for the affiliations for each person, so we’ll need to iterate. To iterate over the affiliations of each author, we can use the by-author iterator. This will let us loop over each author and grab the relevant values.\nIf we just needed the authors, then this would look like:\n$for(by-author)$\n\\author{$by-author.name.literal$}\n$endfor$\nBut, we need to fill in the [...] from above. To start, let’s do the affiliation ids. If they have affiliations, we want to loop over them.\n$for(by-author)$\n\\author[\n$if(by-author.affiliations)$\n$for(by-author.affiliations)$\n$it.id$\n$sep$,\n$endfor$\n$endif$\n]{$by-author.name.literal$}\n$endfor$\nAs you can see, I’ve introduced the special keyword it. It’s essentially a placeholder for the current iterator. Here, that’s each entry of by-author.affiliations and we are accessing the .id from them. We’re separating them with ,s so that they evaluate nicely.\nThe spacing that this outputs is a little funny, so we can collapse it to be\n$for(by-author)$\n\\author[$if(by-author.affiliations)$$for(by-author.affiliations)$$it.id$$sep$,$endfor$$endif$%\n]{$by-author.name.literal$}\n$endfor$\nWe can use the % just as in LaTeX for the normal end of line behavior.\nNow this all works beautifully for the affiliations, but we also need to think about the \\(1\\) and \\(2\\). \\(1\\) means equal first authorship if there are multiple first authors. \\(2\\) means corresponding author assuming that there are multiple first authors. So, \\(1\\) isn’t always there but \\(2\\) should be. Now, we can back out that if there are multiple first authors, then the author listed first will be a first author.\nThis is good news! This means we can check if there is an equal contribution note. We can do this with our good friend LaTeX, by defining a new command for this. If there’s an equal contributor, it’ll always be 1. We can make the corresponding variable vary based on the equal contributor variable.\n\\newcommand{\\equalcont}{1}\n\n$if(equal-contributor)$\n\\newcommand{\\correspond}{2}\n$else$\n\\newcommand{\\correspond}{1}\n$endif$\nWe can then use that in our templating. If an author is listed as an equal author, we want to add a ,\\equalcont after the affiliations. So, we can do just that by adding in another line.\n$for(by-author)$\n\\author[$if(by-author.affiliations)$$for(by-author.affiliations)$$it.id$$sep$,$endfor$$endif$%\n$if(by-author.attributes.equal-contributor)$,\\equalcont$endif$%\n]{$by-author.name.literal$}\n$endfor$\nIn the same way, we can add information about corresponding authors. If an author is listed as the corresponding author, we want to add a ,\\correspond after the affiliations.\n$for(by-author)$\n\\author[$if(by-author.affiliations)$$for(by-author.affiliations)$$it.id$$sep$,$endfor$$endif$%\n$if(by-author.attributes.equal-contributor)$,\\equalcont$endif$%\n$if(by-author.attributes.corresponding)$,\\correspond$endif$%\n]{$by-author.name.literal$}\n$endfor$\nNow, we can turn to setting up the affiliation lines, with \\affil. As with authors, there is a by-affiliation keyword that will let us iterate over the affiliations. For each affilition, we want something of the form:\n\\affil[a]{Affiliation One}\nAt the most basic level, if it was just a name (like “Harvard University”), we could use a loop and the it syntax again to make something like this:\n$for(by-affiliation)$\n\\affil[$it.id$]{$it.name$}\n$endfor$\nWe need it to do a teeny bit more though. We want something instead that uses all of the relevant pieces of the YAML to say:\nHarvard University, Department of Government, Cambridge, MA 02138\nThe components of this are then:\nname, department, city, state, postal-code\nFor each affiliation, we would want something like:\n$if(it.name)$$it.name$$endif$ \n$if(it.department)$, $it.department$$endif$ \n$if(it.city)$, $it.city$$endif$ \n$if(it.state)$, $it.state$$endif$ \n$if(it.postal-code)$, $it.postal-code$$endif$ \n$if(it.country)$, $it.country$$endif$\nNote that we’re separating them with , at the start of each additional argument. This avoids weird spacing in the output. And we should probably add in country at the end, for if not all authors work in the US.\n$if(it.country)$, $it.country$$endif$\nWe can put this all in its own file, called _affiliation.tex in the partials/ directory. We don’t have to, but it seems to be common practice in existing templates. Then, we can call this chunk using $_affiliation.tex()$. The benefit of doing that is it keeps the template cleaner and easier to debug later.\n$for(by-affiliation)$\n\\affil[$it.id$]{$_affiliation.tex()$}\n$endfor$\nDon’t forget to add the partials/_affiliation.tex line to the _extension file. That will now have a template-partials chunk like so:\n      template-partials:\n        - \"partials/doc-class.tex\"\n        - \"partials/_affiliation.tex\"\n        - \"partials/title.tex\"\nNow, a minute ago, we did the hard work for the equal contributors, so we can add in some code to indicate the equal authors too in the template:\n$if(equal-contributor)$\n\\equalauthors{\\textsuperscript{\\equalcont} $equal-contributor$}\n$endif$\nWe add the superscript because we had just a raw number above.\nNow, we can do something similar for the corresponding author. This time we’ll iterate over the authors, again with by-author to find the corresponding one. If they’re the corresponding author, we’ll fill in the ... below:\n$for(by-author)$\n$if(by-author.attributes.corresponding)$\n\\correspondingauthor{...}\n$endif$\n$endfor$\nJust as with the equal contributors, we want to start it with the identifier, \\textsuperscript{\\correspond}. Then we need the PNAS template text: “To whom correspondence should be addressed. E-mail:”. And last, we can get the email from the author with $by-author.email$.\nPut together, that looks like:\n$for(by-author)$\n$if(by-author.attributes.corresponding)$\n\\correspondingauthor{\\textsuperscript{\\correspond}To whom correspondence should be addressed. E-mail: $by-author.email$}\n$endif$\n$endfor$\nFinally, we can build out the keywords. Since there can be multiple, we’ll use a for loop with a separator, like in Section 3.2.1.1. We want to iterate over each one and print them separated by a pipe, |.\n$if(keywords)$\n\\keywords{$for(keywords)$$keywords$$sep$ | $endfor$}\n$endif$\nAll together, the file looks something like:\n\\templatetype{$if(template_type)$$template_type$$else$pnasresearcharticle$endif$}\n\n$if(title)$\n\\title{$title$}\n$endif$\n\n\\newcommand{\\equalcont}{1}\n\n$if(equal-contributor)$\n\\newcommand{\\correspond}{2}\n$else$\n\\newcommand{\\correspond}{1}\n$endif$\n\n$for(by-author)$\n\\author[$if(by-author.affiliations)$$for(by-author.affiliations)$$it.id$$sep$,$endfor$$endif$%\n$if(by-author.attributes.equal-contributor)$,\\equalcont$endif$%\n$if(by-author.attributes.corresponding)$,\\correspond$endif$%\n]{$by-author.name.literal$}\n$endfor$\n\n$for(by-affiliation)$\n\\affil[$it.id$]{$_affiliation.tex()$}\n$endfor$\n\n$if(runningauthor)$\n\\leadauthor{$runningauthor$}\n$endif$\n\n$if(significance)$\n\\significancestatement{$significance$}\n$endif$\n\n$if(author_contributions)$\n\\authorcontributions{$author_contributions$}\n$endif$\n\n$if(conflict_of_interest)$\n\\authordeclaration{$conflict_of_interest$}\n$endif$\n\n$if(equal-contributor)$\n\\equalauthors{\\textsuperscript{\\equalcont} $equal-contributor$}\n$endif$\n\n$for(by-author)$\n$if(by-author.attributes.corresponding)$\n\\correspondingauthor{\\textsuperscript{\\correspond}To whom correspondence should be addressed. E-mail: $by-author.email$}\n$endif$\n$endfor$\n\n$if(keywords)$\n\\keywords{$for(keywords)$$keywords$$sep$ | $endfor$}\n$endif$\nNote that it will eventually be processeed from top to bottom, so we have to define the LaTeX commands before we use them.\n\n\nbefore-body.tex\nOkay, with the hardest part done, we can do this one without too much crazy stuff. This partial is all the stuff that comes after \\begin{document} but before the writing. So, that normally includes making the title and the abstract. As you’ll see in a minute, there’s also some generic LaTeX from the template that we want here.\nFirst, if there’s a title we have to tell it to make it. This looks like much of how we built out the title section.\n$if(title)$\n\\maketitle\n$endif$\nNext, if there’s an abstract, we need to make that. We also need it to be in an abstract environment, so we can toss the whole thing into one if statement. For what it’s worth, this chunk goes in pretty much every template. I just copy the same chunk from old Quarto template to new Quarto template whenever I need it.\n$if(abstract)$\n\\begin{abstract}\n$abstract$\n\\end{abstract}\n$endif$\nAnd really both of these two things are handled by the default partial. I would have preferred to not change it, but we need some defaults to be set here that we see in the template itself. We can just straight up copy those into it. No edits necessary. If you go the route to allow setting the doi, perhaps with a YAML doi:, this is where you would insert that.\n\\dates{This manuscript was compiled on \\today}\n\\doi{\\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}\n\n\\thispagestyle{firststyle}\n\\ifthenelse{\\boolean{shortarticle}}{\\ifthenelse{\\boolean{singlecolumn}}{\\abscontentformatted}{\\abscontent}}{}\nAll together, the before-body partial looks something like:\n$if(title)$\n\\maketitle\n$endif$\n\n$if(abstract)$\n\\begin{abstract}\n$abstract$\n\\end{abstract}\n$endif$\n\n\\dates{This manuscript was compiled on \\today}\n\\doi{\\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}\n\n\\thispagestyle{firststyle}\n\\ifthenelse{\\boolean{shortarticle}}{\\ifthenelse{\\boolean{singlecolumn}}{\\abscontentformatted}{\\abscontent}}{}\nThis is about the “simplest” partial to make, once you’re comfortable with partials, since it is so close to the default. If it weren’t for setting the first page style, we could have omitted it. Don’t forget to add it to _extension.yml.\n\n\nbefore-bib.tex\nIn the PNAS template, just before the bibliography, we need to include the acknowledgements. We also need to make a call to a function, \\showacknow{} to make them appear.\nThe first part is the same as our usual pattern. Check if there are acknowledgements, and if there are, set them.\n$if(acknowledgements)$\n\\acknow{$acknowledgements$}\n\n\\showacknow{} % Display the acknowledgments section\n$endif$\nWithin the check, we’ll add the quick call to make sure they appear.\nAs before, we to add it to _extension.yml. But now we’re done with most of the work!\n\n\n\n\nEditing template.qmd (Part 2: The Body)\nOkay, now we can test and run with the file. To do so, we want to add some text back from the other template that instructs the user how to fill out the template. I’ll start by copying in big chunks from the template and then converting things from LaTeX to the friendlier Quarto (or sometimes R) syntax.\nFor example, if the LaTeX template has:\n\\subsection*{Author Affiliations}\nI’ll make that into:\n## Author Affiliations {.unnumbered}\nWe can replace things like sections or links with some find+replace regex in RStudio:\n\nReplace (\\\\subsection\\*{)(.+?)(}) with ## \\2 {.unnumbered} to fix subsection titles.\nReplace (\\\\subsubsection\\*{)(.+?)(}) with ### \\2 {.unnumbered} to fix subsubsection titles.\nReplace (\\\\href{)(.+?)(})({)(.+?)(}) with [\\5](\\2) to move links from LaTeX to Quarto.\nReplace (\\\\ref{fig:)(.+?)(}) with @fig-\\2 to move cross references to Quarto syntax.\nReplace (\\\\verb\\|)(.+?)(\\|) with \\\\2`` to move verbatim environments to Quarto ones.\n\nAs I was doing this, I got a really weird error.\ncompilation failed- error\nLaTeX Error: Not in outer par mode.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H &lt;return&gt;  for immediate help.\n ...                                              \n                                                  \nl.206 \\end{document}\n                     \n\nsee template.log for more information.\nThe obvious thing was that I probably had a mismatch in } or { somewhere. After a few minutes, that clearly wasn’t the case. They all seemed to match and match correctly.\nAt this point, I looked at the template again and thought about the first page special formatting. The first page has a different column width, so it’s a little special. I added some more paragraphs from the LaTeX template and it rendered fine. Playing around and if there was anything short of a page, it would fail. Otherwise, it worked beautifully. As such, I added a warning to the template in the text, so any overzealous compilers would have a clue what broke.\nA word of warning: This template will fail with a \"Not in outer par mode\" warning if you try to compile it with less than one page of text.\nThe template relies on having a full first page which is styled separately.\nIf you see a warning to the tune of \"LaTeX Error: Not in outer par mode.\" or referencing `\\end{document}`, try writing more or adding filler text and recompiling.\nNext, I’ll replace figures. For the most basic figures, I’ll just use raw Quarto figure syntax.\nFor example:\n\\begin{figure}%[tbhp]\n\\centering\n\\includegraphics[width=.8\\linewidth]{figs/frog.pdf}\n\\caption{Placeholder image of a frog with a long example legend to show justification setting.}\n\\label{fig:frog}\n\\end{figure}\ncan be pretty well replicated with the shorter:\n![Placeholder image of a frog with a long example legend to show justification setting.](figs/frog.pdf){#fig-frog}\nAdditional settings can be set within an R chunk (or other code chunk). I find it very helpful to mention the fig-env argument, such as below:\n\nIn Quarto, we can do these by setting the fig-env command to figure* or SCfigure*\n\n#| label: fig-side\n#| fig-cap: \"This legend would be placed at the side of the figure, rather than below it.\"\n#| fig-env: \"SCfigure*\"\n#| echo: false\n# tell it the options as comments with a | and a space, as above.\n# set echo: false to avoid printing this text\nknitr::include_graphics('figs/frog.pdf')\nTwo column journals need the figure* (often called a “star figure”, “figure star”, or “star” environment) for page wide columns. PNAS also has a side-caption version, which is included in the template as the above example. This gives a clear demo of a simpler approach to including figures, at least than with LaTeX.\nAs for tables, I tend to leave them as-is in LaTeX. Many table environments need a little something else that doesn’t seem to translate super well into Markdown syntax. Of course, you could translate them into Markdown, especially if the LaTeX doesn’t need special options.\nFinally, I like to include anything about the bibliography in a References section, like below. I include the special \\bibsplit command that needs to be set manually at the end with a comment explaining how to use it.\n# References\n\\bibsplit[2]\n&lt;!-- Use \\bibsplit to split the references from the body of the text. Value \"[2]\" represents the number of reference in the left column (Note: Please avoid single column figures & tables on this page.) --&gt;\n\n:::{#refs}\n:::\nThis includes the special reference div (the ::: things) to help make it easier to understand where those will be printed. Details on that div are available in the Quarto documentation.\n\n\n\n\n\n\nNote\n\n\n\nDon’t forget to update the bibliography.bib default file to include any example references you use!"
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#cleaning-up",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#cleaning-up",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Cleaning up",
    "text": "Cleaning up\n\nRevising the README\nWith the Quarto and LaTeX stuff pretty much done, we want to include information for future users.\nFirst, we’ll fill out the Options section. In general, this should describe anything that isn’t default in the YAML or that should generally be set.\nFor example, I want to explain a few things:\n\nclassoption defaults\nsetting a corresponding author\naffiliation IDs should be letters\n\nWords are useful here to refer\nFor the PNAS template, I included the following:\n\nThe default setting for class option generates a two column layout with:\n\nclassoption:\n - twocolumn\n - twoside\nTo set a corresponding author, ensure that the attribute “corresponding” is true and that they have an email listed. For proper formatting, each affiliation should be given a letter id (like a, b, …, z). This template is designed for template_type: pnasresearcharticle (the default). It can also take options pnasmathematics or pnasinvited, but these are not formally supported, as they have official alternative formats available on Overleaf.\nNow, we can run the png code from before.\npdftools::pdf_convert('template.pdf', pages = 1)\nwill generate a file template_1.png.\nTo include that in the readme, we can use Markdown, like:\n![[template.qmd](template.qmd)](template_1.png)\nThat gives people enough information to get started with your template.\n\n\nAdding a .quartoignore\nNow, one last thing we want to do is make sure that people using the template won’t install extra files. Things like template_1.png from the last section are useful in the repo, but not to the end user. We can add a file .quartoignore in the root directory that functions like a .gitignore file.\nMine looks like:\n*.pdf\n*.png\n*.rproj\n*.Rproj\n!figs/*.png\nThis ignores all pdfs, pngs, and R project files. It then un-ignores the figs/ folder where useful example files for the template live. It’s important to include those in the template because you want the file that gets downloaded to run when someone runs\nquarto use template christopherkenny/pnas\nBut, they can be deleted once people have made sure the template works. As such, they don’t need to be included in the _extension.yml file, but shouldn’t be .quartoignored.\n\n\nDeleting unused files\nIt’s probably over. There may be other changes, but it’s probably over. But it’s not an official template, so no one will contact you when they update it. But, one day, you might notice that it is different or someone will comment on GitHub. But for now, you’re free! Time to make another template for that other journal you were thinking about submitting to.\nMore importantly, at this point, you should delete any files or code that you didn’t use. For this template, that means removing styles.css and pnas.lua. Also, remove them from the _extension.yml file. That is, delete:\n      filters:\n        - pnas.lua\nand\n    html:\n      css: styles.css"
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#finishing-up",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#finishing-up",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Finishing up",
    "text": "Finishing up\nLast things last.\nIf your template now works, then it’s time to make it public. A seemingly ridiculous portion of research time is spent recreating resources that surely someone else has somewhere. A semi-functional template is better than starting from scratch.\nAdd a topic to the GitHub repo for quarto-template and anything else relevant! Congrats, you have a template. Share it on Twitter or whatever is still useful. If it doesn’t work for someone, they’ll email you or open an issue. As a wise person once said &gt; all (feedback) is good (feedback)\nEven if it didn’t work for someone, a little information about why it didn’t work can help you a ton for when you have the same issue some day."
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#resources",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#resources",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Resources",
    "text": "Resources\nThe template created within this post is available at christopherkenny/pnas."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Publications\n\nWidespread Partisan Gerrymandering Mostly Cancels Nationally, but Reduces Electoral Competition\n(with Cory McCartan, Tyler Simko, Shiro Kuriwaki, and Kosuke Imai). 2023. PNAS.\n\n\nBibTeX\n\n@article{kenn:etal:23b,\nauthor = {Christopher T. Kenny and Cory McCartan and Tyler Simko and Shiro Kuriwaki and Kosuke Imai},\ntitle = {Widespread partisan gerrymandering mostly cancels nationally, but reduces electoral competition},\njournal = {Proceedings of the National Academy of Sciences},\nvolume = {120},\nnumber = {25},\npages = {e2217322120},\nyear = {2023},\ndoi = {10.1073/pnas.2217322120},\nURL = {https://www.pnas.org/doi/abs/10.1073/pnas.2217322120},\neprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2217322120},\n}\n\n\n\n\nAbstract\n\nCongressional district lines in many U.S. states are drawn by partisan actors, raising concerns about gerrymandering. To isolate the electoral impact of gerrymandering from the effects of other factors including geography and redistricting rules, we compare predicted election outcomes under the enacted plan with those under a large sample of non-partisan, simulated alternative plans for all states. We find that partisan gerrymandering is widespread in the 2020 redistricting cycle, but most of the bias it creates cancels at the national level, giving Republicans two additional seats, on average. In contrast, moderate pro-Republican bias due to geography and redistricting rules remains. Finally, we find that partisan gerrymandering reduces electoral competition and makes the House’s partisan composition less responsive to shifts in the national vote.\n\n\n\nComment: The Essential Role of Policy Evaluation for the 2020 Census Disclosure Avoidance System\n(with Shiro Kuriwaki, Cory McCartan, Evan T. R. Rosenman, and Tyler Simko). 2023. Harvard Data Science Review.\n\n\nBibTeX\n\n@article{kenn:etal:23,\n    author = {Kenny, Christopher T. and Kuriwaki, Shiro and McCartan, Cory and Rosenman, Evan T. R. and Simko, Tyler and Imai, Kosuke},\n    journal = {Harvard Data Science Review},\n    number = {Special Issue 2},\n    year = {2023},\n    month = {jan 31},\n    note = {https://hdsr.mitpress.mit.edu/pub/6ffzuq19},\n    publisher = {},\n    title = {Comment: The {Essential} {Role} of {Policy} {Evaluation} for the 2020 {Census} {DisclosureAvoidance} {System}},\n    volume = { },\n}\n\n\n\nAbstract\n\nIn “Differential Perspectives: Epistemic Disconnects Surrounding the US Census Bureau’s Use of Differential Privacy,” boyd and Sarathy argue that empirical evaluations of the Census Disclosure Avoidance System (DAS), including our published analysis, failed to recognize how the benchmark data against which the 2020 DAS was evaluated is never a ground truth of population counts. In this commentary, we explain why policy evaluation, which was the main goal of our analysis, is still meaningful without access to a perfect ground truth. We also point out that our evaluation leveraged features specific to the decennial Census and redistricting data, such as block-level population invariance under swapping and voter file racial identification, better approximating a comparison with the ground truth. Lastly, we show that accurate statistical predictions of individual race based on the Bayesian Improved Surname Geocoding, while not a violation of differential privacy, substantially increases the disclosure risk of private information the Census Bureau sought to protect. We conclude by arguing that policy makers must confront a key trade-off between data utility and privacy protection, and an epistemic disconnect alone is insufficient to explain disagreements between policy choices.\n\n\n\nSimulated redistricting plans for the analysis and evaluation of redistricting in the United States\n(with Cory McCartan, Tyler Simko, George Garcia III, Kevin Wang, Melissa Wu, Shiro Kuriwaki, and Kosuke Imai). 2022. Scientific Data.\n\n\nBibTeX\n\n@article{50statesSimulations,\n  title = {Simulated Redistricting Plans for the Analysis and Evaluation of Redistricting in the {{United States}}},\n  author = {McCartan, Cory and Kenny, Christopher T. and Simko, Tyler and Garcia, George and Wang, Kevin and Wu, Melissa and Kuriwaki, Shiro and Imai, Kosuke},\n  year = {2022},\n  month = nov,\n  journal = {Scientific Data},\n  volume = {9},\n  number = {1},\n  pages = {689},\n  issn = {2052-4463},\n  doi = {10.1038/s41597-022-01808-2},\n  abstract = {This article introduces the 50stateSimulations, a collection of simulated congressional districting plans and underlying code developed by the Algorithm-Assisted Redistricting Methodology (ALARM) Project. The 50stateSimulations allow for the evaluation of enacted and other congressional redistricting plans in the United States. While the use of redistricting simulation algorithms has become standard in academic research and court cases, any simulation analysis requires non-trivial efforts to combine multiple data sets, identify state-specific redistricting criteria, implement complex simulation algorithms, and summarize and visualize simulation outputs. We have developed a complete workflow that facilitates this entire process of simulation-based redistricting analysis for the congressional districts of all 50 states. The resulting 50stateSimulations include ensembles of simulated 2020 congressional redistricting plans and necessary replication data. We also provide the underlying code, which serves as a template for customized analyses. All data and code are free and publicly available. This article details the design, creation, and validation of the data.}\n}\n\n\n\n\nAbstract\n\nThis article introduces the 50stateSimulations, a collection of simulated congressional districting plans and underlying code developed by the Algorithm-Assisted Redistricting Methodology (ALARM) Project. The 50stateSimulations allow for the evaluation of enacted and other congressional redistricting plans in the United States. While the use of redistricting simulation algorithms has become standard in academic research and court cases, any simulation analysis requires non-trivial efforts to combine multiple data sets, identify state-specific redistricting criteria, implement complex simulation algorithms, and summarize and visualize simulation outputs. We have developed a complete workflow that facilitates this entire process of simulation-based redistricting analysis for the congressional districts of all 50 states. The resulting 50stateSimulations include ensembles of simulated 2020 congressional redistricting plans and necessary replication data. We also provide the underlying code, which serves as a template for customized analyses. All data and code are free and publicly available. This article details the design, creation, and validation of the data.\n\n\n\nThe use of differential privacy for census data and its impact on redistricting: The case of the 2020 U.S. Census\n(with Shiro Kuriwaki, Cory McCartan, Evan T. R. Rosenman, and Tyler Simko). 2021. Science Advances.\nCovered by The Washington Post, Associated Press, NC Policy Watch, and The Harvard Crimson.\n\n\nBibTeX\n\n@article{kenn:etal:21,\nauthor = {Christopher T. Kenny  and Shiro Kuriwaki  and Cory McCartan  and Evan T. R. Rosenman  and Tyler Simko  and Kosuke Imai },\ntitle = {The Use of Differential Privacy for Census Data and its Impact on Redistricting: The Case of the 2020 U.S. Census},\njournal = {Science Advances},\nvolume = {7},\nnumber = {41},\npages = {eabk3283},\nyear = {2021},\ndoi = {10.1126/sciadv.abk3283},\nURL = {https://www.science.org/doi/abs/10.1126/sciadv.abk3283},\neprint = {https://www.science.org/doi/pdf/10.1126/sciadv.abk3283},\n}\n\n\n\nAbstract\n\nThe US Census Bureau plans to protect the privacy of 2020 Census respondents through its Disclosure Avoidance System (DAS), which attempts to achieve differential privacy guarantees by adding noise to the Census microdata. By applying redistricting simulation and analysis methods to DAS-protected 2010 Census data, we find that the protected data are not of sufficient quality for redistricting purposes. We demonstrate that the injected noise makes it impossible for states to accurately comply with the One Person, One Vote principle. Our analysis finds that the DAS-protected data are biased against certain areas, depending on voter turnout and partisan and racial composition, and that these biases lead to large and unpredictable errors in the analysis of partisan and racial gerrymanders. Finally, we show that the DAS algorithm does not universally protect respondent privacy. Based on the names and addresses of registered voters, we are able to predict their race as accurately using the DAS-protected data as when using the 2010 Census data. Despite this, the DAS-protected data can still inaccurately estimate the number of majority-minority districts. We conclude with recommendations for how the Census Bureau should proceed with privacy protection for the 2020 Census.\n\n\n\nThe Essential Role of Empirical Validation in Legislative Redistricting Simulation\n(with Benjamin Fifield, Kosuke Imai, and Jun Kawahara). 2020. Statistics and Public Policy.\n\n\nBibTeX\n\n@article{fife:etal:20,\n  author = {Benjamin Fifield and Kosuke Imai and Jun Kawahara and Christopher T. Kenny},\n  title = {The Essential Role of Empirical Validation in Legislative Redistricting Simulation},\n  journal = {Statistics and Public Policy},\n  volume = {7},\n  number = {1},\n  pages = {52-68},\n  year  = {2020},\n  publisher = {Taylor & Francis},\n  doi = {10.1080/2330443X.2020.1791773},\n  URL = {https://doi.org/10.1080/2330443X.2020.1791773},\n  eprint = {https://doi.org/10.1080/2330443X.2020.1791773},\n}\n\n\n\nAbstract\n\nAs granular data about elections and voters become available, redistricting simulation methods are playing an increasingly important role when legislatures adopt redistricting plans and courts determine their legality. These simulation methods are designed to yield a representative sample of all redistricting plans that satisfy statutory guidelines and requirements such as contiguity, population parity, and compactness. A proposed redistricting plan can be considered gerrymandered if it constitutes an outlier relative to this sample according to partisan fairness metrics. Despite their growing use, an insufficient effort has been made to empirically validate the accuracy of the simulation methods. We apply a recently developed computational method that can efficiently enumerate all possible redistricting plans and yield an independent sample from this population. We show that this algorithm scales to a state with a couple of hundred geographical units. Finally, we empirically examine how existing simulation methods perform on realistic validation datasets.\n\n\n\n\nWorking Papers\n\nIndividual and Differential Harm in Redistricting\n(with Cory McCartan). Current version: 2022-06-24.\n\n\nBibTeX\n\n@misc{mcca:kenn:22,\n  doi = {10.31235/osf.io/nc2x7},\n  url = {https://osf.io/preprints/socarxiv/nc2x7/},\n  author = {McCartan, Cory and Kenny, Christopher T.},\n  keywords = {representation, redistricting, voting rights, individual harm},\n  title = {Individual and Differential Harm in Redistricting},\n  publisher = {SocArXiv},\n  year = {2022}\n}\n\n\n\n\nAbstract\n\nSocial scientists have developed dozens of measures for assessing partisan bias in redistricting.But these measures cannot be easily adapted to other groups, including those defined by race, class, or geography. Nor are they applicable to single- or no-party contexts such as local redistricting. To overcome these limitations, we propose a unified framework of harm for evaluating the impacts of a districting plan on individual voters and the groups to which they belong. We consider a voter harmed if their chosen candidate is not elected under the current plan, but would be under a different plan. Harm improves on existing measures by both focusing on the choices of individual voters and directly incorporating counterfactual plans. We discuss strategies for estimating harm, and demonstrate the utility of our framework through analyses of partisan gerrymandering in New Jersey, voting rights litigation in Alabama, and racial dynamics of Boston City Council elections.\n\n\n\nEvaluating Bias and Noise Induced by the U.S. Census Bureau’s Privacy Protection Methods\n(with Shiro Kuriwaki, Cory McCartan, Tyler Simko, and Kosuke Imai)\n\n\nBibTeX\n\n@misc{kenn:etal:2023c,\n      title={Evaluating Bias and Noise Induced by the U.S. Census Bureau's Privacy Protection Methods}, \n      author={Christopher T. Kenny and Shiro Kuriwaki and Cory McCartan and Tyler Simko and Kosuke Imai},\n      year={2023},\n      eprint={2306.07521},\n      archivePrefix={arXiv},\n      primaryClass={cs.CY}\n}\n\n\n\n\nAbstract\n\nThe United States Census Bureau faces a difficult trade-off between the accuracy of Census statistics and the protection of individual information. We conduct the first independent evaluation of bias and noise induced by the Bureau’s two main disclosure avoidance systems: the TopDown algorithm employed for the 2020 Census and the swapping algorithm implemented for the 1990, 2000, and 2010 Censuses. Our evaluation leverages the recent release of the Noisy Measure File (NMF) as well as the availability of two independent runs of the TopDown algorithm applied to the 2010 decennial Census. We find that the NMF contains too much noise to be directly useful alone, especially for Hispanic and multiracial populations. TopDown’s post-processing dramatically reduces the NMF noise and produces similarly accurate data to swapping in terms of bias and noise. These patterns hold across census geographies with varying population sizes and racial diversity. While the estimated errors for both TopDown and swapping are generally no larger than other sources of Census error, they can be relatively substantial for geographies with small total populations.\n\n\n\n\nWorks-in-Progress\n\nInequality in Administrative Democracy: Large-Sample Evidence from American Financial Regulation\n(with Daniel P. Carpenter, Angelo Dagonel, Devin Judge-Lord, Brian Libgober, Steven Rashin, Jacob Waggoner, and Susan Webb Yackee)\nAwarded the 2021 Herbert Kaufman Award.\n\n\nAlgorithm-Assisted Redistricting Methodology\n(with Kosuke Imai, Cory McCartan, and Tyler Simko). Book project."
  },
  {
    "objectID": "posts/2023-07-07-making-maps-with-feltr/index.html",
    "href": "posts/2023-07-07-making-maps-with-feltr/index.html",
    "title": "Making Maps with feltr",
    "section": "",
    "text": "I make a lot of maps, almost always in R. Recently, I was introduced to felt.com. It’s a clean interface for web maps, including some great features, like drawing directly on a map or adding text annotations.\nThe new feltr package offers an interface to the Felt API, so you can upload data to Felt directly from R. It also includes tools for reading data from Felt into R as sf objects.\nYou can install feltr with:\nBelow, I’ll demo making a map with point locations of Dunkins in Cambridge, MA, from a csv file of Dunkin addresses."
  },
  {
    "objectID": "posts/2023-07-07-making-maps-with-feltr/index.html#dunkins-in-cambridge-ma",
    "href": "posts/2023-07-07-making-maps-with-feltr/index.html#dunkins-in-cambridge-ma",
    "title": "Making Maps with feltr",
    "section": "Dunkins in Cambridge, MA",
    "text": "Dunkins in Cambridge, MA\nFirst, we’ll load a few packages.\n\nlibrary(feltr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/chris/Documents/GitHub/christopherkenny.github.io\n\n\nOne of the cool things with Felt is its “Upload Anything” feature, where we can upload anything. Here, we have a csv file of addresses for every Dunkin in Cambridge. It is simple, just text addresses separated into appropriate fields.\n\npath_dunkin_ma &lt;- here('posts/2023-07-07-making-maps-with-feltr/dunkin_ma.csv')\nread_csv(path_dunkin_ma, show_col_types = FALSE)\n\n# A tibble: 1,062 × 5\n   address           city     state zipcode    id\n   &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 147 N Quincy St   Abington MA    02351       1\n 2 259 Brockton Ave  Abington MA    02351       2\n 3 323 Centre Ave    Abington MA    02351       3\n 4 937 Bedford St    Abington MA    02351       4\n 5 100 Powdermill Rd Acton    MA    01720       5\n 6 182 Great Rd      Acton    MA    01720       6\n 7 212 Main St       Acton    MA    01720       7\n 8 315 Main St       Acton    MA    01720       8\n 9 44 Great Rd       Acton    MA    01720       9\n10 150 S Main St     Acushnet MA    02743      10\n# ℹ 1,052 more rows\n\n\nTo share this data with Felt, we first have to make a new map. We don’t have to give it any information, it’ll just make an empty map. We can pass it a title and some starting information, like where to center the map and how far to zoom.\n\ndunk &lt;- felt_create_map(\n  title = 'Cambridge Dunkin Desert', \n  zoom = 14.5, lat = 42.3799, lon = -71.10668\n)\n\nThen once we have the map, we can upload the csv file directly to Felt. No local geocoding necessary, it’ll handle that. We can label the layer with name or supply colors, like fill_color and stroke_color.\n\nlayer_id &lt;- felt_add_map_layers(\n  map_id = dunk$id, name = 'Dunkin', file_names = path_dunkin_ma, \n  fill_color = '#FF671F', stroke_color = '#DA1884'\n)\n\nOnce we do that, after a couple of minutes, we have a map. Normally it’s a few seconds if we uploaded a geojson or shp file, but geocoding takes a small bit of time.\n\n\n\nDefault Felt Layout\n\n\nWhat I find great about this is that I can handle all of the data work in R and then adjust the map as needed after. For example, I can annotate where the Department of Government buildings are with a green star or highlight where Darwin’s was (until recently) with a blue x.\n\n\n\nAnnotated Map\n\n\nClearly, Darwin’s old location would be a great place for a new Dunkin, near the middle of an existing Dunkin desert.\nfeltr has additional features, including:\n\ndeleting maps with felt_delete_map()\nlisting details of existing maps with felt_get_map() and felt_get_map_layers()\ndownloading shapes with felt_get_map_sf(), felt_get_map_geojson(), and felt_get_map_elements()\nretrieving user details with felt_get_user().\n\nAll current features of the Felt API are supported in the CRAN version of feltr, as of July 2023. To offer feedback on feltr or ask questions, open an issue on GitHub."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christopher T. Kenny",
    "section": "",
    "text": "I am a PhD candidate in the Department of Government, studying American Politics and Political Methodology. I am currently teaching a course on redistricting entitled “Drawing Democracies,” aimed at Harvard college sophomores. My substantive focus is on redistricting and gerrymandering. I work on open source R tools for analyzing redistricting and voting rights in context. I am affiliated with the Center for American Political Studies at Harvard University, the Institute for Quantitative Social Science, and the Algorithm-Assisted Redistricting Methodology (ALARM) Project. In 2022, I was a fellow at the Election Law Clinic at Harvard Law School.\n\n\n\nALARM Project\nInstitute for Quantitative Social Science\nCenter for American Political Studies\n\n\n\nHarvard University | Cambridge, MA\nPh.D. in Government | Expected May 2024\nM.A. in Government | August 2019 - May 2021\nCornell University | Ithaca, NY\nB.A. in Mathematics and Government | August 2015 - May 2019"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Christopher T. Kenny",
    "section": "",
    "text": "I am a PhD candidate in the Department of Government, studying American Politics and Political Methodology. I am currently teaching a course on redistricting entitled “Drawing Democracies,” aimed at Harvard college sophomores. My substantive focus is on redistricting and gerrymandering. I work on open source R tools for analyzing redistricting and voting rights in context. I am affiliated with the Center for American Political Studies at Harvard University, the Institute for Quantitative Social Science, and the Algorithm-Assisted Redistricting Methodology (ALARM) Project. In 2022, I was a fellow at the Election Law Clinic at Harvard Law School."
  },
  {
    "objectID": "index.html#current-affiliations",
    "href": "index.html#current-affiliations",
    "title": "Christopher T. Kenny",
    "section": "",
    "text": "ALARM Project\nInstitute for Quantitative Social Science\nCenter for American Political Studies"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Christopher T. Kenny",
    "section": "",
    "text": "Harvard University | Cambridge, MA\nPh.D. in Government | Expected May 2024\nM.A. in Government | August 2019 - May 2021\nCornell University | Ithaca, NY\nB.A. in Mathematics and Government | August 2015 - May 2019"
  }
]