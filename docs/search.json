[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Harvard College, Spring 2023 and Spring 2024.\nInstructor and course designer.\nA sophomore tutorial on redistricting and elections designed as an introduction to producing original research. A public copy of my syllabus is available here.\n\n\nFull Course Description\n\nFederal, state, and local governments are often built from geographic districts. In redistricting, the districts are drawn for a decade at a time. Commissions, legislatures, and courts draw those geographic districts for future elections. The process of redistricting can be done in as little as a few days, but the maps define constituencies for the next decade. We will study the fundamental building blocks of American democracy. Congress is entirely dependent on how its districts are drawn. Who wins when lines are the drawn? Do district-based systems empower or weaken minority communities? How do we distinguish between effects on race and party? Should we protect incumbents when there are different benefits to accountability and experience? Is it better to have non-partisan groups draw the lines? At the end of the day, how can we say if a map is good or bad?\nIn this course, we’ll explore the many aspects of how districts are drawn in America and their effects on our many geographic democracies. We’ll start by building a baseline understanding of how laws and rules from local, state, and federal entities come together to regulate mapmaking. We’ll explore the ways that we can describe maps quantitatively and qualitatively. We’ll learn from court decisions and recent research into redistricting. All the while, we will draw maps for local, state, and federal districts to better understand the tradeoffs and the high stakes in mapmaking. As new members of the Government department, we will develop your research skills, with an emphasis on how to measure and think about measuring some of the many, often-fuzzy concepts in political science.\nCourse readings draw on legal authorities, law review articles, expert reports, and empirical political science to explore the field. With a diverse set of readings it is important to realize that redistricting is partisan in many ways. We will engage with writers (and researchers) from the left and right. Disagreement is encouraged, as it is vital to advancement of knowledge, but we will make all efforts to ensure that disagreement is both substantive and polite.\n\n\n\n\n\n\n\nHarvard University and Harvard Extension School, Fall 2021.\nTeaching Fellow with Gary King.\nA first graduate course in political methodology.\n\n\nFull Course Description\n\nThis is a first graduate course in political methodology, the methodological subfield of the discipline of political science, akin to econometrics within economics, psychometrics within psychology, sociological methodology within sociology, biostatistics within public health and medicine, and dozens of others. These methodological subfields are increasingly interconnected across disciplines and are often known together under broader monikers, such as data science, applied statistics, or computational social science. Political science is an unusually diverse discipline, welcoming of an exceptionally broad array of approaches, substantive questions, theories, and scholars. As such, learning political methodology gives you experience with a broader array of specific methods and a focus on deeper, more unifying perspectives even when originating in many other areas.\nThe goal of political methodology and this course is to give you the tools necessary to do high quality scholarly research. This involves (1) learning statistical inference, using facts you know to learn about facts you don’t know, so that you feel completely comfortable using these methods in your own scholarship. With this knowledge, you will be able to easily digest articles about new methods invented after this class ends, implement the methods, apply them to your data, interpret the results, and explain them to others. You will also learn (2) how to write and publish novel substantive contributions in scholarly journals. This sounds hard, but almost everyone gets there and numerous graduate and undergraduate students in this class in previous years have published revised versions of their class papers in scholarly journals as their first professional publication. Large numbers of class papers have also turned into books, senior theses, dissertations, and conference presentations, and many have won awards and have been reported in the media.\n\nReceived the Certificate of Distinction in Teaching from the Derek Bok Center for Teaching and Learning."
  },
  {
    "objectID": "teaching.html#full-courses",
    "href": "teaching.html#full-courses",
    "title": "Teaching",
    "section": "",
    "text": "Harvard College, Spring 2023 and Spring 2024.\nInstructor and course designer.\nA sophomore tutorial on redistricting and elections designed as an introduction to producing original research. A public copy of my syllabus is available here.\n\n\nFull Course Description\n\nFederal, state, and local governments are often built from geographic districts. In redistricting, the districts are drawn for a decade at a time. Commissions, legislatures, and courts draw those geographic districts for future elections. The process of redistricting can be done in as little as a few days, but the maps define constituencies for the next decade. We will study the fundamental building blocks of American democracy. Congress is entirely dependent on how its districts are drawn. Who wins when lines are the drawn? Do district-based systems empower or weaken minority communities? How do we distinguish between effects on race and party? Should we protect incumbents when there are different benefits to accountability and experience? Is it better to have non-partisan groups draw the lines? At the end of the day, how can we say if a map is good or bad?\nIn this course, we’ll explore the many aspects of how districts are drawn in America and their effects on our many geographic democracies. We’ll start by building a baseline understanding of how laws and rules from local, state, and federal entities come together to regulate mapmaking. We’ll explore the ways that we can describe maps quantitatively and qualitatively. We’ll learn from court decisions and recent research into redistricting. All the while, we will draw maps for local, state, and federal districts to better understand the tradeoffs and the high stakes in mapmaking. As new members of the Government department, we will develop your research skills, with an emphasis on how to measure and think about measuring some of the many, often-fuzzy concepts in political science.\nCourse readings draw on legal authorities, law review articles, expert reports, and empirical political science to explore the field. With a diverse set of readings it is important to realize that redistricting is partisan in many ways. We will engage with writers (and researchers) from the left and right. Disagreement is encouraged, as it is vital to advancement of knowledge, but we will make all efforts to ensure that disagreement is both substantive and polite.\n\n\n\n\n\n\n\nHarvard University and Harvard Extension School, Fall 2021.\nTeaching Fellow with Gary King.\nA first graduate course in political methodology.\n\n\nFull Course Description\n\nThis is a first graduate course in political methodology, the methodological subfield of the discipline of political science, akin to econometrics within economics, psychometrics within psychology, sociological methodology within sociology, biostatistics within public health and medicine, and dozens of others. These methodological subfields are increasingly interconnected across disciplines and are often known together under broader monikers, such as data science, applied statistics, or computational social science. Political science is an unusually diverse discipline, welcoming of an exceptionally broad array of approaches, substantive questions, theories, and scholars. As such, learning political methodology gives you experience with a broader array of specific methods and a focus on deeper, more unifying perspectives even when originating in many other areas.\nThe goal of political methodology and this course is to give you the tools necessary to do high quality scholarly research. This involves (1) learning statistical inference, using facts you know to learn about facts you don’t know, so that you feel completely comfortable using these methods in your own scholarship. With this knowledge, you will be able to easily digest articles about new methods invented after this class ends, implement the methods, apply them to your data, interpret the results, and explain them to others. You will also learn (2) how to write and publish novel substantive contributions in scholarly journals. This sounds hard, but almost everyone gets there and numerous graduate and undergraduate students in this class in previous years have published revised versions of their class papers in scholarly journals as their first professional publication. Large numbers of class papers have also turned into books, senior theses, dissertations, and conference presentations, and many have won awards and have been reported in the media.\n\nReceived the Certificate of Distinction in Teaching from the Derek Bok Center for Teaching and Learning."
  },
  {
    "objectID": "teaching.html#short-courses-and-workshops",
    "href": "teaching.html#short-courses-and-workshops",
    "title": "Teaching",
    "section": "Short courses and workshops",
    "text": "Short courses and workshops\n\nlawyR: R Basics for Lawyers\n\n\n\nElection Law Clinic at Harvard Law School, Fall 2022.\nInstructor.\nAn 8 week workshop on basics of R for the social sciences, focusing on relevant issues to election lawyers.\n\n\nMath Prefresher for Political Scientists\n\n\n\nHarvard University, Summer 2022.\nInstructor.\nA PhD-level refresher (often called “math camp”) on math (linear algebra, calculus, and probability theory) and introduction to R for incoming graduate students in the Department of Government.\n\n\nFull Course Description\n\nMath prefresher (or “math camp”) programs in political science invite newly admitted PhD students to graduate school a week or two before their official start date to attend classes on math, statistics, computer science, and related technical material designed specially for them. At Harvard’s Department of Government, we have welcomed students to the prefresher since 1995. No grades are assigned. No individual attendance records are kept. The program is entirely voluntary, but almost all students usually choose to attend the entire program, regardless of background or interests. A faculty advisor organizes and guides the program and senior graduate students serve as instructors.\n\n\n\nThe Data Science of Redistricting\nHarvard College, Spring 2022.\nInstructor with Tyler Simko.\nAn undergraduate workshop on the data science tools used for studying redistricting. Materials for the workshop are available on GitHub.\n\n\nFull Course Description\n\nEvery decade following the US Census, states redraw their district maps for offices up and down the ballot like Congress and state legislatures. This process assigns particular geographic areas to political districts, where candidates then run for office. For example, most of Cambridge is assigned to Massachusetts Congressional Districts #5 (Katherine Clark) and #7 (Ayanna Pressley). Decisions about how to draw these maps are intensely political, as you may have heard about from ongoing lawsuits on racial and partisan “gerrymandering” and “vote dilution” in states like Alabama, New York, Ohio, and Pennsylvania.\nWhat is required when drawing a map? How do we evaluate the “fairness” of a particular map? How can we use data to evaluate these claims? In this workshop, we will cover the data science of redistricting. Starting with a discussion of the legal context and requirements in redistricting, we will think about how to evaluate maps in measurable ways. Along the way, we’ll cover tools for creating maps in R. Then, we will cover creating our own maps using algorithmic “simulation” methods, which create alternative plans according to particular criteria that statutes specify. These techniques are being used in cases across the country right now, and this workshop will provide you with all the background information you need to understand, evaluate, and maybe even contribute your own analyses."
  },
  {
    "objectID": "teaching.html#proposed-classes",
    "href": "teaching.html#proposed-classes",
    "title": "Teaching",
    "section": "Proposed Classes",
    "text": "Proposed Classes\n\nComputational Social Science: A Course in R\nA proposed course for advanced undergraduates, masters students, and PhD students that teaches programming for computational social scientists. A draft of my syllabus is available here.\n\n\nFull Course Description\n\nResearch in the social sciences is never hurt, and is frequently improved, by applying computational tools and techniques. Yet, we almost never teach necessary computational skills to social science students. This course is designed to fill that gap.\nThis course will teach you how to use the R programming language. You will not learn statistics in this course. Instead, you will learn how to use R to download, manipulate, and analyze data. Often, R is “taught” as a set of functions or packages to fill existing needs. This course will teach you how to think in R, so that you can solve new problems as they arise. The goal is not to give you experience with a set of tools, but to give you the skills to use any tool."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "(with Cory McCartan, Tyler Simko, Emma Ebowe, Michael Y. Zhao, and Kosuke Imai).\nSlides from PolMeth 2024\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nPolitical actors frequently manipulate redistricting plans to gain electoral advantages, a process commonly known as gerrymandering. To address this problem, several states have implemented institutional reforms including the establishment of map-drawing commissions. It is difficult to assess the impact of such reforms because each state structures bundles of complex rules in different ways. We propose to model redistricting processes as a sequential game. The equilibrium solution to the game summarizes multi-step institutional interactions as a single dimensional score. This score measures the leeway political actors have over the partisan lean of the final plan. Using a differences-in-differences design, we demonstrate that reforms reduce partisan bias and increase competitiveness when they constrain partisan actors. We perform a counterfactual policy analysis to estimate the partisan effects of enacting recent institutional reforms nationwide. We find that instituting redistricting commissions generally reduces the current Republican advantage, but Michigan-style reforms would yield a much greater pro-Democratic effect than types of redistricting commissions adopted in Ohio and New York.\n\n\n\n\n@article{mcca:etal:24a,\n  title={Redistricting Reforms Reduce Gerrymandering by Constraining Partisan Actors},\n  author={McCartan, Cory and Kenny, Christopher T. and  Simko, Tyler and Ebowe, Emma and Zhao, Michael Y. and Imai, Kosuke},\n      year={2024},\n      eprint={2407.11336},\n      archivePrefix={arXiv},\n      primaryClass={stat.AP},\n      url={https://arxiv.org/abs/2407.11336}, \n}\n\n\n\n\n\n(with Cory McCartan).\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nSocial scientists have developed dozens of measures for assessing partisan bias in redistricting.But these measures cannot be easily adapted to other groups, including those defined by race, class, or geography. Nor are they applicable to single- or no-party contexts such as local redistricting. To overcome these limitations, we propose a unified framework of harm for evaluating the impacts of a districting plan on individual voters and the groups to which they belong. We consider a voter harmed if their chosen candidate is not elected under the current plan, but would be under a different plan. Harm improves on existing measures by both focusing on the choices of individual voters and directly incorporating counterfactual plans. We discuss strategies for estimating harm, and demonstrate the utility of our framework through analyses of partisan gerrymandering in New Jersey, voting rights litigation in Alabama, and racial dynamics of Boston City Council elections.\n\n\n\n\n@misc{mcca:kenn:22,\n  doi = {10.31235/osf.io/nc2x7},\n  url = {https://osf.io/preprints/socarxiv/nc2x7/},\n  author = {McCartan, Cory and Kenny, Christopher T.},\n  keywords = {representation, redistricting, voting rights, individual harm},\n  title = {Individual and Differential Harm in Redistricting},\n  publisher = {SocArXiv},\n  year = {2022}\n}\n\n\n\n\n\n(with Daniel P. Carpenter, Angelo Dagonel, Devin Judge-Lord, Brian Libgober, Steven Rashin, Jacob Waggoner, and Susan Webb Yackee)\nAwarded the 2021 Herbert Kaufman Award.\n\n\nAbstract\n\n\n\n\nResearch on inequality overlooks administrative policymaking, where most U.S. law is currently made, under pressure from vast flows of money, lobbying, and political mobilization. Analyzing a new database of over 260,000 comments on agency rules implementing the Dodd-Frank Act, we identify the lobbying activities of over 6,000 organizations. Leveraging measures of organizations’ wealth, participation in administrative politics, sophistication, and lobbying success, we provide the first large-scale assessment of wealth-based inequality in agency rulemaking. We find that wealthier organizations are more likely to participate in rulemaking and enjoy more success in shifting the content of federal agency rules. These patterns are not explained by membership differentials. More profit-driven organizations are also more likely to participate and enjoy more success in shifting the content of federal agency rules. Wealthier organizations’ ability to marshal legal and technical expertise appears to be a key mechanism by which wealth leads to lobbying success.\n\n\n\n\n\n(with Jacob R. Brown and Tyler Simko)\nSend me an email for a current draft.\n\n\nAbstract\n\n\n\n\nResidential segregation in the United States is widespread, has persisted over time, and threatens fair economic opportunity and social cohesion. Most commonly used measures of segregation rely on aggregate data that impose arbitrary definitions of local geography. Previous work demonstrates that segregation measures are sensitive to the particular aggregation. Using recent advances in redistricting software, we evaluate the bias and uncertainty created by these measurement choices. We sample alternative Census tract maps that follow Census guidelines, ensuring that maps are contiguous and meet certain population bounds. We then calculate segregation metrics for each map to construct probabilistic distributions of segregation indices. With these data we provide bias-corrected estimates of racial segregation across U.S. cities and quantify the uncertainty induced by aggregation measurement error. The data demonstrate that official Census Tract definitions overstate the degree of racial segregation in the United States. This measurement error is most pronounced in small and medium sized cities and is most severe for two-group, rather than multi-group, segregation measures. We offer these new data as a tool for researchers and demonstrate their potential by re-examining contemporary and over-time segregation.\n\n\n\n\n\n\n\nAbstract\n\n\n\n\nPolicymakers in America often hold power over democratic institutions, especially electoral institutions. Their decisions can place principled decision-making at odds with partisan goals, especially in polarized times. How do policymakers balance these competing interests? I argue that policymakers are more likely to side against partisan interests when the law is more explicit. I apply this to partisan gerrymandering in the United States, where map drawers can manipulate district boundaries to favor one party. This provides a hard test, where partisan interests are directly at odds with democratic principles and the stakes of any decision are high. Using new data on the 2020 redistricting cycle combined with redistricting simulations, I find that map drawers typically follow rules that protect partisan fairness. Further, if a partisan gerrymandering case is brought against a redistricting plan, courts are more likely to rule against a plan when there is an explicit law against partisan gerrymandering. When courts intervene, they consistently, but only moderately, decrease the partisan bias of the plan. I then demonstrate that compliance with other, nonpartisan redistricting rules is highest when it is easiest to measure violations. This contributes optimistic evidence that rules effectively bind partisans."
  },
  {
    "objectID": "research.html#selected-working-papers",
    "href": "research.html#selected-working-papers",
    "title": "Research",
    "section": "",
    "text": "(with Cory McCartan, Tyler Simko, Emma Ebowe, Michael Y. Zhao, and Kosuke Imai).\nSlides from PolMeth 2024\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nPolitical actors frequently manipulate redistricting plans to gain electoral advantages, a process commonly known as gerrymandering. To address this problem, several states have implemented institutional reforms including the establishment of map-drawing commissions. It is difficult to assess the impact of such reforms because each state structures bundles of complex rules in different ways. We propose to model redistricting processes as a sequential game. The equilibrium solution to the game summarizes multi-step institutional interactions as a single dimensional score. This score measures the leeway political actors have over the partisan lean of the final plan. Using a differences-in-differences design, we demonstrate that reforms reduce partisan bias and increase competitiveness when they constrain partisan actors. We perform a counterfactual policy analysis to estimate the partisan effects of enacting recent institutional reforms nationwide. We find that instituting redistricting commissions generally reduces the current Republican advantage, but Michigan-style reforms would yield a much greater pro-Democratic effect than types of redistricting commissions adopted in Ohio and New York.\n\n\n\n\n@article{mcca:etal:24a,\n  title={Redistricting Reforms Reduce Gerrymandering by Constraining Partisan Actors},\n  author={McCartan, Cory and Kenny, Christopher T. and  Simko, Tyler and Ebowe, Emma and Zhao, Michael Y. and Imai, Kosuke},\n      year={2024},\n      eprint={2407.11336},\n      archivePrefix={arXiv},\n      primaryClass={stat.AP},\n      url={https://arxiv.org/abs/2407.11336}, \n}\n\n\n\n\n\n(with Cory McCartan).\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nSocial scientists have developed dozens of measures for assessing partisan bias in redistricting.But these measures cannot be easily adapted to other groups, including those defined by race, class, or geography. Nor are they applicable to single- or no-party contexts such as local redistricting. To overcome these limitations, we propose a unified framework of harm for evaluating the impacts of a districting plan on individual voters and the groups to which they belong. We consider a voter harmed if their chosen candidate is not elected under the current plan, but would be under a different plan. Harm improves on existing measures by both focusing on the choices of individual voters and directly incorporating counterfactual plans. We discuss strategies for estimating harm, and demonstrate the utility of our framework through analyses of partisan gerrymandering in New Jersey, voting rights litigation in Alabama, and racial dynamics of Boston City Council elections.\n\n\n\n\n@misc{mcca:kenn:22,\n  doi = {10.31235/osf.io/nc2x7},\n  url = {https://osf.io/preprints/socarxiv/nc2x7/},\n  author = {McCartan, Cory and Kenny, Christopher T.},\n  keywords = {representation, redistricting, voting rights, individual harm},\n  title = {Individual and Differential Harm in Redistricting},\n  publisher = {SocArXiv},\n  year = {2022}\n}\n\n\n\n\n\n(with Daniel P. Carpenter, Angelo Dagonel, Devin Judge-Lord, Brian Libgober, Steven Rashin, Jacob Waggoner, and Susan Webb Yackee)\nAwarded the 2021 Herbert Kaufman Award.\n\n\nAbstract\n\n\n\n\nResearch on inequality overlooks administrative policymaking, where most U.S. law is currently made, under pressure from vast flows of money, lobbying, and political mobilization. Analyzing a new database of over 260,000 comments on agency rules implementing the Dodd-Frank Act, we identify the lobbying activities of over 6,000 organizations. Leveraging measures of organizations’ wealth, participation in administrative politics, sophistication, and lobbying success, we provide the first large-scale assessment of wealth-based inequality in agency rulemaking. We find that wealthier organizations are more likely to participate in rulemaking and enjoy more success in shifting the content of federal agency rules. These patterns are not explained by membership differentials. More profit-driven organizations are also more likely to participate and enjoy more success in shifting the content of federal agency rules. Wealthier organizations’ ability to marshal legal and technical expertise appears to be a key mechanism by which wealth leads to lobbying success.\n\n\n\n\n\n(with Jacob R. Brown and Tyler Simko)\nSend me an email for a current draft.\n\n\nAbstract\n\n\n\n\nResidential segregation in the United States is widespread, has persisted over time, and threatens fair economic opportunity and social cohesion. Most commonly used measures of segregation rely on aggregate data that impose arbitrary definitions of local geography. Previous work demonstrates that segregation measures are sensitive to the particular aggregation. Using recent advances in redistricting software, we evaluate the bias and uncertainty created by these measurement choices. We sample alternative Census tract maps that follow Census guidelines, ensuring that maps are contiguous and meet certain population bounds. We then calculate segregation metrics for each map to construct probabilistic distributions of segregation indices. With these data we provide bias-corrected estimates of racial segregation across U.S. cities and quantify the uncertainty induced by aggregation measurement error. The data demonstrate that official Census Tract definitions overstate the degree of racial segregation in the United States. This measurement error is most pronounced in small and medium sized cities and is most severe for two-group, rather than multi-group, segregation measures. We offer these new data as a tool for researchers and demonstrate their potential by re-examining contemporary and over-time segregation.\n\n\n\n\n\n\n\nAbstract\n\n\n\n\nPolicymakers in America often hold power over democratic institutions, especially electoral institutions. Their decisions can place principled decision-making at odds with partisan goals, especially in polarized times. How do policymakers balance these competing interests? I argue that policymakers are more likely to side against partisan interests when the law is more explicit. I apply this to partisan gerrymandering in the United States, where map drawers can manipulate district boundaries to favor one party. This provides a hard test, where partisan interests are directly at odds with democratic principles and the stakes of any decision are high. Using new data on the 2020 redistricting cycle combined with redistricting simulations, I find that map drawers typically follow rules that protect partisan fairness. Further, if a partisan gerrymandering case is brought against a redistricting plan, courts are more likely to rule against a plan when there is an explicit law against partisan gerrymandering. When courts intervene, they consistently, but only moderately, decrease the partisan bias of the plan. I then demonstrate that compliance with other, nonpartisan redistricting rules is highest when it is easiest to measure violations. This contributes optimistic evidence that rules effectively bind partisans."
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\nEvaluating Bias and Noise Induced by the U.S. Census Bureau’s Privacy Protection Methods\n(with Cory McCartan, Shiro Kuriwaki, Tyler Simko, and Kosuke Imai). 2024. Science Advances.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nThe United States Census Bureau faces a difficult trade-off between the accuracy of Census statistics and the protection of individual information. We conduct the first independent evaluation of bias and noise induced by the Bureau’s two main disclosure avoidance systems: the TopDown algorithm employed for the 2020 Census and the swapping algorithm implemented for the 1990, 2000, and 2010 Censuses. Our evaluation leverages the recent release of the Noisy Measure File (NMF) as well as the availability of two independent runs of the TopDown algorithm applied to the 2010 decennial Census. We find that the NMF contains too much noise to be directly useful alone, especially for Hispanic and multiracial populations. TopDown’s post-processing dramatically reduces the NMF noise and produces similarly accurate data to swapping in terms of bias and noise. These patterns hold across census geographies with varying population sizes and racial diversity. While the estimated errors for both TopDown and swapping are generally no larger than other sources of Census error, they can be relatively substantial for geographies with small total populations.\n\n\n\n\n@misc{kenn:etal:24b,\n  author = {Christopher T. Kenny and Cory McCartan and Shiro Kuriwaki and Tyler Simko and Kosuke Imai},\n  title = {Evaluating bias and noise induced by the U.S. Census Bureau’s privacy protection methods},\n  journal = {Science Advances},\n  volume = {10},\n  number = {18},\n  pages = {eadl2524},\n  year = {2024},\n  doi = {10.1126/sciadv.adl2524},\n  URL = {https://www.science.org/doi/abs/10.1126/sciadv.adl2524},\n eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.adl2524},\n}\n\n\n\n\nCensus Officials Must Constructively Engage with Independent Evaluations\n(with Cory McCartan, Tyler Simko, and Kosuke Imai). 2024. PNAS.\n\n\nFirst paragraph\n\n\nBibTeX\n\n\n\n\nCurrent and former Census Bureau officials Jarmin et al. argue that differential privacy, which underlies the 2020 Census’s Disclosure Avoidance System (DAS), satisfies more desirable theoretical criteria than alternatives. They provide detailed criticisms of many published evaluations of the 2020 DAS, including our work. In this letter, we show that their criticisms are unfounded, grossly mischaracterize our research, and ignore critical issues that merit public discussion.\n\n\n\n\n@article{kenn:etal:24a,\n  title={Census officials must constructively engage with independent evaluations},\n  author={Kenny, Christopher T. and McCartan, Cory and Simko, Tyler and Imai, Kosuke},\n  journal={Proceedings of the National Academy of Sciences},\n  volume={121},\n  number={11},\n  pages={e2321196121},\n  year={2024},\n  publisher={National Acad Sciences}\n}\n\n\n\n\nWidespread Partisan Gerrymandering Mostly Cancels Nationally, but Reduces Electoral Competition\n(with Cory McCartan, Tyler Simko, Shiro Kuriwaki, and Kosuke Imai). 2023. PNAS.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nCongressional district lines in many U.S. states are drawn by partisan actors, raising concerns about gerrymandering. To isolate the electoral impact of gerrymandering from the effects of other factors including geography and redistricting rules, we compare predicted election outcomes under the enacted plan with those under a large sample of non-partisan, simulated alternative plans for all states. We find that partisan gerrymandering is widespread in the 2020 redistricting cycle, but most of the bias it creates cancels at the national level, giving Republicans two additional seats, on average. In contrast, moderate pro-Republican bias due to geography and redistricting rules remains. Finally, we find that partisan gerrymandering reduces electoral competition and makes the House’s partisan composition less responsive to shifts in the national vote.\n\n\n\n\n@article{kenn:etal:23b,\nauthor = {Christopher T. Kenny and Cory McCartan and Tyler Simko and Shiro Kuriwaki and Kosuke Imai},\ntitle = {Widespread partisan gerrymandering mostly cancels nationally, but reduces electoral competition},\njournal = {Proceedings of the National Academy of Sciences},\nvolume = {120},\nnumber = {25},\npages = {e2217322120},\nyear = {2023},\ndoi = {10.1073/pnas.2217322120},\nURL = {https://www.pnas.org/doi/abs/10.1073/pnas.2217322120},\neprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2217322120},\n}\n\n\n\n\nComment: The Essential Role of Policy Evaluation for the 2020 Census Disclosure Avoidance System\n(with Shiro Kuriwaki, Cory McCartan, Evan T. R. Rosenman, and Tyler Simko). 2023. Harvard Data Science Review.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nIn “Differential Perspectives: Epistemic Disconnects Surrounding the US Census Bureau’s Use of Differential Privacy,” boyd and Sarathy argue that empirical evaluations of the Census Disclosure Avoidance System (DAS), including our published analysis, failed to recognize how the benchmark data against which the 2020 DAS was evaluated is never a ground truth of population counts. In this commentary, we explain why policy evaluation, which was the main goal of our analysis, is still meaningful without access to a perfect ground truth. We also point out that our evaluation leveraged features specific to the decennial Census and redistricting data, such as block-level population invariance under swapping and voter file racial identification, better approximating a comparison with the ground truth. Lastly, we show that accurate statistical predictions of individual race based on the Bayesian Improved Surname Geocoding, while not a violation of differential privacy, substantially increases the disclosure risk of private information the Census Bureau sought to protect. We conclude by arguing that policy makers must confront a key trade-off between data utility and privacy protection, and an epistemic disconnect alone is insufficient to explain disagreements between policy choices.\n\n\n\n\n@article{kenn:etal:23,\n    author = {Kenny, Christopher T. and Kuriwaki, Shiro and McCartan, Cory and Rosenman, Evan T. R. and Simko, Tyler and Imai, Kosuke},\n    journal = {Harvard Data Science Review},\n    number = {Special Issue 2},\n    year = {2023},\n    month = {jan 31},\n    note = {https://hdsr.mitpress.mit.edu/pub/6ffzuq19},\n    publisher = {},\n    title = {Comment: The {Essential} {Role} of {Policy} {Evaluation} for the 2020 {Census} {DisclosureAvoidance} {System}},\n    volume = { },\n}\n\n\n\n\nSimulated redistricting plans for the analysis and evaluation of redistricting in the United States\n(with Cory McCartan, Tyler Simko, George Garcia III, Kevin Wang, Melissa Wu, Shiro Kuriwaki, and Kosuke Imai). 2022. Scientific Data.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nThis article introduces the 50stateSimulations, a collection of simulated congressional districting plans and underlying code developed by the Algorithm-Assisted Redistricting Methodology (ALARM) Project. The 50stateSimulations allow for the evaluation of enacted and other congressional redistricting plans in the United States. While the use of redistricting simulation algorithms has become standard in academic research and court cases, any simulation analysis requires non-trivial efforts to combine multiple data sets, identify state-specific redistricting criteria, implement complex simulation algorithms, and summarize and visualize simulation outputs. We have developed a complete workflow that facilitates this entire process of simulation-based redistricting analysis for the congressional districts of all 50 states. The resulting 50stateSimulations include ensembles of simulated 2020 congressional redistricting plans and necessary replication data. We also provide the underlying code, which serves as a template for customized analyses. All data and code are free and publicly available. This article details the design, creation, and validation of the data.\n\n\n\n\n@article{50statesSimulations,\n  title = {Simulated Redistricting Plans for the Analysis and Evaluation of Redistricting in the {{United States}}},\n  author = {McCartan, Cory and Kenny, Christopher T. and Simko, Tyler and Garcia, George and Wang, Kevin and Wu, Melissa and Kuriwaki, Shiro and Imai, Kosuke},\n  year = {2022},\n  month = nov,\n  journal = {Scientific Data},\n  volume = {9},\n  number = {1},\n  pages = {689},\n  issn = {2052-4463},\n  doi = {10.1038/s41597-022-01808-2},\n}\n\n\n\n\nThe use of differential privacy for census data and its impact on redistricting: The case of the 2020 U.S. Census\n(with Shiro Kuriwaki, Cory McCartan, Evan T. R. Rosenman, and Tyler Simko). 2021. Science Advances.\nCovered by The Washington Post, Associated Press, NC Policy Watch, and The Harvard Crimson.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nThe US Census Bureau plans to protect the privacy of 2020 Census respondents through its Disclosure Avoidance System (DAS), which attempts to achieve differential privacy guarantees by adding noise to the Census microdata. By applying redistricting simulation and analysis methods to DAS-protected 2010 Census data, we find that the protected data are not of sufficient quality for redistricting purposes. We demonstrate that the injected noise makes it impossible for states to accurately comply with the One Person, One Vote principle. Our analysis finds that the DAS-protected data are biased against certain areas, depending on voter turnout and partisan and racial composition, and that these biases lead to large and unpredictable errors in the analysis of partisan and racial gerrymanders. Finally, we show that the DAS algorithm does not universally protect respondent privacy. Based on the names and addresses of registered voters, we are able to predict their race as accurately using the DAS-protected data as when using the 2010 Census data. Despite this, the DAS-protected data can still inaccurately estimate the number of majority-minority districts. We conclude with recommendations for how the Census Bureau should proceed with privacy protection for the 2020 Census.\n\n\n\n\n@article{kenn:etal:21,\nauthor = {Christopher T. Kenny  and Shiro Kuriwaki  and Cory McCartan  and Evan T. R. Rosenman  and Tyler Simko  and Kosuke Imai },\ntitle = {The Use of Differential Privacy for Census Data and its Impact on Redistricting: The Case of the 2020 U.S. Census},\njournal = {Science Advances},\nvolume = {7},\nnumber = {41},\npages = {eabk3283},\nyear = {2021},\ndoi = {10.1126/sciadv.abk3283},\nURL = {https://www.science.org/doi/abs/10.1126/sciadv.abk3283},\neprint = {https://www.science.org/doi/pdf/10.1126/sciadv.abk3283},\n}\n\n\n\n\nThe Essential Role of Empirical Validation in Legislative Redistricting Simulation\n(with Benjamin Fifield, Kosuke Imai, and Jun Kawahara). 2020. Statistics and Public Policy.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nAs granular data about elections and voters become available, redistricting simulation methods are playing an increasingly important role when legislatures adopt redistricting plans and courts determine their legality. These simulation methods are designed to yield a representative sample of all redistricting plans that satisfy statutory guidelines and requirements such as contiguity, population parity, and compactness. A proposed redistricting plan can be considered gerrymandered if it constitutes an outlier relative to this sample according to partisan fairness metrics. Despite their growing use, an insufficient effort has been made to empirically validate the accuracy of the simulation methods. We apply a recently developed computational method that can efficiently enumerate all possible redistricting plans and yield an independent sample from this population. We show that this algorithm scales to a state with a couple of hundred geographical units. Finally, we empirically examine how existing simulation methods perform on realistic validation datasets.\n\n\n\n\n@article{fife:etal:20,\n  author = {Benjamin Fifield and Kosuke Imai and Jun Kawahara and Christopher T. Kenny},\n  title = {The Essential Role of Empirical Validation in Legislative Redistricting Simulation},\n  journal = {Statistics and Public Policy},\n  volume = {7},\n  number = {1},\n  pages = {52-68},\n  year  = {2020},\n  publisher = {Taylor & Francis},\n  doi = {10.1080/2330443X.2020.1791773},\n  URL = {https://doi.org/10.1080/2330443X.2020.1791773},\n  eprint = {https://doi.org/10.1080/2330443X.2020.1791773},\n}"
  },
  {
    "objectID": "research.html#works-in-progress",
    "href": "research.html#works-in-progress",
    "title": "Research",
    "section": "Works-in-Progress",
    "text": "Works-in-Progress\n\nAlgorithm-Assisted Redistricting Methodology\n(with Kosuke Imai, Cory McCartan, and Tyler Simko). Book project.\n\n\nAn Individual Causal Framework for Evaluating Electoral Systems\n(with Cory McCartan)"
  },
  {
    "objectID": "research.html#public-writing",
    "href": "research.html#public-writing",
    "title": "Research",
    "section": "Public Writing",
    "text": "Public Writing\n\nExpert Report in Sakhnovsky, et al v. City of Daytona Beach\nCase No. 2024 10140 CICI. Expert report on census data and map drawing for Daytona Beach. 2024.\n\n\nRedistricting Process Reform\nThe University of Chicago Center for Effective Government’s Democracy Reform Primer Series. 2024. With Steve Ansolabehere.\n\n\nAmici Curiae Brief of Fair Districts Georgia and Election Law Clinic in Support of Plaintiffs\nAlpha Phi Alpha Fraternity, Inc. et al. v. Brad Raffensperger. 2021. With the Election Law Clinic at Harvard Law School.\n\n\nMaryland Congressional District Memo\nMemo to the Maryland Redistricting Commission. 2021. With Jonathan Rodden."
  },
  {
    "objectID": "posts/2025-06-17-rockland-sare/index.html",
    "href": "posts/2025-06-17-rockland-sare/index.html",
    "title": "Thinking SMARTer about election fraud in Rockland County, NY",
    "section": "",
    "text": "An election fraud case was recently allowed to go forward in Rockland County, NY. The case is centered around the idea that ballot machines were not correctly counting the ballots and includes at least one huge ask:\nIt’s an interesting case and all of the filings are on the NYSCEF website. The case is brought by a candidate for a small third party (Diane Sare), two voters, and SMART Legislation, an electoral reform group.\nSince I’m in a short quiet period between finishing a PhD in political science at Harvard and starting a postdoc at Princeton, I thought I’d take a look at the data and major claims. I also grew up in Rockland and spend a lot of my time here, so will opine a bit on the quirks of the area. This is a blog post focused on the data and mechanisms for the claims for my own interest, has not been paid for by any party to the lawsuit, and should not be interpreted as expert evidence. As it is a blog post and not my normal academic or expert writing, I include some personal reactions and opinions.\nBelow, I start by giving a brief overview of the case. Then, I explain some thoughts on three pieces of evidence:\nThe short of it is that I’m quite skeptical of the evidence presented in the case thus far. The evidence presented doesn’t really ring any alarm bells, despite the extravagant claims made with it. Pushing very lightly on the obscure statistics chosen makes most of the claims fall apart. Further, the petitioners misrepresent the academic literature that they cite to. Despite lots of discussion online, everyone should be cautious in considering this case as a big win for election deniers."
  },
  {
    "objectID": "posts/2025-06-17-rockland-sare/index.html#sare-v.-rockland-county-board-of-elections-background",
    "href": "posts/2025-06-17-rockland-sare/index.html#sare-v.-rockland-county-board-of-elections-background",
    "title": "Thinking SMARTer about election fraud in Rockland County, NY",
    "section": "Sare v. Rockland County Board of Elections Background",
    "text": "Sare v. Rockland County Board of Elections Background\nDiane Sare ran for the US Senate in NY in November 2024. Less than a month later, she sued the Rockland County Board of Elections. The suit is joined by SMART Legislation, an advocacy group affiliated with SMART Elections. There are also two voters on the case who claim to have voted for Sare and want to ensure that their votes, along with other voters for Sare, had their votes counted correctly.\nNow, in June 2025, the case is picking up again after a state Supreme Court judge ruled that it should continue to discovery. As a quick reminder, the Supreme Court in NY is the lowest court in the state, not the court of last resort. A boisterous press release has increased the following for the case. In the release, Lulu Friesdat of SMART Legislation says the following:\n\nThere is clear evidence that the Senate results are incorrect, and there are statistical indications that the presidential results are highly unlikely.\n\nIn support of these claims, the suit includes sworn affidavits from several voters, primarily neighbors of Sare, who swear that they voted for her. Further, they link to a slide deck by a physicist and astronomer. The summary of the slide deck is:\n\nHarris underperformed Gillibrand\nTrump outperformed Sapraicone\nHarris’s underperformance is “STATISTICALLY SIGNIFICANT[ly]” different compared to how Biden performed against Mondaire Jones in 2020\n\nThe press release further links to a Bluesky thread that posts images from an online election site for Rockland County. These point to a small number of precincts where Kamala Harris received a small number of votes, while Kirsten Gillibrand received hundreds. Further, in two precincts Harris received 0 and 2 votes, while Gillibrand received 331 and 909 votes.\nThese are superficially weird, I admit. However, none of this is surprising when you consider the underlying data generating process. First, the statistics used are relatively weak and don’t necessarily replicate (where there is sufficient information to attempt replication). Second, much of the drop-off (aka rolloff) would be consistent with saying that “winning candidates get more votes than losing candidates”. This is because the petitioners and slide deck author use inappropriate statistics. Third, the precincts with weird patterns are in the heart of an Orthodox Jewish community that has a history of similar bloc voting, which would likely explain large swings between candidates."
  },
  {
    "objectID": "posts/2025-06-17-rockland-sare/index.html#ballot-rolloff",
    "href": "posts/2025-06-17-rockland-sare/index.html#ballot-rolloff",
    "title": "Thinking SMARTer about election fraud in Rockland County, NY",
    "section": "Ballot rolloff",
    "text": "Ballot rolloff\nOne of the main allegations in the petition is that there is ballot rolloff at unusual rates. A good summary of their approach is listed here:\n\n\nPetitioner SMART Legislation conducted an analysis of the results of the votes cast in Rockland County. Petitioner found that 23% of the voters who voted for the Republican Presidential Candidate Donald Trump did not vote for the Republican Senate Candidate, Michael D. Sapraicone. At the state level, 9% of voters cast their ballot for the Republican Presidential candidate but abstained from recording a vote for the Republican Senate candidate. This occurrence is referred to as the “drop-off rate” which describes a ballot cast by a voter for, in this case, the candidate at the top of the ballot, but does not submit a selection in a race for a lower-level office (here, US Senate) of the same party or who makes no selection at all among the candidates for that lower-level office.\n\n\nThis definition of rolloff (aka drop-off) is pretty close to what you might expect. The key idea is that voters who vote for the office of US President may not vote for the lower office, such as the Senate. The difference in number of votes for the higher office and the lower office form the rolloff. Generally, rolloff is going to be positive, where more people vote for higher offices than lower offices. This occurs across scales, where big contests like the Senate may see more rolloff by the time the voter reaches a municipal or town race. In general, we expect the number of votes to decrease as you get from the biggest contests to the smallest ones. There may be exceptions, such as if a local race is hotly contested.\nThe petition alleges that Harris sees a bunch of negative rolloff. Specifically, the US Senate race Democratic candidate got more votes than the US presidential race Democratic candidate. That sounds bad, right? Not really, because the petition makes a dubious switch here.\nWhat differs here from the normal idea of rolloff to the one used in the petition? The petition indicates that there is rolloff within parties! This means that any candidate who splits their ticket is counted as rolling off. What happens if you prefer Harris (D) to Trump (R), but you prefer say Lawler (R) to Jones (D) in the House race? That would be counted in this new special type of rolloff, because you voted for a Democrat in the higher office, but not in the lower office. Clearly, within-party rolloff is conceptually very different and should not be conflated with actual rolloff.\nFor a moment, take this one step further: if the rolloff within parties shouldn’t be negative, then we would expect only rare cases where a lower candidate outperforms a candidate for higher office. When there’s a blowout against a higher office candidate, their party would be expected to lose every lower race. Yet, the lower office candidates do win quite a lot, and this is often a sign of a good candidate. For example, in the 2024 election, 538’s Nathaniel Rakich shows 12 US Senate candidates outperformed their party’s president by at least 5 points. 125 US House candidates did the same! That’s nearly 30% of all US House races, not something rare.\nPutting the conceptual concerns aside, we can also look at the data and think about the empirical side. Recent press releases from SMART include a few slides from Max Bonamente, a physicist and astronomer, which can be found here. To the best of my ability, I believe the author is doing the following:\n\nCollecting precinct returns by ward for the 2024 presidential and senatorial races\nComputing a within-party rolloff by precinct\n\n\\[\\frac{\\textrm{Votes}_{i,p,r=P} - \\textrm{Votes}_{i,p,r=S}}{\\textrm{Votes}_{i,p,r=P}} \\]\nFor precinct \\(i\\), party \\(p\\), and race \\(r\\) = \\(P\\)residential or \\(S\\)enatorial.\n\nComputing an average rolloff by town by summing over the \\(i\\)s above.\nComputing an average rolloff by town by summing for the 2020 races as well, with race \\(r\\) = \\(P\\)residential or \\(H\\)ouse\n\nThe author repeats this for four of the five towns in Rockland, leaving out Ramapo, where much of the other pieces of evidence are focused.\nBelow, I’ve tried to replicate the first three steps to the best of my ability. (I’ll get back to the fourth in a bit.) I downloaded the election data for 2024 from the Rockland County Board of Elections and cleaned the data. All code for this is available publicly on my GitHub.\n\n\n\nReplication of Bonamente’s slide’s 4 town analyses\n\n\nTo read this, consider the following. Each of the four facets shows a town, as labeled above. The x-axis (horizontal) shows the election district number within the town. These numbers don’t particularly matter, but nearby numbers are typically geographically close to each other. The y-axis shows the estimate of the within-party rolloff across races. Each dot shows the estimate for that precinct. A dot at +10 would indicate that the presidential candidate outperformed the Senate candidate by 10 percentage points in that precinct. The horizontal lines show the town average for the within-party rolloff. Note that the town average may not be the same as the average of the dots, as each precinct can have a different number of total votes.\nUnfortunately, the results are close but do not perfectly replicate. Without more information than a slide deck, I can only guess why they are different, but they are similar. With this in mind, I see the same pattern, but with different point estimates:\n\nHarris underperformed Gillibrand\nTrump outperformed Sapraicone\n\nI disagree with the conclusion of the slides, however. To illustrate why, let’s make a similar comparison: the choice of comparison with the US Senate candidates is arbitrary. Let’s make the same graph, but benchmark against the US House instead.\n\n\n\nReplication of Bonamente’s slide’s 4 town analyses, using House instead of Senate\n\n\nWoah! Trump now underperforms the US House Republican incumbent Mike Lawler in two towns. This is getting heated: by the petitioner’s logic, we just uncovered huge additional fraud against Trump. So the fraud isn’t just against Harris, it’s also against Trump… woah.\nOr more realistically, this is a poor measure of fraud because it is extremely sensitive! The choice of a single candidate as a baseline is ill-advised as the candidates matter. And that’s actually a good sign for democracy, not evidence of fraud undermining it. People aren’t just blindly voting for one party, but are exercising their ability to choose between candidates.\nFurther, in a recent working paper by Ebanks, Katz, and King, they estimate an incumbency advantage that varies from around 2-10 percentage points across the last 70ish years of politics (in Figure 9 (d)). A small, dip like 2% in these four towns would well be in the range for an incumbent like Gillibrand and a not-really incumbent like Harris. That seems like a very plausible difference, yet it isn’t discussed in the analyses of the slides or the petition itself.\nFor the sake of completeness, I’ve also created a similar plot for the 2020 election. As there was no senate race in NY in 2020, I use the US House election as the baseline. As before, I’ve collected all of this data from the Rockland County Board of Elections.\n\n\n\nExtension of Bonamente’s slide’s 4 town analyses to 2020, using House votes\n\n\nAnd since I downloaded the 2016 data years ago, we can do the same, though this data is no longer on the Rockland County Board of Elections.\n\n\n\nExtension of Bonamente’s slide’s 4 town analyses to 2020, using Senate votes\n\n\nTaken together, everyone should see that these measures are very volatile. They are sensitive to the choice of candidates and bounce around a lot. The 2016 example makes this very clear, as in 2016, Schumer (D) was running against Wendy Long (R). Long’s campaign was not long for life and she lost handedly to the incumbent Schumer. As such, Hillary Clinton trailed far behind Schumer, because of the differences in the campaigns and candidates.\nAll in all, this type of evidence is more indicative of winning candidates getting more votes than losing candidates. Across the board, these large shifts are typically related to comparing within-party winners to losers.\nNow, back to the slides. I won’t address the specific p-values cited here, as it is unclear to me (1) what hypothesis we are testing and (2) how we are supposed to test it. If the difference represented in the slides is supposed to represent how Harris underperformed Gillibrand while Biden outperformed Mondaire Jones, then that’s not really interesting in the first place. There was no US Senate candidate in 2020, so my assumption is that the down-ballot here represents the US House candidate (or perhaps other down-ballot offices). There are so many other things going on there that we need substantially more information before putting time into addressing the underspecified, yet quite forceful claim being made.\nFinally, on the issue of dropoff, I want to take a final look at the normal way that we might analyze this. As a reminder, dropoff is typically looking at how voters leave blank votes for lower offices but vote for higher offices at high rates. With the election data that we have, we also have the total number of ballots cast in each precinct. For each of the three 2024 races, how many people who took a ballot voted for them?\nFirst, we look to the presidential race. These look fairly normal, with the average being 99% of votes being cast for president county-wide.\n\n\n\nPercentage of ballots voting for US President\n\n\nLooking at the US Senate, we see an increased rolloff. On average, 89% of people cast a vote for the US Senate county-wide. This seems reasonable and is in line with voters voting, just not voting straight ticket (ie casting all votes for one party).\n\n\n\nPercentage of ballots voting for US Senate\n\n\nFinally, we can repeat this for the US House. As this was a more competitive race than the other two, this race had more rolloff than the presidential race but less than the senatorial race. On average, 96% of people cast a vote for the US House county-wide.\n\n\n\nPercentage of ballots voting for US House\n\n\nNext, we can also think a bit about whether this level of ticket-splitting is normal or not based on other academic literature. In the petition, petitioners also claim the following:\n\n\nA study conducted by researchers at Yale, Harvard, MIT, and Columbia Universities found that split-ticket voting averaged between 1-2 percent in the 2020 election. Therefore, split-ticket voting seems a very unlikely explanation of the Rockland BOE data.\n\n\nHaving read Kuriwaki et al 2024 (the article cited) and seen the authors of this study present related work, I was surprised by this claim. The claims that match those numbers in the article are focused on (1) a small set of battleground states and (2) conditioning on voters already being strong partisan voters. To say the second part differently, if you condition on a voter voting for the same party for every contested down-ballot race, then they are very unlikely to switch parties for their presidential vote. This is nowhere near the claim being made in the petition and is so excruciatingly incorrect that it should be amended for the benefit of honesty in the courtroom.\n\n\n\n\n\n\nThe 1-2% figure in the petition is not a valid comparison\n\n\n\nThis equates: 1. Voters voting for Democrats (or Republicans) in every down-ballot race but the president (ie, something unlikely) to 2. Voters voting for different parties for the offices of president and senate regardless of their other down-ballot votes\n\n\nThe measure discussed in the petition is simply asking: do people vote for the same party in the presidential and senate races? As stated, we can download the CVR data from Kuriwaki et al 2024. Starting from the authors’ code in the paper, I replicate the setup with two key differences:\n\nI subset to states where there was a US Senate race and US Presidential race on the ballot\nI drop Georgia, which had 2 Senate races in 2020\n\nThe rest of it is very similar. I take the set of counties where both a Republican and a Democrat were on the ballot. This gives us data from 12 states (Arizona, Colorado, Delaware, Illinois, Iowa, Michigan, New Jersey, Oregon, Rhode Island, Tennessee, Texas, and West Virginia). We have a total of 12,374,210 ballots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSenate\n\nPresident\n\n\n\nDemocratic\nRepublican\nOther\n\n\n\n\nDemocratic\n5,891,251\n282,086\n62,684\n\n\nRepublican\n276,155\n5,145,739\n87,282\n\n\nOther\n91,359\n62,043\n475,611\n\n\n\n\n\n\n\nThis shows the total number of votes cast by party by choice for each race. Other represents votes for third parties, write-ins, overvotes, and undervotes.\nWe can also look at this as a percentage of the total ballots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSenate\n\nPresident\n\n\n\nDemocratic\nRepublican\nOther\n\n\n\n\nDemocratic\n47.6%\n2.3%\n0.5%\n\n\nRepublican\n2.2%\n41.6%\n0.7%\n\n\nOther\n0.7%\n0.5%\n3.8%\n\n\n\n\n\n\n\nSo, around 7% of voters split their vote (by summing the off-diagonal boxes).\nNote that this isn’t a perfect comparison, as NY does not make cast vote records available. This means that you can’t employ the same approach as the Kuriwaki et al article to make an exact comparison. Notably, there is a lot of variation in this and the 7% throws away interesting variation.\nConsider for example that we’re comparing 12 states to a county or 4 towns. The towns and counties will likely have more variation because they’re small. As such, below we can see a histogram of the percentage of ballots split by county.\n\n\n\nPercent of ballots with different party votes for Senate and President by county\n\n\nFurther, let’s use this data to also see the variation in within-party rolloff by county. Below, I’ve computed this for each county in the 2020 CVR data. I’ve then overlaid the estimate for the average rolloff in Rockland County in 2024. Note that these are not exact comparisons, as we do not have individual ballot records, so this is likely an underestimate for the county-level lines.\n\n\n\nWithin-party rolloff for Rockland 2024 compared to estimates from Cast Vote Records\n\n\nHere, Harris performs similarly to how Biden did relative to US Senate races by county. Trump in 2024 outperforms his past 2020 within-party rolloff based on US Senate votes. Again though, it is worth cautioning that these are not exact comparisons, as they are not computed from cast vote records. Further, they are comparing the data for different races and candidates. Since Trump won in 2024 but lost in 2020, it does not shock me that his performance looks better here."
  },
  {
    "objectID": "posts/2025-06-17-rockland-sare/index.html#no-votes-for-harris-but-votes-for-gillibrand-in-the-monsey-area",
    "href": "posts/2025-06-17-rockland-sare/index.html#no-votes-for-harris-but-votes-for-gillibrand-in-the-monsey-area",
    "title": "Thinking SMARTer about election fraud in Rockland County, NY",
    "section": "No votes for Harris, but votes for Gillibrand in the Monsey area",
    "text": "No votes for Harris, but votes for Gillibrand in the Monsey area\nThere are a few precincts that have facially surprising results insofar as there is a big difference in how Harris and Gillibrand performed in them. These precincts are primarily around Monsey within the town of Ramapo. Understanding what’s going on requires a bit of background on the area, especially as to its unique demographics.\nMonsey and its surrounding hamlets are the center of a massive, growing Orthodox Jewish population. Without going into too much detail, the communities that live in Monsey have lifestyles that differ from much of the US. For example, most of the people in the communities refrain from the use of electricity and walk to their synagogues during the weekly Sabbath. As such, there are over 100 synagogues in the area and the many rebbes are also leaders of these tight-knit communities. Further, m.\nMonsey and the surrounding hamlets also overlap with the East Ramapo Central School District. This school district is notable for the many lawsuits against it for not providing for the basic needs of the students. There was recently a lawsuit against the school district which cited to bloc voting by members of the communities to elect members of the school board who acted against the wishes of the Hispanic and Black students who attend the public school.\nThat said, this is an atypical region in many ways, including politically. While the petitioners contend that the existence of precincts with many, many fewer votes for Harris than Gillibrand, an alternative is that this is due to bloc voting. The idea would be that the local leaders, who are often close to families in the area due to their committed practice of their Orthodox Jewish faith, give strong suggestions of who to vote for. If this is the case, we would expect to see the precincts that are odd exhibit the following characteristics:\n\nBe entirely within the Orthodox Jewish neighborhoods, where there are no outsiders to add noise to the bloc.\nBe geographically located together, where drops in one election district are similar to the drops in the nearby districts.\n\nAs an important note, there are other groups in the US that vote in blocs. Yet, only in cases such as these do we have true geographic separation that could allow it to be seen. To detect such bloc voting, we would need something similar to these two characteristics above.\nAlternatively, if this was due to errors in voting machines, I would expect to see a pattern that’s somewhat random. Specifically, we should expect that random precincts, equally geographically dispersed have these large dropoffs.\nBelow, we can see a map of these dropoffs in raw votes. A score of -200 here means that Harris received 200 fewer votes than Gillibrand received. White lines and black text show the boundaries of the major villages within Ramapo. A small number of precincts that have (1) less than 10 votes for Harris, but (2) more than 100 votes for Gillibrand are labeled with white text.\n\n\n\nMap of Ramapo showing within-party dropoff in raw votes for Harris with Gillibrand as the baseline\n\n\nHere, we see a light pink through much of the town of Ramapo, consistent with only a small within-party dropoff. Darker colors indicate large amounts of within-party dropoff. The most extreme precincts in this regard are found within New Square and Kaser. To a local, this is unsurprising, as they are dense villages with large amounts of Orthodox Jewish people.\nWikipedia describes the two villages as follows:\n\nKaiser: “a village with a population consisting almost entirely of Hasidic Jews”\nNew Square: “Its inhabitants are predominantly members of the Skverer Hasidic movement who seek to maintain a Hasidic lifestyle disconnected from the secular world”\n\nGiven this, it seems likely that the surprising drop for Harris is due to bloc voting, based on how it geographically concentrates, rather than some widespread evidence of fraud.\nFinally, here is the same plot made looking at the within-party dropoff for Biden compared to Mondaire Jones in the US House race in 2020.\n\n\n\nMap of Ramapo showing within-party dropoff in raw votes for Biden with Jones as the baseline\n\n\nAgain, unsurprisingly, with a bit of knowledge about the local area, this makes sense with a bloc voting theory. The pattern replicates nearly identically in 2020 as the pattern in 2024. Just because a group of voters votes systematically does not mean that there is fraud. Simple, readily available information on the areas and their voting patterns shows that this is normal for where we are looking."
  },
  {
    "objectID": "posts/2025-06-17-rockland-sare/index.html#voter-affidavits-for-sare-than-recorded-votes-for-sare",
    "href": "posts/2025-06-17-rockland-sare/index.html#voter-affidavits-for-sare-than-recorded-votes-for-sare",
    "title": "Thinking SMARTer about election fraud in Rockland County, NY",
    "section": "Voter affidavits for Sare than recorded votes for Sare",
    "text": "Voter affidavits for Sare than recorded votes for Sare\nNearly all of this post has focused on the broader empirical claims and why they are not indicative of fraud. The last empirical piece is a series of affidavits claiming to have voted for Sare in the US Senate race. These are sworn and notarized, and as such provide evidence to the court.\nTo me, this seems analogous to eyewitness testimony. It may be the best you have, but it has to be taken with a grain of salt because recall is imperfect. Scholars have long studied the ability of voters to accurately recall when they made political decisions. To quote an abstract of a more recent paper, Durand, Deslauriers, and Valois (2015), “voting is not a salient, memorable behaviour for all voters.” Confusion about vote choice goes as far as the Cooperative Election Survey providing information on if they could validate that a voter even voted! This is to say that it would not be atypical for there to be recall issues.\nWith that in mind, we can look a bit at what data we can see about these sworn voters. The voter file in NY is publicly available. Using a copy dated 2025-06-16, I can locate all 16 sworn voters across the precincts. Each of them did vote in the 2024 general election (as expected). However, three of the 16 appear to have moved, as their sworn addresses do not match their current voter file addresses. Those three do not live in the reported precincts in my version of the voter file. If they did not move, then a simple error of matching voters to precincts could help mollify the differences between sworn and reported votes.\nThere are three affidavits featuring sworn Sare voters. Each of these is for voters in Ramapo, but I will focus on Ramapo 39 and Ramapo 62, where district-level affidavits were filed. Zooming into that area, we are in the midst of Sloatsburg and Montebello, with an unnamed region in between. These precincts are located below:\n\n\n\nMap of affidavit precincts around Sloatsburg and Montebello\n\n\nThe precincts shown in orange each have affidavits that at least 8 voters voted for Sare in Ramapo 39 and 4 voters voted for Sare in Ramapo 62. (A different filing indicates that there should be 5 affidavits for Sare in Ramapo 62, but I only see 4 filed and the map shows 4, so for now I’ll say 4. Across documents, one of the election district 62 affidavits is filed twice, which may explain this.)\nWhat do the election returns show? In Ramapo 39, there are 5 votes for Sare, 3 votes short. In Ramapo 62, there are 3 votes for Sare, 1 vote short.\nThese returns are fairly typical in the area, with Sare receiving only up to 5 votes per precinct in the area around her residence. The most votes she received in any precinct in Rockland was 8, with a total of 397 votes in her home county.\n\n\n\nMap of votes for Sare by precinct around Sloatsburg and Montebello\n\n\nCould there be an innocuous explanation? One idea would be that people voting for third-party candidates might mismark their ballots and unintentionally overvote if they are otherwise voting for a straight party line. Below, we see the overvotes, but these are rare and unlikely to be involved.\n\n\n\nMap of overvotes for Sare around Sloatsburg and Montebello\n\n\nUndervotes are quite common in this area. An undervote is simply leaving the line for that race blank. The median precinct had 23 undervotes. That’s to say that not voting for any candidate was much more popular than voting for Sare. Below, we can see a corresponding map of undervotes.\n\n\n\nMap of undervotes around Sloatsburg and Montebello\n\n\nAlso from the petition, there is a bit of description of how the sworn voters were found.\n\n\nUpon information and belief, Candidate Diane Sare is confident that she will find many more voters who voted for her, than were recorded by the Rockland County Board of Elections, as she found these voters, simply by going down her block and asking a few neighbors.\n\n\nLess as a political scientist and more as a person, I can sympathize with this. Imagine that your neighbor is running for the US Senate (or any other office for that matter). You get asked repeatedly to vote for them after years of hearing about their political beliefs. Eventually, you just say “sure, sure” knowing that the ballot is private. The election comes and you vote for someone else. Then, the candidate comes around and asks if you voted for them. You say “Yes, of course!” when you should have said “No, sorry…” because you gave up the fight a while ago already. Eventually, the candidate comes back to ask you to sign on paper that you voted for her, while you live that lie. Next thing you know it’s in a court case. That feels a bit like an episode of a sitcom.\nI do not mean to malign the sworn voters nor to expressly question any of their honesty, but it is just a human thing worth considering. They did swear that they did vote for Sare and that should not be thrown away lightly, but instead cautiously considered."
  },
  {
    "objectID": "posts/2025-06-17-rockland-sare/index.html#concluding-thoughts",
    "href": "posts/2025-06-17-rockland-sare/index.html#concluding-thoughts",
    "title": "Thinking SMARTer about election fraud in Rockland County, NY",
    "section": "Concluding thoughts",
    "text": "Concluding thoughts\nSocial science is actually quite hard. Election cases are especially difficult, largely by design. We have a secret ballot that ensures that you can go to the ballot box and profess your true intentions. This makes it so that we are almost always working on second-best data.\nHere, that second-best data is a collection of election returns and information from neighbors of a candidate who claim to have voted for her. Ideally, for data, we could have cast vote records. Instead, assessing the claims requires that we think calmly and carefully about the set of possible data generating processes. We should be open to credible evidence of election fraud, but should also expect that there is a bar of reasonable evidence necessary to reopen detailed election records and force recounts.\nThat said, the security of elections is very important. If there is credible evidence of fraud or other issues, we should absolutely investigate each and every case. Unfortunately, the evidence presented thus far from within the case is not very careful, nor very credible.\n\n\n\n\n\n\nNote\n\n\n\nAll of the data and code necessary for the production of this post are on GitHub."
  },
  {
    "objectID": "posts/2024-06-29-positron-settings/index.html",
    "href": "posts/2024-06-29-positron-settings/index.html",
    "title": "Settings, Keybindings, and Extensions for Positron",
    "section": "",
    "text": "This post introduces my initial set of settings, keybindings, and extensions for Positron. Positron is Posit’s data science oriented IDE built on Code - OSS. The open beta just began this week (at the time of writing). Most of my life is spent in RStudio, but I’ve been trying to use Positron to get a sense of it.\nBelow, I list out the settings, keybinds, and some details on extensions that I like so far. I plan to update this over time or make a gist, but the simplest thing to start was just make a short blog post.\nFor clarity, the exploration to get here is not random. I use RStudio as main my IDE, so I started with their keybindings discussed here. I then ported over random settings from VS Code, which I was using for typst and markdown editing, following the untimely demise of Atom. The rest of the settings are things I’ve found when Googling random bits of information about VS Code and how to fix things I disliked."
  },
  {
    "objectID": "posts/2024-06-29-positron-settings/index.html#extensions-that-i-tried-but-wasnt-convinced",
    "href": "posts/2024-06-29-positron-settings/index.html#extensions-that-i-tried-but-wasnt-convinced",
    "title": "Settings, Keybindings, and Extensions for Positron",
    "section": "Extensions that I tried but wasn’t convinced",
    "text": "Extensions that I tried but wasn’t convinced\n\nCode Spell Checker\nThis looks really helpful, but was flagging hundreds of false positive words when doing package development. There may be better settings for it, but there was way too much noise to be useful by default.\n\n\nSpell Right\nSpell check is hard, but I couldn’t get this extension to work at all. Everything resulted in a modal dialogue for an error, unfortunately. Similar to the other spellchecker, this might just be designed for a different use case than I have."
  },
  {
    "objectID": "posts/2023-12-21-cran-wrapped/index.html",
    "href": "posts/2023-12-21-cran-wrapped/index.html",
    "title": "Packages 2023 Wrapped",
    "section": "",
    "text": "With CRAN closing its submission queue for the holiday break tomorrow, it seems the right time to look over the past year. I maintain and contribute to quite a few packages on CRAN, primarily focused on social science data and methods. This year included adding 5 new packages to CRAN, with a minor shift towards tidy interfaces for web-based APIs. Below, I talk about some of the new packages, updates to existing packages, and then look at the downloads for all of my packages."
  },
  {
    "objectID": "posts/2023-12-21-cran-wrapped/index.html#new-packages",
    "href": "posts/2023-12-21-cran-wrapped/index.html#new-packages",
    "title": "Packages 2023 Wrapped",
    "section": "New packages",
    "text": "New packages\nI’ve added five new packages to CRAN this year.\n\napportion\n\n\n\napportion is a relatively simple package. It calculates apportionments, the allocation of seats to states based on population. It includes functions for the most common apportionment methods:\n\nthe Adams Method (app_adams())\nthe Balinski Young Method (app_balinski_young())\nthe Dean Method (app_dean())\nthe D’Hondt Method (app_dhondt())\nthe Hamilton-Vinton Method (app_hamilton_vinton())\nthe Huntington-Hill Method (app_huntington_hill())\nthe Jefferson Method (app_jefferson())\nthe Webster Method (app_webster())\n\n\n\ncrayons\n\n\n\ncrayons takes a few dozen packs of crayons and turns them into color palettes. The package itself is pretty thin, relying on scale_color_crayons() and scale_fill_crayons() to create the palettes.\n\nlibrary(ggplot2)\nlibrary(crayons)\n\nmpg |&gt;\n  ggplot() + \n  geom_point(aes(displ, hwy, colour = class)) + \n  scale_color_crayons(palette = 'original') + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\ngptzeror\n\n\n\nIn the peak of worries about students using ChatGPT, GPTZero offered an API for estimating if text was human or AI generated. I wrapped an R interface to this API in gptzeror. It seems to work somewhat well, but is far from perfect. The risk of false positives is really high, so I’m not using this without other substantial evidence.     \n\n\nfeltr\n\n\n\nfeltr is a package for working with the Felt. It covers all of the API endpoints, which lets you upload shapes from R directly to Felt. You can delete them, update them, extract comments, and more. I covered an application of this package in a prior blog post on loosely focused on Dunkin Donuts in Cambridge.      \n\n\nbskyr\n\n\n\nbskyr is a package for working with the Bluesky Social API. It’s focused largely on collecting tidy data from Bluesky. Given the decentralized nature of Bluesky, it seems like it has immense opportunity for social science research. You can design entire feeds as treatments, letting people push further in treatment arms without the same need for industry-academy partnerships as with Facebook or X/Twitter.\nOf course, it also contains all of the tools for posting and otherwise interacting with Bluesky. I even have a small bot going which tracks CRAN Updates that I’ll cover soon in a holiday-times blog post. It’s run entirely through bskyr, which has been working even better than expected."
  },
  {
    "objectID": "posts/2023-12-21-cran-wrapped/index.html#updates-to-existing-packages",
    "href": "posts/2023-12-21-cran-wrapped/index.html#updates-to-existing-packages",
    "title": "Packages 2023 Wrapped",
    "section": "Updates to existing packages",
    "text": "Updates to existing packages\nMy CRAN updates have not all been new packages, I also maintain a handful of packages. This year, I’ve made 14 submissions across 10 packages. 7 of these are related to the 5 new packages above (5 first submission + 2 updates).\n\n\n\n\n\n\n\n\nName\nVersion\nDate\nTitle\n\n\n\n\ncvap\n0.1.3\n2023-03-17\nCitizen Voting Age Population\n\n\nredist\n4.1.0\n2023-03-19\nSimulation Methods for Legislative Redistricting\n\n\nredist\n4.1.1\n2023-04-03\nSimulation Methods for Legislative Redistricting\n\n\ngeomander\n2.2.1\n2023-04-16\nGeographic Tools for Studying Gerrymandering\n\n\ncvap\n0.1.4\n2023-07-01\nCitizen Voting Age Population\n\n\ntinytiger\n0.0.8\n2023-10-17\nLightweight Interface to TIGER/Line Shapefiles\n\n\nredistmetrics\n1.0.7\n2023-12-12\nRedistricting Metrics\n\n\n\n\n\n\n\nUpdates to cvap and tinytiger added support for new years of Census Bureau data. redist and redistmetrics each saw primarily bug fixes and performance improvements, without any major changes. geomander similar saw mostly bug fixes. Its one update also drastically cleaned up the dependencies to make the package easier to install."
  },
  {
    "objectID": "posts/2023-12-21-cran-wrapped/index.html#footnotes",
    "href": "posts/2023-12-21-cran-wrapped/index.html#footnotes",
    "title": "Packages 2023 Wrapped",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDear Spotify, please do something like this next year to account late releases like 1989 in October.↩︎\nContributions welcome!↩︎"
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html",
    "title": "Creating Quarto Journal Article Templates",
    "section": "",
    "text": "This post is largely written for my future self. If you’re not me and you’ve ended up here, I do hope that this is helpful for you, but it may fall below or above your current experience. I’ll include informative links as I think of them to help with the latter. For the former, this post may be too specific to my process to be helpful.\n\n\nAs with all blog posts, I’m assuming you both know everything and nothing. You have every clue how Quarto, Markdown, LaTeX, pandoc, and Lua work and simultaneously have never used anything but Microsoft Word.\nMore realistically, you need some experience with one of Quarto, RMarkdown, or Markdown and definitely some experience with LaTeX.\nYou needn’t be an expert on the first part, but if you’re new to LaTeX, take some time to write a few documents before trying to build templates that rely on it for pdf-output.\n\n\n\nI’ve made a handful of templates for Quarto and RMarkdown. A few of them are public, but many of them are hacky one-off solutions to some problem. I’m focused here on the more-focused public template, where you want it to work in general for a journal’s set of problems.\nPerhaps most importantly, I’ve got about eight years of self-taught LaTeX experience. That’s good for me and bad for you (or maybe good for you and bad for me). My solutions to problems often (1) produce correct output and (2) are sub-optimal. That’s not the best, but if you care about (1) more, then, hey, we’ve got some work to do.\nFor full context: This post was mostly written while I was making a PNAS template for Quarto. Previously, I used the rticles::pnas_template() but (at the time of writing) I was trying to get a better sense of how Quarto built things. So, I have a sense of what it should look like, but no guarantee of understanding how the details work."
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#background",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#background",
    "title": "Creating Quarto Journal Article Templates",
    "section": "",
    "text": "This post is largely written for my future self. If you’re not me and you’ve ended up here, I do hope that this is helpful for you, but it may fall below or above your current experience. I’ll include informative links as I think of them to help with the latter. For the former, this post may be too specific to my process to be helpful.\n\n\nAs with all blog posts, I’m assuming you both know everything and nothing. You have every clue how Quarto, Markdown, LaTeX, pandoc, and Lua work and simultaneously have never used anything but Microsoft Word.\nMore realistically, you need some experience with one of Quarto, RMarkdown, or Markdown and definitely some experience with LaTeX.\nYou needn’t be an expert on the first part, but if you’re new to LaTeX, take some time to write a few documents before trying to build templates that rely on it for pdf-output.\n\n\n\nI’ve made a handful of templates for Quarto and RMarkdown. A few of them are public, but many of them are hacky one-off solutions to some problem. I’m focused here on the more-focused public template, where you want it to work in general for a journal’s set of problems.\nPerhaps most importantly, I’ve got about eight years of self-taught LaTeX experience. That’s good for me and bad for you (or maybe good for you and bad for me). My solutions to problems often (1) produce correct output and (2) are sub-optimal. That’s not the best, but if you care about (1) more, then, hey, we’ve got some work to do.\nFor full context: This post was mostly written while I was making a PNAS template for Quarto. Previously, I used the rticles::pnas_template() but (at the time of writing) I was trying to get a better sense of how Quarto built things. So, I have a sense of what it should look like, but no guarantee of understanding how the details work."
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#making-the-template",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#making-the-template",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Making the Template",
    "text": "Making the Template\nFirst things first, we need to initialize the template. We can do that in the terminal with:\nquarto create extension journal\nIt’ll prompt you to give it a name. Names tend to be short and normally match a common journal abbrevation. Quarto has a list of templates, as does Awesome Quarto. Some names I’ve used:\n\n\n\njournal\nname\n\n\n\n\nAPSR\napsr\n\n\nPNAS\npnas\n\n\nPA, PSRM, BJPS, …\ncambridge-medium\n\n\nScientific Data\nscientific-data\n\n\n\nAs you can see, some are obvious. The Cambridge medium one tripped me up a bit, as it was intended for at least five journals. As such, I just named it for the template and added text to make it clear what journals it was for.\nNow, once you have a name, it creates a new R project named for the name you provided. You can open that and give the structure a look.\n\n\n\n\n\n\nNote\n\n\n\nFor the rest of this post, I’ll use building a PNAS template as the example. pnas can be replaced with the name you chose above.\n\n\nOn creation, the file structure should look something like this:\n.\n├── bibliography.bib\n├── pnas.Rproj\n├── README.md\n├── template.qmd\n└── _extensions\n    └── pnas\n        ├── header.tex\n        ├── pnas.lua\n        ├── styles.css\n        └── _extension.yml\nFor a journal template, we’re going to have to augment the _extensions/pnas folder. For now some quick bits of information on included files:\n\nREADME.md is the general readme for the whole template. It’ll hold installation instructions, option documentation, and anything else you need users to know.\ntemplate.qmds is where our example template goes.\nbibliography.bib is a placeholder bibtex file.\n_extensions/pnas/_extension.yml will hold the metadata for the template. This tells Quarto which files to use and such.\n\n_extensions/pnas/header.tex is included in the default template and will be inserted in the header of the LaTeX template.\n\n_extensions/pnas/styles.css can probably be deleted at this point, unless you want to make a website version of the template, but you probably aren’t if you’re making a journal template.\n_extensions/pnas/pnas.lua is the default Lua filter. Lua is powerful, but only necessary for more complicated features.\n\n\nThe README\nThe first thing I edit is the README. It can’t be finished yet, but I find it helpful to organize right away.\nUsing RStudio’s find+replace, I change the github organization placeholder to my GitHub account name (christopherkenny). Then, using the same tool, I replace the file name and title with template name from before. (If you now feeling anxious about the name, this is the right time to fix it before doing anything.) Then, I remove the (now complete) TODOs for those.\nIn the “Example” section, I then start to add more info. First, I add some template code to include a screenshot.\n&lt;!-- pdftools::pdf_convert('template.pdf',pages = 1) \n![[template.qmd](template.qmd)](template_1.png) --&gt;\nOnce the template is finished, I’ll run the first line in R and uncomment the second. Including a screenshot is inspired by Cory McCartan’s pre-print template. I think it’s a nice touch.\nThe options section is helpful for the end user, but may not be super clear right now. I think it is best to hold off on that and figure it out later.\nFor now, the last thing I’ll do is add a bit of information on licensing. Odds are good that I’m working off of some official or officially endorsed template. Even if it isn’t official, you want to give credit to the LaTeX wizardry going on behind the scenes. My license section typically takes the form, using {glue} syntax for placeholders:\n## License\nThis modifies the {Owner} {What} Template, available at &lt;{some_link}&gt;.\nThe original template is licensed under the [{license_name}]({license_link}).\n{any_other_required_notes_about_modifications}"
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#adding-latex-files-into-the-template",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#adding-latex-files-into-the-template",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Adding LaTeX Files into the Template",
    "text": "Adding LaTeX Files into the Template\nBefore you dive into building the Quarto part of the template, I’ll next download any relevant files for the LaTeX part of the template. For the PNAS template, I downloaded the source files from Overleaf.\nFrom the template, I extract all of the LaTeX-related files (.bst, .sty, .cls, and in this case .ldf files). Those files go into the _extensions/pnas folder. Any similar files that we want to be available to the template itself should go there.\nAny files that are necessary for demonstrating the template can go into a higher-level folder. For example, in the PNAS template, they use a frog as the placeholder image. I make a folder called figs/ that can hold such things.\nAt this point, my file structure looks like:\n.\n├── bibliography.bib\n├── figs\n│   └── frog.pdf\n├── pnas.Rproj\n├── README.md\n├── template.qmd\n└── _extensions\n    └── pnas\n        ├── header.tex\n        ├── jabbrv-ltwa-all.ldf\n        ├── jabbrv-ltwa-en.ldf\n        ├── jabbrv.sty\n        ├── pnas-new.bst\n        ├── pnas-new.cls\n        ├── pnas.lua\n        ├── pnasresearcharticle.sty\n        ├── styles.css\n        └── _extension.yml\nPutting the files here only does so much. We have to let the template know that we have them. This all happens in _extensions/pnas/_extension.yml\nThere are a pair of lines in the file telling it what to use for PDFs.\n    pdf:\n      include-in-header: header.tex\nWe’ll add some new instructions for the format-resources listing out the new files that we have.\n    pdf:\n      include-in-header: header.tex\n      format-resources:\n        - pnas-new.cls\n        - pnas-new.bst\n        - pnasresearcharticle.sty\n        - jabbrv.sty\n        - jabbrv-ltwa-all.ldf\n        - jabbrv-ltwa-en.ldf\n\nEditing template.qmd (Part 1: The YAML)\nOnce we’ve got all the files in place, it’s time to make some edits to template.qmd so that we can render it and see how we’re doing.\nThe first big part is setting up the Quarto YAML. This is where all of the information that we will later use in the template will go.\nThe first few lines of the document are standard, but we’ll make two changes to the below:\ntitle: Pnas Template\nformat:\n  pnas-pdf:\n    keep-tex: true  \n  pnas-html: default\nFirst, I’ll give it a more fun title, here “Quarto Template for PNAS Submissions”. Second, I’m focused on making the pnas-pdf class, so I’ll remove the pnas-html: default line from line 6 (or so) of the YAML. For journals that need PDFs for submission, the extra type doesn’t seem worth the effort.\n\nAuthors and Affiliations YAML\nNot all templates use all possible information about authors. But, there is a pretty extensive set of possible pieces of information.\nWe need to include every relevant piece of information that the journal template will use. If we don’t include everything, there might be weird empty spaces. If we over include, that’s okay, but some things will get ignored.\nUnlike the rticles templates that RMarkdown used to use, Quarto has tried to build an exhaustive schema for people’s names and their affiliations. The schema are detailed extensively at https://quarto.org/docs/journals/authors.html.\nIf we peek at some recent PNAS articles, we can see what author and affiliation information are used.\nAuthors look something like\n\nAuthor One\\(^{a, c, 1}\\), Author Two\\(^{b, 1, 2}\\), and Author Three^\\(a\\)\n\nwhich is kind of a mess.\n1 indicates that the authors contributed equally. 2 indicates the corresponding author. a,b,c are all affiliations.\nAffiliations themselves are pretty standard here. They should get printed to look like\nUniversity, Department, City, State ZIP\nWith that much information, we can assemble the YAML for an author. To make sure we can see the difference in these, I’ll do set up the affiliations as:\n\na: Harvard University, Department of Government, Cambridge, MA 02138\nb: Yale University, Department of Political Science, New Haven, CT 06511\nc: Harvard University, Department of Statistics, Cambridge, MA 02138\n\nauthor:\n  - name: Author One\n    affiliations:\n      - name: Harvard University\n        id: a\n        department: Department of Government\n        city: Cambridge\n        state: MA\n        postal-code: 02138\n      - name: Harvard University\n        id: c\n        department: Department of Statistics\n        city: Cambridge\n        state: MA\n        postal-code: 02138\n    attributes:\n      equal-contributor: true\nNote here that Author One and Author Two contributed equally and Author Two is the corresponding author.\nWith that, we can build out the other authors.\n  - name: Author Two\n    affiliations:\n      - name: Yale University\n        id: b\n        department: Department of Political Science\n        city: New Haven\n        state: CT\n        postal-code: 06511\n    attributes:\n      equal-contributor: true\n      corresponding: true\n  - name: Author Three\n    affiliations:\n      - ref: a\nBuilding out Author Two’s affiliation is very similar, we just add a corresponding: true under attributes along with their information. For Author Three, we can do a cool thing and reference affiliation a, since it’s the same.\nNow at this point, I know I’ll also need the email for the corresponding author, so I’ll add an email: corresponding@email.com line to the Author Two chunk.\n\n\nCustom YAML Components\nThe rest of the YAML varies immensely by journal. To figure out what we need, we have to look back at the LaTeX template. We want to identify anything where we either need to tell the template about options to use or replace filler text. To that point, I’ve pulled out each thing that I see as something we need to be controllable in the YAML.\n\\templatetype{pnasresearcharticle} % Choose template\n% {pnasresearcharticle} = Template for a two-column research article\n% {pnasmathematics} %= Template for a one-column mathematics article\n% {pnasinvited} %= Template for a PNAS invited submission\nIt looks like we need a template_type argument to the YAML. Later, we can then list the options in the README.\n\npnasresearcharticle: Template for a two-column research article\npnasmathematics: Template for a one-column mathematics article\npnasinvited: Template for a PNAS invited submission\n\nWe should be aware though that these actually have different official templates. This gives an important choice:\n\ntry to support it\nlet it be set but make it clear that the template is optimized for pnasresearcharticle types\ndon’t make it settable and just always use pnasresearcharticle\n\nI tend to go for (2) since it might work really easily without changing anything. Aiming for (1) is commendable, but may open a new can of worms. Option (3) is really safe and might actually be the best option, but if it at least mostly works, then someone else trying to do the other type now has to start from scratch when they didn’t have to. Part of how I’ll approach this is to make it the default in LaTeX if nothing is specified.\n\\leadauthor{Lead author last name}\nThis is a pretty common request, as it’s used in the header of pages. To be consistent with other journal types, such as apsr, I’ll call this field runningauthor. It’s always better to be consistent, since it makes switching easier, but if I didn’t know about the other name, I probably would have called it lead_author.\n\\significancestatement{Authors must submit a 120-word maximum statement about the significance of their research paper written at a level understandable to an undergraduate educated scientist outside their field of speciality. The primary goal of the significance statement is to explain the relevance of the work in broad context to a broad readership. The significance statement appears in the paper itself and is required for all research papers.}\nFor this, we can just give a new argument called significance, since it is just a bunch of text.\n\\authorcontributions{Please provide details of author contributions here.}\nFor this, we can also make a new argument called author_contributions. It’s a bunch of text and readable is best.\n\\authordeclaration{Please declare any competing interests here.}\nI tend to call this conflict_of_interest, as I’ve seen it that way in the old rticles days.\n\\keywords{Keyword 1 $|$ Keyword 2 $|$ Keyword 3 $|$ ...}\nKeywords come up in many templates and are included by default in the automatically generated YAML as keywords.\n\\doi{\\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}\nThis one looked like something that we might want to change, but is probably best to leave for now. With how publications and submissions work for PNAS, you won’t know the doi until after you’re done using the template.\n\\firstpage{12}\n% Use \\firstpage to indicate which paragraph and line will start the second page and subsequent formatting. In this example, there are a total of 11 paragraphs on the first page, counting the first level heading as a paragraph. The value {12} represents the number of the paragraph starting the second page. If a paragraph runs over onto the second page, include a bracket with the paragraph line number starting the second page, followed by the paragraph number in curly brackets, e.g. \"\\firstpage[4]{11}\".\nNow this one presents an interesting question. We could include a first_page argument. Though, the formatting is kinda tricky. We would need to support both \\firstpage{12} and \\firstpage[4]{11}. It seems to me that this would be a good thing to leave out of the YAML but use within the template.qmd, so that it’s obvious it should be used but doesn’t require multi-option handling. That is my preference and I can’t say for sure if this is best practice.\n\\acknow{Please include your acknowledgments here, set in a single paragraph. Please do not include any acknowledgments in the Supporting Information, or anywhere else in the manuscript.}\nWe can give this a name like acknowledgements, again looking to other templates to see what they use as names, since this is a pretty common thing to have.\nOkay now that gives us pretty set of YAML option to add. They’ll look like:\nrunningauthor: \"One, Two, and Three\"\nsignificance: |\n  Authors must submit a 120-word maximum statement about the significance of their research paper written at a level understandable to an undergraduate educated scientist outside their field of speciality. The primary goal of the significance statement is to explain the relevance of the work in broad context to a broad readership. The significance statement appears in the paper itself and is required for all research papers.\nauthor_contributions: \"Please provide details of author contributions here.\"\nconflict_of_interest: \"Please declare any competing interests here.\"\nkeywords: [template, demo]\nacknowledgements: | \n  Please include your acknowledgments here, set in a single paragraph. Please do not include any acknowledgments in the Supporting Information, or anywhere else in the manuscript.\nAs you can see, where applicable, I’m using the instructions as the placeholder text where applicable. This has the benefit of keeping the end-user instructions near the end user without them having to deep dive into the original LaTeX template.\nThe bad news: that was the easy part. The good news: the next part walks you through the hard part.\n\n\n\n\n\n\nUse Git\n\n\n\nIf you aren’t already using Git at this point, now is a good time to run usethis::use_git(), initialize a repo, and commit what you’ve done. This is a good save point so that if you start playing with things for the next part, you can easily remove them if you break anything.\n\n\n\n\n\nUsing the YAML in LaTeX\nAdding the arguments to the YAML doesn’t automatically change anything about the output. It makes variables available to Pandoc’s processing as the name of the YAML set. So, for example, if our YAML looked like:\ntitle: Quarto Template for PNAS Submissions\nthen a variable called title would be available for us to use. We can extract its value with $title$.\nPossibly the most common is to check if a value is set and then use it if it is. For something like title, where we want to pass it to the LaTeX function \\title{}, we can do this with the following pattern.\n$if(title)$\n\\title{$title$}\n$endif$\nThe first and third lines here are how if statements are done. So, if the title is set then in your YAML, then the outputted .tex file will say:\n\\title{Quarto Template for PNAS Submissions}\nFor more details on how these types of variables work, take a look at Template syntax section of the Pandoc manual.\nMost of the templating from here is going to be finding the correct place to evaluate a variable. To find those places, we have to introduce the idea of partials, which are short files that get injected into the bigger template.\n\nLaTeX Partials\nPartials represent little snippets of LaTeX that get joined in a specific order. Quarto is built off of a series of default partials. If you’re rendering a regular document without any of this templating, it passing through those. The defaults are very good: they cover lots of cases and do them in a smart, efficient way. The ideal is that you only have to change a small number of partials. Journals have all sorts of format choices that you want to match, so we will have to replace some.\nAs assumed before, you know some LaTeX. A familiar, stylized LaTeX document then looks something like:\n% first line is the *doc-class*\n\\documentclass{article}\n\n% then we have the header\n% like where we load packages with \\usepackage{}\n\n% This next group of lines are the *title*\n\\title{Some Latex Document}\n\\author{Christopher Kenny}\n\\date{July 2023}\n\n\\begin{document}\n\n% then we have *before-body*\n\\maketitle\n\nThe body of the document\n\n% then we have *after-body*\n\n% then we have the *before-bib*\n\nThe references\n% then we have *biblio* which makes the bibliography\n\n\\end{document}\nEach of the things in *s is a partial and the general place where it goes. There are more partials, like toc, a table of contents partial. There’s also a partial for pandoc, for some things that Pandoc needs for rendering from LaTeX. Odds are good that you don’t have to touch the Pandoc one!\nA full description of Quarto partials is available in the Quarto documentation. This lists out all of the partials and a short description of what they do. The source files for the partials are on GitHub.\nAny partials that we need will live in a partials folder, below where the _extension.yml folder lives. Each partial will be a .tex file. So, they will all be something like: _extensions/pnas/partials/*.tex. Each file that we add to this folder has to be listed in _extension.yml, which I’ll show below.\nWith the general idea of partials down, we can go in order from the top down and see what we need.\n\ndoc-class.tex\nOkay, so first things first is the document class. In the template, it looks like this:\n\\documentclass[9pt,twocolumn,twoside]{pnas-new}\nArguably, we don’t need to change this one out. This whole partial will evaluate to one line. The default looks like this:\n\\documentclass[\n$if(fontsize)$\n  $fontsize$,\n$endif$\n$if(papersize)$\n  $papersize$paper,\n$endif$\n$if(beamer)$\n  ignorenonframetext,\n$if(handout)$\n  handout,\n$endif$\n$if(aspectratio)$\n  aspectratio=$aspectratio$,\n$endif$\n$endif$\n$for(classoption)$\n  $classoption$$sep$,\n$endfor$\n]{$documentclass$}\nI say that we don’t need to necessarily change this one because we could pass the following to the YAML.\nfontsize: \"9pt\"\nclassoption:\n - twocolumn\n - twoside\ndocumentclass: \"pnas-new\"\nThis would do the following:\n\n$if(fontsize)$ is true, so return the fontsize (9pt) and include the ,.\n$if(papersize)$ isn’t specified, so it’s false and nothing happens.\n$if(beamer)$ isn’t specified, so it’s false and nothing happens.\n$if(handout)$ isn’t specified, so it’s false and nothing happens.\n$if(aspectratio)$ isn’t specified, so it’s false and nothing happens.\n\nThen we get a for loop. This works like the if syntax from before. It takes each element of classoption (a length two vector with twocolumn and twoside), returns them followed by the seperator. The $sep$, $endfor$ syntax just says “hey this is the separator ,, a comma followed by a space.”\nFinally, document class would be pnas-new. So that completes it as\n\\documentclass[9pt,twocolumn,twoside]{pnas-new}\nThere are at least three advantages to implementing this simple partial as a custom partial. First, we can hard code the documentclass to be just the class we want to support. This avoids weird errors related to incorrect class specifications by downstream users. Second, if we decide that something is important enough to elevate to an additional YAML argument, we can then do so and add it with a little if syntax. For example, the APSR has a special nonblind argument that is really important for the submission. Knowing that, I could then make it a YAML option here. Third, we can remove some of the arguments that aren’t relevant to our template, like aspectratio.\nWith that in mind, I’ll first trim the options:\n\\documentclass[\n$if(fontsize)$\n  $fontsize$,\n$endif$\n$for(classoption)$\n  $classoption$$sep$,\n$endfor$\n]{$documentclass$}\nThen I’ll hard code the documentclass\n\\documentclass[\n$if(fontsize)$\n  $fontsize$,\n$endif$\n$for(classoption)$\n  $classoption$$sep$,\n$endfor$\n]{pnas-new}\nAt this point, I’ll reopen the template.qmd file and add to the YAML the relevant options for what we were just looking at.\nfontsize: \"9pt\"\nclassoption:\n - twocolumn\n - twoside\nThat’s it for that file. Now we just have to let the _extension.yml file know we have it, like we did for the class files above.\nLook for the lines we edited before and below it we’ll add a template-partials section to indicate that we have this file.\n    pdf:\n      include-in-header: header.tex\n      format-resources:\n        - pnas-new.cls\n        - pnas-new.bst\n        - pnasresearcharticle.sty\n        - jabbrv.sty\n        - jabbrv-ltwa-all.ldf\n        - jabbrv-ltwa-en.ldf\n      template-partials:\n        - \"partials/doc-class.tex\"\n\n\ntitle.tex\nNow, the title block definitely needs work for this example and probably for the majority of cases. The title partial will tell the document how to create the title, identify the authors, and identify anything else important that needs to be specified at the start of the document. For the PNAS template, we need to tell it how to:\n\nSpecify the \\templatetype{}\nCall \\title{}\nIdentify the authors (this one is the hardest!)\nCall \\leadauthor{}\nCall \\significancestatement{}\nCall \\authorcontributions{}\nCall \\authordeclaration{}\nIdentify equal authors\nIdentify corresponding authors\nSpecify the keywords\n\nFor the template type, we want to use an if-else statement. This lets us specify the default option for the case where no type was set. So, since there is an else, we can put it all directly inside the call to \\templatetype{}. This will look something like:\n\\templatetype{\n$if(template_type)$\n$template_type$\n$else$\npnasresearcharticle\n$endif$}\nwhere $else$ is how we specify the else part of the if-else.\nThe next part of this is going to be familiar. If there’s a title, we want to write the title.\n$if(title)$\n\\title{$title$}\n$endif$\nIf we hold onto the author pieces for a second, we can repeat that pattern for each of \\leadauthor{},\\significancestatement{},\\authorcontributions{}, and \\authordeclaration{}.\n$if(runningauthor)$\n\\leadauthor{$runningauthor$}\n$endif$\n\n$if(significance)$\n\\significancestatement{$significance$}\n$endif$\n\n$if(author_contributions)$\n\\authorcontributions{$author_contributions$}\n$endif$\n\n$if(conflict_of_interest)$\n\\authordeclaration{$conflict_of_interest$}\n$endif$\nNow back to the authors and affiliations. This is often the hardest part of making a Quarto template for journals. As a reminder, we want the output to look like:\n\\author[a,c,1]{Author One}\n\\author[b,1,2]{Author Two}\n\\author[a]{Author Three}\n\n\\affil[a]{Affiliation One}\n\\affil[b]{Affiliation Two}\n\\affil[c]{Affiliation Three}\nLet’s break apart what we need for Author One.\n\nA name\nAffiliation with a\n\nNeed to make the \\affil for a\n\nAffiliation with b\n\nNeed to make that \\affil for b too\n\na 1 to indicate joint first authorship\n\nWe’ll fill that part out in a minute.\n\n\nIf we look at the normalized schema for author first, we can figure out how to access it. The relevant part of the YAML looks like:\nauthor:\n  - id: string\n    number: number\n    name:\n      given: string\n      family: string\n      literal: string\nHow do we get the value? We can build it like so\n\nit’s part of the author: $author.\n\nit’s part of the name: name.\n\nwe want the full thing and that’s it literal$\n\n\n\nSo going down the little tree, we get that it is \\(author.name.literal\\). If we needed something like the first name, it would be \\(author.name.given\\).\nSo, we can throw that into:\n\\author[...]{$author.name.literal$}\nleaving the ... for this next step.\nWe need to extract the IDs for the affiliations for each person, so we’ll need to iterate. To iterate over the affiliations of each author, we can use the by-author iterator. This will let us loop over each author and grab the relevant values.\nIf we just needed the authors, then this would look like:\n$for(by-author)$\n\\author{$by-author.name.literal$}\n$endfor$\nBut, we need to fill in the [...] from above. To start, let’s do the affiliation ids. If they have affiliations, we want to loop over them.\n$for(by-author)$\n\\author[\n$if(by-author.affiliations)$\n$for(by-author.affiliations)$\n$it.id$\n$sep$,\n$endfor$\n$endif$\n]{$by-author.name.literal$}\n$endfor$\nAs you can see, I’ve introduced the special keyword it. It’s essentially a placeholder for the current iterator. Here, that’s each entry of by-author.affiliations and we are accessing the .id from them. We’re separating them with ,s so that they evaluate nicely.\nThe spacing that this outputs is a little funny, so we can collapse it to be\n$for(by-author)$\n\\author[$if(by-author.affiliations)$$for(by-author.affiliations)$$it.id$$sep$,$endfor$$endif$%\n]{$by-author.name.literal$}\n$endfor$\nWe can use the % just as in LaTeX for the normal end of line behavior.\nNow this all works beautifully for the affiliations, but we also need to think about the \\(1\\) and \\(2\\). \\(1\\) means equal first authorship if there are multiple first authors. \\(2\\) means corresponding author assuming that there are multiple first authors. So, \\(1\\) isn’t always there but \\(2\\) should be. Now, we can back out that if there are multiple first authors, then the author listed first will be a first author.\nThis is good news! This means we can check if there is an equal contribution note. We can do this with our good friend LaTeX, by defining a new command for this. If there’s an equal contributor, it’ll always be 1. We can make the corresponding variable vary based on the equal contributor variable.\n\\newcommand{\\equalcont}{1}\n\n$if(equal-contributor)$\n\\newcommand{\\correspond}{2}\n$else$\n\\newcommand{\\correspond}{1}\n$endif$\nWe can then use that in our templating. If an author is listed as an equal author, we want to add a ,\\equalcont after the affiliations. So, we can do just that by adding in another line.\n$for(by-author)$\n\\author[$if(by-author.affiliations)$$for(by-author.affiliations)$$it.id$$sep$,$endfor$$endif$%\n$if(by-author.attributes.equal-contributor)$,\\equalcont$endif$%\n]{$by-author.name.literal$}\n$endfor$\nIn the same way, we can add information about corresponding authors. If an author is listed as the corresponding author, we want to add a ,\\correspond after the affiliations.\n$for(by-author)$\n\\author[$if(by-author.affiliations)$$for(by-author.affiliations)$$it.id$$sep$,$endfor$$endif$%\n$if(by-author.attributes.equal-contributor)$,\\equalcont$endif$%\n$if(by-author.attributes.corresponding)$,\\correspond$endif$%\n]{$by-author.name.literal$}\n$endfor$\nNow, we can turn to setting up the affiliation lines, with \\affil. As with authors, there is a by-affiliation keyword that will let us iterate over the affiliations. For each affilition, we want something of the form:\n\\affil[a]{Affiliation One}\nAt the most basic level, if it was just a name (like “Harvard University”), we could use a loop and the it syntax again to make something like this:\n$for(by-affiliation)$\n\\affil[$it.id$]{$it.name$}\n$endfor$\nWe need it to do a teeny bit more though. We want something instead that uses all of the relevant pieces of the YAML to say:\nHarvard University, Department of Government, Cambridge, MA 02138\nThe components of this are then:\nname, department, city, state, postal-code\nFor each affiliation, we would want something like:\n$if(it.name)$$it.name$$endif$ \n$if(it.department)$, $it.department$$endif$ \n$if(it.city)$, $it.city$$endif$ \n$if(it.state)$, $it.state$$endif$ \n$if(it.postal-code)$, $it.postal-code$$endif$ \n$if(it.country)$, $it.country$$endif$\nNote that we’re separating them with , at the start of each additional argument. This avoids weird spacing in the output. And we should probably add in country at the end, for if not all authors work in the US.\n$if(it.country)$, $it.country$$endif$\nWe can put this all in its own file, called _affiliation.tex in the partials/ directory. We don’t have to, but it seems to be common practice in existing templates. Then, we can call this chunk using $_affiliation.tex()$. The benefit of doing that is it keeps the template cleaner and easier to debug later.\n$for(by-affiliation)$\n\\affil[$it.id$]{$_affiliation.tex()$}\n$endfor$\nDon’t forget to add the partials/_affiliation.tex line to the _extension file. That will now have a template-partials chunk like so:\n      template-partials:\n        - \"partials/doc-class.tex\"\n        - \"partials/_affiliation.tex\"\n        - \"partials/title.tex\"\nNow, a minute ago, we did the hard work for the equal contributors, so we can add in some code to indicate the equal authors too in the template:\n$if(equal-contributor)$\n\\equalauthors{\\textsuperscript{\\equalcont} $equal-contributor$}\n$endif$\nWe add the superscript because we had just a raw number above.\nNow, we can do something similar for the corresponding author. This time we’ll iterate over the authors, again with by-author to find the corresponding one. If they’re the corresponding author, we’ll fill in the ... below:\n$for(by-author)$\n$if(by-author.attributes.corresponding)$\n\\correspondingauthor{...}\n$endif$\n$endfor$\nJust as with the equal contributors, we want to start it with the identifier, \\textsuperscript{\\correspond}. Then we need the PNAS template text: “To whom correspondence should be addressed. E-mail:”. And last, we can get the email from the author with $by-author.email$.\nPut together, that looks like:\n$for(by-author)$\n$if(by-author.attributes.corresponding)$\n\\correspondingauthor{\\textsuperscript{\\correspond}To whom correspondence should be addressed. E-mail: $by-author.email$}\n$endif$\n$endfor$\nFinally, we can build out the keywords. Since there can be multiple, we’ll use a for loop with a separator, like in Section 3.2.1.1. We want to iterate over each one and print them separated by a pipe, |.\n$if(keywords)$\n\\keywords{$for(keywords)$$keywords$$sep$ | $endfor$}\n$endif$\nAll together, the file looks something like:\n\\templatetype{$if(template_type)$$template_type$$else$pnasresearcharticle$endif$}\n\n$if(title)$\n\\title{$title$}\n$endif$\n\n\\newcommand{\\equalcont}{1}\n\n$if(equal-contributor)$\n\\newcommand{\\correspond}{2}\n$else$\n\\newcommand{\\correspond}{1}\n$endif$\n\n$for(by-author)$\n\\author[$if(by-author.affiliations)$$for(by-author.affiliations)$$it.id$$sep$,$endfor$$endif$%\n$if(by-author.attributes.equal-contributor)$,\\equalcont$endif$%\n$if(by-author.attributes.corresponding)$,\\correspond$endif$%\n]{$by-author.name.literal$}\n$endfor$\n\n$for(by-affiliation)$\n\\affil[$it.id$]{$_affiliation.tex()$}\n$endfor$\n\n$if(runningauthor)$\n\\leadauthor{$runningauthor$}\n$endif$\n\n$if(significance)$\n\\significancestatement{$significance$}\n$endif$\n\n$if(author_contributions)$\n\\authorcontributions{$author_contributions$}\n$endif$\n\n$if(conflict_of_interest)$\n\\authordeclaration{$conflict_of_interest$}\n$endif$\n\n$if(equal-contributor)$\n\\equalauthors{\\textsuperscript{\\equalcont} $equal-contributor$}\n$endif$\n\n$for(by-author)$\n$if(by-author.attributes.corresponding)$\n\\correspondingauthor{\\textsuperscript{\\correspond}To whom correspondence should be addressed. E-mail: $by-author.email$}\n$endif$\n$endfor$\n\n$if(keywords)$\n\\keywords{$for(keywords)$$keywords$$sep$ | $endfor$}\n$endif$\nNote that it will eventually be processeed from top to bottom, so we have to define the LaTeX commands before we use them.\n\n\nbefore-body.tex\nOkay, with the hardest part done, we can do this one without too much crazy stuff. This partial is all the stuff that comes after \\begin{document} but before the writing. So, that normally includes making the title and the abstract. As you’ll see in a minute, there’s also some generic LaTeX from the template that we want here.\nFirst, if there’s a title we have to tell it to make it. This looks like much of how we built out the title section.\n$if(title)$\n\\maketitle\n$endif$\nNext, if there’s an abstract, we need to make that. We also need it to be in an abstract environment, so we can toss the whole thing into one if statement. For what it’s worth, this chunk goes in pretty much every template. I just copy the same chunk from old Quarto template to new Quarto template whenever I need it.\n$if(abstract)$\n\\begin{abstract}\n$abstract$\n\\end{abstract}\n$endif$\nAnd really both of these two things are handled by the default partial. I would have preferred to not change it, but we need some defaults to be set here that we see in the template itself. We can just straight up copy those into it. No edits necessary. If you go the route to allow setting the doi, perhaps with a YAML doi:, this is where you would insert that.\n\\dates{This manuscript was compiled on \\today}\n\\doi{\\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}\n\n\\thispagestyle{firststyle}\n\\ifthenelse{\\boolean{shortarticle}}{\\ifthenelse{\\boolean{singlecolumn}}{\\abscontentformatted}{\\abscontent}}{}\nAll together, the before-body partial looks something like:\n$if(title)$\n\\maketitle\n$endif$\n\n$if(abstract)$\n\\begin{abstract}\n$abstract$\n\\end{abstract}\n$endif$\n\n\\dates{This manuscript was compiled on \\today}\n\\doi{\\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}\n\n\\thispagestyle{firststyle}\n\\ifthenelse{\\boolean{shortarticle}}{\\ifthenelse{\\boolean{singlecolumn}}{\\abscontentformatted}{\\abscontent}}{}\nThis is about the “simplest” partial to make, once you’re comfortable with partials, since it is so close to the default. If it weren’t for setting the first page style, we could have omitted it. Don’t forget to add it to _extension.yml.\n\n\nbefore-bib.tex\nIn the PNAS template, just before the bibliography, we need to include the acknowledgements. We also need to make a call to a function, \\showacknow{} to make them appear.\nThe first part is the same as our usual pattern. Check if there are acknowledgements, and if there are, set them.\n$if(acknowledgements)$\n\\acknow{$acknowledgements$}\n\n\\showacknow{} % Display the acknowledgments section\n$endif$\nWithin the check, we’ll add the quick call to make sure they appear.\nAs before, we to add it to _extension.yml. But now we’re done with most of the work!\n\n\n\n\nEditing template.qmd (Part 2: The Body)\nOkay, now we can test and run with the file. To do so, we want to add some text back from the other template that instructs the user how to fill out the template. I’ll start by copying in big chunks from the template and then converting things from LaTeX to the friendlier Quarto (or sometimes R) syntax.\nFor example, if the LaTeX template has:\n\\subsection*{Author Affiliations}\nI’ll make that into:\n## Author Affiliations {.unnumbered}\nWe can replace things like sections or links with some find+replace regex in RStudio:\n\nReplace (\\\\subsection\\*{)(.+?)(}) with ## \\2 {.unnumbered} to fix subsection titles.\nReplace (\\\\subsubsection\\*{)(.+?)(}) with ### \\2 {.unnumbered} to fix subsubsection titles.\nReplace (\\\\href{)(.+?)(})({)(.+?)(}) with [\\5](\\2) to move links from LaTeX to Quarto.\nReplace (\\\\ref{fig:)(.+?)(}) with @fig-\\2 to move cross references to Quarto syntax.\nReplace (\\\\verb\\|)(.+?)(\\|) with \\\\2`` to move verbatim environments to Quarto ones.\n\nAs I was doing this, I got a really weird error.\ncompilation failed- error\nLaTeX Error: Not in outer par mode.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H &lt;return&gt;  for immediate help.\n ...                                              \n                                                  \nl.206 \\end{document}\n                     \n\nsee template.log for more information.\nThe obvious thing was that I probably had a mismatch in } or { somewhere. After a few minutes, that clearly wasn’t the case. They all seemed to match and match correctly.\nAt this point, I looked at the template again and thought about the first page special formatting. The first page has a different column width, so it’s a little special. I added some more paragraphs from the LaTeX template and it rendered fine. Playing around and if there was anything short of a page, it would fail. Otherwise, it worked beautifully. As such, I added a warning to the template in the text, so any overzealous compilers would have a clue what broke.\nA word of warning: This template will fail with a \"Not in outer par mode\" warning if you try to compile it with less than one page of text.\nThe template relies on having a full first page which is styled separately.\nIf you see a warning to the tune of \"LaTeX Error: Not in outer par mode.\" or referencing `\\end{document}`, try writing more or adding filler text and recompiling.\nNext, I’ll replace figures. For the most basic figures, I’ll just use raw Quarto figure syntax.\nFor example:\n\\begin{figure}%[tbhp]\n\\centering\n\\includegraphics[width=.8\\linewidth]{figs/frog.pdf}\n\\caption{Placeholder image of a frog with a long example legend to show justification setting.}\n\\label{fig:frog}\n\\end{figure}\ncan be pretty well replicated with the shorter:\n![Placeholder image of a frog with a long example legend to show justification setting.](figs/frog.pdf){#fig-frog}\nAdditional settings can be set within an R chunk (or other code chunk). I find it very helpful to mention the fig-env argument, such as below:\n\nIn Quarto, we can do these by setting the fig-env command to figure* or SCfigure*\n\n#| label: fig-side\n#| fig-cap: \"This legend would be placed at the side of the figure, rather than below it.\"\n#| fig-env: \"SCfigure*\"\n#| echo: false\n# tell it the options as comments with a | and a space, as above.\n# set echo: false to avoid printing this text\nknitr::include_graphics('figs/frog.pdf')\nTwo column journals need the figure* (often called a “star figure”, “figure star”, or “star” environment) for page wide columns. PNAS also has a side-caption version, which is included in the template as the above example. This gives a clear demo of a simpler approach to including figures, at least than with LaTeX.\nAs for tables, I tend to leave them as-is in LaTeX. Many table environments need a little something else that doesn’t seem to translate super well into Markdown syntax. Of course, you could translate them into Markdown, especially if the LaTeX doesn’t need special options.\nFinally, I like to include anything about the bibliography in a References section, like below. I include the special \\bibsplit command that needs to be set manually at the end with a comment explaining how to use it.\n# References\n\\bibsplit[2]\n&lt;!-- Use \\bibsplit to split the references from the body of the text. Value \"[2]\" represents the number of reference in the left column (Note: Please avoid single column figures & tables on this page.) --&gt;\n\n:::{#refs}\n:::\nThis includes the special reference div (the ::: things) to help make it easier to understand where those will be printed. Details on that div are available in the Quarto documentation.\n\n\n\n\n\n\nNote\n\n\n\nDon’t forget to update the bibliography.bib default file to include any example references you use!"
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#cleaning-up",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#cleaning-up",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Cleaning up",
    "text": "Cleaning up\n\nRevising the README\nWith the Quarto and LaTeX stuff pretty much done, we want to include information for future users.\nFirst, we’ll fill out the Options section. In general, this should describe anything that isn’t default in the YAML or that should generally be set.\nFor example, I want to explain a few things:\n\nclassoption defaults\nsetting a corresponding author\naffiliation IDs should be letters\n\nWords are useful here to refer\nFor the PNAS template, I included the following:\n\nThe default setting for class option generates a two column layout with:\n\nclassoption:\n - twocolumn\n - twoside\nTo set a corresponding author, ensure that the attribute “corresponding” is true and that they have an email listed. For proper formatting, each affiliation should be given a letter id (like a, b, …, z). This template is designed for template_type: pnasresearcharticle (the default). It can also take options pnasmathematics or pnasinvited, but these are not formally supported, as they have official alternative formats available on Overleaf.\nNow, we can run the png code from before.\npdftools::pdf_convert('template.pdf', pages = 1)\nwill generate a file template_1.png.\nTo include that in the readme, we can use Markdown, like:\n![[template.qmd](template.qmd)](template_1.png)\nThat gives people enough information to get started with your template.\n\n\nAdding a .quartoignore\nNow, one last thing we want to do is make sure that people using the template won’t install extra files. Things like template_1.png from the last section are useful in the repo, but not to the end user. We can add a file .quartoignore in the root directory that functions like a .gitignore file.\nMine looks like:\n*.pdf\n*.png\n*.rproj\n*.Rproj\n!figs/*.png\nThis ignores all pdfs, pngs, and R project files. It then un-ignores the figs/ folder where useful example files for the template live. It’s important to include those in the template because you want the file that gets downloaded to run when someone runs\nquarto use template christopherkenny/pnas\nBut, they can be deleted once people have made sure the template works. As such, they don’t need to be included in the _extension.yml file, but shouldn’t be .quartoignored.\n\n\nDeleting unused files\nIt’s probably over. There may be other changes, but it’s probably over. But it’s not an official template, so no one will contact you when they update it. But, one day, you might notice that it is different or someone will comment on GitHub. But for now, you’re free! Time to make another template for that other journal you were thinking about submitting to.\nMore importantly, at this point, you should delete any files or code that you didn’t use. For this template, that means removing styles.css and pnas.lua. Also, remove them from the _extension.yml file. That is, delete:\n      filters:\n        - pnas.lua\nand\n    html:\n      css: styles.css"
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#finishing-up",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#finishing-up",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Finishing up",
    "text": "Finishing up\nLast things last.\nIf your template now works, then it’s time to make it public. A seemingly ridiculous portion of research time is spent recreating resources that surely someone else has somewhere. A semi-functional template is better than starting from scratch.\nAdd a topic to the GitHub repo for quarto-template and anything else relevant! Congrats, you have a template. Share it on Twitter or whatever is still useful. If it doesn’t work for someone, they’ll email you or open an issue. As a wise person once said &gt; all (feedback) is good (feedback)\nEven if it didn’t work for someone, a little information about why it didn’t work can help you a ton for when you have the same issue some day."
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#resources",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#resources",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Resources",
    "text": "Resources\nThe template created within this post is available at christopherkenny/pnas."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christopher T. Kenny",
    "section": "",
    "text": "Ph.D., Department of Government, Harvard University\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrevious\n\n\n\nNext\n\n\n\n\n\n\nAbout Me\nMy research focuses on American politics and political methodology, with a specialization in redistricting. I am a founding member of the Algorithm-Assisted Redistricting Methodology (ALARM) Project. I am affiliated with the Center for American Political Studies at Harvard University and the Institute for Quantitative Social Science. In 2022, I was a fellow at the Election Law Clinic at Harvard Law School.\nMy research focuses on primarily on American democratic institutions, with a particular emphasis on redistricting. My prior work has examined the net effect of gerrymandering in the US House in the 2020 cycle (PNAS 2023). With the ALARM Project, I’ve contributed to the creation of simulated redistricting plans for all congressional redistricting plans in the 2020 cycle (Nature: Scientific Data 2022) with ongoing work to extend this to the 2010 cycle. My research further includes a working paper with Cory McCartan, “Individual and Differential Harm in Redistricting”, which proposes an individual approach to fairness in redistricting. In a new working paper with the ALARM Project, “Redistricting Reforms Reduce Gerrymandering by Constraining Partisan Actors”, we demonstrate that restrictive reforms improve partisan fairness of redistricting plans.\nMy work on census data relates to how the construction and privacy protection of census data impacts end users. My work on census data includes a policy evaluation paper which details the negative impact of differential privacy in the 2020 Census on redistricting and voting rights enforcement (Science Advances 2021). It further characterizes how certain communities, including Hispanic and multi-racial populations, are disproportionately impacted by the privacy noise in the 2020 Census (Science Advances 2024).\nTo support my own research and the research of others, I have developed over two dozen R packages, including redist, geomander, and censable. In 2024, my packages were downloaded over 100,000 times from CRAN. For more information on the full suite of packages that I’ve developed, see my software page.\nIn Summer 2025, I will join Princeton DDSS as a Postdoctoral Research Associate. I received a PhD in Government from Harvard University in May 2025. I received a B.A. in Mathematics and Government from Cornell University in May 2019 and an M.A. in Government from Harvard University in May 2021."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Thinking SMARTer about election fraud in Rockland County, NY\n\n\n\nelections\n\n\n\nA look at the data and empirical claims in a local election fraud case brought by an unsuccessful third party candidate and SMART Legislation in Rockland County, NY.\n\n\n\n\n\nJun 17, 2025\n\n\nChristopher T. Kenny\n\n\n\n\n\n\n\n\n\n\n\n\nPackages 2024 Wrapped\n\n\n\nr-pkg\n\n\n\nA late look at my R package updates from 2024.\n\n\n\n\n\nMar 21, 2025\n\n\nChristopher T. Kenny\n\n\n\n\n\n\n\n\n\n\n\n\nSettings, Keybindings, and Extensions for Positron\n\n\n\nide\n\npositron\n\n\n\nMy initial keybindings, settings, and extensions for Positron, Posit’s new IDE built on “Code - OSS” (aka Visual Studio Code aka VS Code).\n\n\n\n\n\nJun 29, 2024\n\n\nChristopher T. Kenny\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a Scheduled Bot for Bluesky Social in R\n\n\n\nr-pkg\n\nbskyr\n\n\n\nExplaining the mechanics behind my CRAN Updates bot for Bluesky Social.\n\n\n\n\n\nJan 3, 2024\n\n\nChristopher T. Kenny\n\n\n\n\n\n\n\n\n\n\n\n\nPackages 2023 Wrapped\n\n\n\nr-pkg\n\n\n\nA quick look at my R package updates this year.\n\n\n\n\n\nDec 21, 2023\n\n\nChristopher T. Kenny\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Maps with feltr\n\n\n\nmaps\n\nr-pkg\n\n\n\nA brief introduction to the feltr package.\n\n\n\n\n\nJul 7, 2023\n\n\nChristopher T. Kenny\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Quarto Journal Article Templates\n\n\n\nquarto\n\n\n\nA long walk through creating Quarto templates for journal articles.\n\n\n\n\n\nJul 1, 2023\n\n\nChristopher T. Kenny\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "package-relationships.html",
    "href": "package-relationships.html",
    "title": "Package relationships",
    "section": "",
    "text": "flowchart LR\n  redist{{redist}}\n  redistmetrics{{redistmetrics}}\n  geomander{{geomander}}\n  PL94171{{PL94171}}\n  censable{{censable}}\n  tinytiger{{tinytiger}}\n  alarmdata{{alarmdata}}\n  ggredist{{ggredist}}\n  redistverse{{redistverse}}\n  dots{{dots}}\n  cvap{{cvap}}\n  ppmf{{ppmf}}\n  divseg{{divseg}}\n  name{{name}}\n  jot{{jot}}\n  ei{{ei}}\n  congress{{congress}}\n  feltr{{feltr}}\n  planscorer{{planscorer}}\n  crayons{{crayons}}\n  apportion{{apportion}}\n  gptzeror{{gptzeror}}\n  bskyr{{bskyr}}\n  redistio((redistio))\n  palette{{palette}}\n  ThemePark((ThemePark))\n  baf{{baf}}\n  redistmetrics --&gt; redist\n  censable --&gt; geomander\n  tinytiger --&gt; geomander\n  tinytiger --&gt; PL94171\n  tinytiger --&gt; censable\n  censable --&gt; cvap\n  censable --&gt; ppmf\n  censable --&gt; alarmdata\n  geomander --&gt; alarmdata\n  redist --&gt; alarmdata\n  redistmetrics --&gt; alarmdata\n  tinytiger --&gt; alarmdata\n  palette --&gt; ggredist\n  redist --&gt; redistverse\n  redistmetrics --&gt; redistverse\n  geomander --&gt; redistverse\n  ggredist --&gt; redistverse\n  censable --&gt; redistverse\n  tinytiger --&gt; redistverse\n  PL94171 --&gt; redistverse\n  alarmdata --&gt; redistverse\n  palette --&gt; crayons\n  geomander --&gt; redistio\n  ggredist --&gt; redistio\n  redistmetrics --&gt; redistio"
  },
  {
    "objectID": "posts/2023-07-07-making-maps-with-feltr/index.html",
    "href": "posts/2023-07-07-making-maps-with-feltr/index.html",
    "title": "Making Maps with feltr",
    "section": "",
    "text": "I make a lot of maps, almost always in R. Recently, I was introduced to felt.com. It’s a clean interface for web maps, including some great features, like drawing directly on a map or adding text annotations.\nThe new feltr package offers an interface to the Felt API, so you can upload data to Felt directly from R. It also includes tools for reading data from Felt into R as sf objects.\nYou can install feltr with:\nBelow, I’ll demo making a map with point locations of Dunkins in Cambridge, MA, from a csv file of Dunkin addresses."
  },
  {
    "objectID": "posts/2023-07-07-making-maps-with-feltr/index.html#dunkins-in-cambridge-ma",
    "href": "posts/2023-07-07-making-maps-with-feltr/index.html#dunkins-in-cambridge-ma",
    "title": "Making Maps with feltr",
    "section": "Dunkins in Cambridge, MA",
    "text": "Dunkins in Cambridge, MA\nFirst, we’ll load a few packages.\n\nlibrary(feltr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/chris/Documents/GitHub/christopherkenny.github.io\n\n\nOne of the cool things with Felt is its “Upload Anything” feature, where we can upload anything. Here, we have a csv file of addresses for every Dunkin in Cambridge. It is simple, just text addresses separated into appropriate fields.\n\npath_dunkin_ma &lt;- here('posts/2023-07-07-making-maps-with-feltr/dunkin_ma.csv')\nread_csv(path_dunkin_ma, show_col_types = FALSE)\n\n# A tibble: 1,062 × 5\n   address           city     state zipcode    id\n   &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 147 N Quincy St   Abington MA    02351       1\n 2 259 Brockton Ave  Abington MA    02351       2\n 3 323 Centre Ave    Abington MA    02351       3\n 4 937 Bedford St    Abington MA    02351       4\n 5 100 Powdermill Rd Acton    MA    01720       5\n 6 182 Great Rd      Acton    MA    01720       6\n 7 212 Main St       Acton    MA    01720       7\n 8 315 Main St       Acton    MA    01720       8\n 9 44 Great Rd       Acton    MA    01720       9\n10 150 S Main St     Acushnet MA    02743      10\n# ℹ 1,052 more rows\n\n\nTo share this data with Felt, we first have to make a new map. We don’t have to give it any information, it’ll just make an empty map. We can pass it a title and some starting information, like where to center the map and how far to zoom.\n\ndunk &lt;- felt_create_map(\n  title = 'Cambridge Dunkin Desert', \n  zoom = 14.5, lat = 42.3799, lon = -71.10668\n)\n\nThen once we have the map, we can upload the csv file directly to Felt. No local geocoding necessary, it’ll handle that. We can label the layer with name or supply colors, like fill_color and stroke_color.\n\nlayer_id &lt;- felt_add_map_layers(\n  map_id = dunk$id, name = 'Dunkin', file_names = path_dunkin_ma, \n  fill_color = '#FF671F', stroke_color = '#DA1884'\n)\n\nOnce we do that, after a couple of minutes, we have a map. Normally it’s a few seconds if we uploaded a geojson or shp file, but geocoding takes a small bit of time.\n\n\n\nDefault Felt Layout\n\n\nWhat I find great about this is that I can handle all of the data work in R and then adjust the map as needed after. For example, I can annotate where the Department of Government buildings are with a green star or highlight where Darwin’s was (until recently) with a blue x.\n\n\n\nAnnotated Map\n\n\nClearly, Darwin’s old location would be a great place for a new Dunkin, near the middle of an existing Dunkin desert.\nfeltr has additional features, including:\n\ndeleting maps with felt_delete_map()\nlisting details of existing maps with felt_get_map() and felt_get_map_layers()\ndownloading shapes with felt_get_map_sf(), felt_get_map_geojson(), and felt_get_map_elements()\nretrieving user details with felt_get_user().\n\nAll current features of the Felt API are supported in the CRAN version of feltr, as of July 2023. To offer feedback on feltr or ask questions, open an issue on GitHub."
  },
  {
    "objectID": "posts/2024-01-03-bskyr-bot/index.html",
    "href": "posts/2024-01-03-bskyr-bot/index.html",
    "title": "Making a Scheduled Bot for Bluesky Social in R",
    "section": "",
    "text": "This post walks through how I set up a simple bot in Bluesky Social. The bot, @cranupdates.bsky.social, posts every 2 hours with details about packages that have been updated, added, or removed from CRAN. Everything is run in R, primarily using the bskyr package. It’s run for free on GitHub Actions and data is stored between runs using Google Sheets.\nThe basic mechanics of the bot are:\nThis bot is entirely schedule based, so it doesn’t need to interact with other Bluesky users. Below, I detail how I set up the bot, including how to authenticate with Google Sheets (using googlesheets4) and GitHub Actions."
  },
  {
    "objectID": "posts/2024-01-03-bskyr-bot/index.html#sec-schedule",
    "href": "posts/2024-01-03-bskyr-bot/index.html#sec-schedule",
    "title": "Making a Scheduled Bot for Bluesky Social in R",
    "section": "Scheduling the run",
    "text": "Scheduling the run\nTo schedule the run, we need to tell it a few things:\n\nwhen to run it\nwhat to run\n\nwhat environment variables it needs\nwhat R version to use\nwhat R packages to install\nwhat script to run\n\n\nBelow, I explain these steps. But first, the completed workflow looks like:\non:\n  push:\n    branches: main\n  schedule:\n    - cron: '0 1,5,9,13,17,21 * * *'\n\nname: Post\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      BLUESKY_APP_USER: ${{ secrets.BLUESKY_APP_USER }}\n      BLUESKY_APP_PASS: ${{ secrets.BLUESKY_APP_PASS }}\n      GARGLE_KEY: ${{ secrets.GARGLE_KEY }}\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n\n      - uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: 'release'\n\n      - uses: r-lib/actions/setup-r-dependencies@v2\n        with:\n          packages:\n            any::here\n            any::dplyr\n            any::stringr\n            any::googlesheets4\n            any::bskyr\n\n      - run: Rscript 'post.R'\nThis whole file lives in .github/workflows/post.yml in the repo.\nFirst, let’s break down the when part:\non:\n  push:\n    branches: main\n  schedule:\n    - cron: '0 1,5,9,13,17,21 * * *'\nThis tells GitHub Actions to run the workflow when there’s a push to the main branch. It also says to schedule the workflow to run every 4 hours. CRON entries are minute hour day month weekday, so 0 1,5,9,13,17,21 * * * means to run at 1am, 5am, 9am, 1pm, 5pm, and 9pm every day. As implied, setting the star says to run it every day, every month, and every weekday. More documentation for the schedule part can be found here.\nThen we give the job a name, “Post”, with name: Post.\nThe next section simply indicates we want the job to run on the latest version of Ubuntu:\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\nNext, we provide environment variables, that we set in the repo settings. See {#sec-auth} for an explanation of the env GARGLE_KEY variable. This step is like setting an .Rprofile file locally, but for GitHub Actions.\n    env:\n      BLUESKY_APP_USER: ${{ secrets.BLUESKY_APP_USER }}\n      BLUESKY_APP_PASS: ${{ secrets.BLUESKY_APP_PASS }}\n      GARGLE_KEY: ${{ secrets.GARGLE_KEY }}\nNow, we can give it the steps to use.\nFirst, it needs to download the contents of the repo, so that it can access our post.R script.\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\nThen, we have to let it know to install R. We can use one of the r-lib/actions actions, setup-r@v2. I’m using the released R version (4.3.2) at the time of writing this. You could pin a specific version, but for bots, I plan to do the minor maintenance necessary as R versions increment.\n      - uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: 'release'\nAnd then we list the packages we need. This uses the setup-r-dependencies@v2 action, again from r-lib/actions. Below, I make use of the prefixes: any:: and github::. any:: is used for packages on CRAN, generally, but will also run if you’ve installed it elsewhere through a prior dependency.\n\n\n\n\n\n\nUsing GitHub Packages\n\n\n\n\n\nYou can use github:: in place of any:: for packages on GitHub. You just need to specify the user and repo. So, if you want to use the dev version of a package, like bskyr, you could use the syntax: github::christopherkenny/bskyr.\n\n\n\n      - uses: r-lib/actions/setup-r-dependencies@v2\n        with:\n          packages:\n            any::here\n            any::dplyr\n            any::stringr\n            any::googlesheets4\n            any::bskyr\nIf you already use renv, you could instead use that with the setup-renv action.\nFinally, we tell it to run post.R with:\n      - run: Rscript 'post.R'\nThat’s sufficient to tell GitHub Actions everything it needs to know to run the bot. Below, I describe how to set up the authentication with Google Sheets. This is only necessary if you’re using it, like I am, to store data about the prior bot run."
  },
  {
    "objectID": "posts/2024-01-03-bskyr-bot/index.html#footnotes",
    "href": "posts/2024-01-03-bskyr-bot/index.html#footnotes",
    "title": "Making a Scheduled Bot for Bluesky Social in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou could use something more private, but for a generic bot, public means GitHub Actions is free and can get the job done.↩︎\nThis created a small issue in the bot: Several of the first set of posts were about new packages that weren’t actually new. This was because the first run of the bot was on a Windows laptop, while the second was on a Linux server. By default, available.packages() returns packages available for the current OS, hence the setting of filters = c('CRAN', 'duplicates'), which overwrites the default that filters to OS-available too.↩︎"
  },
  {
    "objectID": "posts/2025-03-21-software-2024/index.html",
    "href": "posts/2025-03-21-software-2024/index.html",
    "title": "Packages 2024 Wrapped",
    "section": "",
    "text": "It’s tax season, so I’m thinking about the past year. I never wrote a wrap up for 2024 on packages like I had done for 2023. So, here’s a late look at my R package updates from 2024."
  },
  {
    "objectID": "posts/2025-03-21-software-2024/index.html#new-packages",
    "href": "posts/2025-03-21-software-2024/index.html#new-packages",
    "title": "Packages 2024 Wrapped",
    "section": "New packages",
    "text": "New packages\nI’ve added six new package to CRAN this year, up from five in 2023. My hope is that this is going to settle down a bit in 2025, but there’s quite a bit of work in progress already…\n\nalarmdata\n\n\n\nalarmdata provides an interface to data from the ALARM Project. This gives a simple way to download data on things like congressional districts or precinct election results.      \n\n\nplanscorer\n\n\n\nplanscorer is an R port (plus a little extra) of the python package for PlanScore. PlanScore is a tool for evaluating redistricting plans, particularly for partisan fairness. planscorer allows you to upload a plan to PlanScore and get back a report on the plan’s partisan fairness.      \n\n\nredistverse\n\n\n\nThe redistverse is a collection of packages for redistricting, centered on the award-winning R package, redist. This combines many of our packages for redistricting, like redist, redistmetrics, geomander, ggredist, censable, tinytiger, PL94171, and alarmdata into one easy-to-install bundle.      \n\n\npalette\n\n\n\npalette is a package for creating color palettes. It’s based on the vctrs package, so it’s easy to use with the tidyverse. My hope is that this will be a useful tool for creating consistent color schemes across projects by providing basic methods for creating palettes.      \n\n\nbaf\n\n\n\nbaf is a package for accessing Block Assignment Files (BAFs) from the US Census Bureau. These files crosswalk blocks from the decennial census to other geographies, like congressional districts or state legislative districts.      \n\n\nopengraph\n\n\n\nopengraph is a package for reading metadata from web pages. With the Open Graph Protocol, many websites provide structured data about their pages. This allows packages like bskyr to provide a preview of a web page within posts. To see this in action, check out bskyr PR #18."
  },
  {
    "objectID": "posts/2025-03-21-software-2024/index.html#updates-to-existing-packages",
    "href": "posts/2025-03-21-software-2024/index.html#updates-to-existing-packages",
    "title": "Packages 2024 Wrapped",
    "section": "Updates to existing packages",
    "text": "Updates to existing packages\nOne of the good parts of package development is that packages eventually reach a stable point. There will be bugs to fix or enhancements to make, but the bulk of the work is done. Among the 19 packages on CRAN going into 2024, 7 of them recieved a single update this past year.\n\n\n\n\n\n\n\n\nName\nVersion\nDate\nTitle\n\n\n\n\ncongress\n0.0.3\n2024-01-09\nAccess the Congress.gov API\n\n\nbskyr\n0.1.2\n2024-01-09\nInteract with 'Bluesky' Social\n\n\nredist\n4.2.0\n2024-01-13\nSimulation Methods for Legislative Redistricting\n\n\ngeomander\n2.3.0\n2024-02-15\nGeographic Tools for Studying Gerrymandering\n\n\ncvap\n0.1.5\n2024-03-21\nCitizen Voting Age Population\n\n\ncrayons\n0.0.3\n2024-03-21\nColor Palettes from Crayon Boxes\n\n\ntinytiger\n0.0.9\n2024-06-04\nLightweight Interface to TIGER/Line Shapefiles\n\n\nredistverse\n0.1.0\n2024-06-18\nEasily Install and Load Redistricting Software"
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "Quarto Extensions",
    "section": "",
    "text": "Journal templates\n\n    Templates for general science and social science journals.\n\n    \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        Annual Reviews\n                            \n                                \n                                    Quarto\n                                \n                                    LaTeX\n                                \n                            \n                        \n                            For Annual Reviews drafts.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        APSR\n                            \n                                \n                                    Quarto\n                                \n                                    LaTeX\n                                \n                            \n                        \n                            For the American Political Science Review submissions.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        Cambridge Medium\n                            \n                                \n                                    Quarto\n                                \n                                    LaTeX\n                                \n                            \n                        \n                            For Cambridge Medium journal submissions, including PA, PSRM, BJPS, SPQ, and more.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        Nature\n                            \n                                \n                                    Quarto\n                                \n                                    LaTeX\n                                \n                            \n                        \n                            For Springer Nature journal submissions.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        PNAS\n                            \n                                \n                                    Quarto\n                                \n                                    LaTeX\n                                \n                            \n                        \n                            For PNAS submissions.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        PNAS Supporting Information\n                            \n                                \n                                    Quarto\n                                \n                                    LaTeX\n                                \n                            \n                        \n                            For PNAS SI (appendices) submissions.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        Scientific Data\n                            \n                                \n                                    Quarto\n                                \n                                    LaTeX\n                                \n                            \n                        \n                            For Scientific Data \"Data Descriptors\" submissions.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        Perspectives\n                            \n                                \n                                    Quarto\n                                \n                                    LaTeX\n                                \n                            \n                        \n                            For Perspectives on Politics submissions.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        Science\n                            \n                                \n                                    rticles\n                                \n                                    LaTeX\n                                \n                            \n                        \n                            For Science and Science Advances submissions.\n                        \n                    \n            \n\n        \n\n    \n\n    Quarto filters\n\n    Quarto extensions which provide additional document functions.\n\n    \n\n        \n\n            \n                \n                    \n                        wordcount\n                            \n                                \n                                    Quarto\n                                \n                                    LaTeX\n                                \n                                    HTML\n                                \n                            \n                        \n                            Insert word counts into the body of a document.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        typst-function\n                            \n                                \n                                    Quarto\n                                \n                                    Typst\n                                \n                            \n                        \n                            Add Typst functions to Quarto documents using div and span syntax.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        spellcheck\n                            \n                                \n                                    Quarto\n                                \n                                    LaTeX\n                                \n                                    Typst\n                                \n                                    HTML\n                                \n                            \n                        \n                            Print misspelled words to the console using Hunspell.\n                        \n                    \n            \n\n        \n\n    \n\n    Other templates\n\n    Formats for non-journal documents.\n\n    \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        tufte\n                            \n                                \n                                    Quarto\n                                \n                                    Typst\n                                \n                            \n                        \n                            For Tufte-style documents in Quarto with a Typst backend.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        ctk-article\n                            \n                                \n                                    Quarto\n                                \n                                    Typst\n                                \n                            \n                        \n                            A general purpose template for preprints and submissions using Quarto and Typst.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        ctk-memo\n                            \n                                \n                                    Quarto\n                                \n                                    Typst\n                                \n                            \n                        \n                            For memos in Quarto with a Typst backend.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        cv\n                            \n                                \n                                    Quarto\n                                \n                                    Typst\n                                \n                            \n                        \n                            For a shortcode + BibTeX powered CV in Quarto with a Typst backend.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        projector\n                            \n                                \n                                    Quarto\n                                \n                                    Typst\n                                \n                            \n                        \n                            Make slides in Quarto powered by Typst's Polylux.\n                        \n                    \n            \n\n        \n\n            \n                \n                    \n                        \n                    \n                \n                    \n                        harvard-diss\n                            \n                                \n                                    Quarto\n                                \n                                    Typst\n                                \n                            \n                        \n                            Write a Harvard GSAS Dissertation in Quarto with a Typst backend.\n                        \n                    \n            \n\n        \n\n    \n\nNo matching items"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Packages on CRAN\n\n  \n  \n      Packages for redistricting\n  \n\n\n\n  \n    \n      \n    \n  \n  \n    \n      redist: Simulation Methods for Legislative Redistricting\n        \n          (with Cory McCartan, Ben Fifield, and Kosuke Imai)\n        \n        Enables researchers to sample redistricting plans from a pre-specified target distribution using Sequential Monte Carlo and Markov Chain Monte Carlo algorithms. The package allows for the implementation of various constraints in the redistricting process such as geographic compactness and population parity requirements. Tools for analysis such as computation of various summary statistics and plotting functionality are also included. The package implements the SMC algorithm of McCartan and Imai (2023), the enumeration algorithm of Fifield, Imai, Kawahara, and Kenny (2020), the Flip MCMC algorithm of Fifield, Higgins, Imai and Tarr (2020), the Merge-split/Recombination algorithms of Carter et al. (2019) and DeFord et al. (2021), and the Short-burst optimization algorithm of Cannon et al. (2020).\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      redistmetrics: Redistricting metrics\n        \n          (with Cory McCartan, Ben Fifield, and Kosuke Imai)\n        \n        Reliable and flexible tools for scoring redistricting plans using common measures and metrics. These functions provide key direct access to tools useful for non-simulation analyses of redistricting plans, such as for measuring compactness or partisan fairness. Tools are designed to work with the 'redist' package seamlessly.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      geomander: Geographic Tools for Studying Gerrymandering\n        \n        A compilation of tools to complete common tasks for studying gerrymandering. This focuses on the geographic tool side of common problems, such as linking different levels of spatial units or estimating how to break up units. Functions exist for creating redistricting-focused data for the US.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      alarmdata: Download, Merge, and Process Redistricting Data\n        \n          (with Cory McCartan, Tyler Simko, Michael Zhao, and Kosuke Imai)\n        \n        Utility functions to download and process data produced by the ALARM Project, including 2020 redistricting files Kenny and McCartan (2021) https://alarm-redist.org/posts/2021-08-10-census-2020/ and the 50-State Redistricting Simulations of McCartan, Kenny, Simko, Garcia, Wang, Wu, Kuriwaki, and Imai (2022). The package extends the data introduced in McCartan, Kenny, Simko, Garcia, Wang, Wu, Kuriwaki, and Imai (2022) to also include states with only a single district.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      redistverse: Easily Install and Load Redistricting Software\n        \n          (with Cory McCartan)\n        \n        Easy installation, loading, and control of packages for redistricting data downloading, spatial data processing, simulation, analysis, and visualization. This package makes it easy to install and load multiple 'redistverse' packages at once. The 'redistverse' is developed and maintained by the Algorithm-Assisted Redistricting Methodology (ALARM) Project. For more details see https://alarm-redist.org.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      baf: Block Assignment Files\n        \n        Download and read US Census Bureau data relationship files. Provides support for cleaning and using block assignment files since 2010, as described in https://www.census.gov/geographies/reference-files/time-series/geo/block-assignment-files.html. Also includes support for working with block equivalency files, used for years outside of decennial census years.\n    \n  \n\n\n\n  \n      Packages for working with Census Bureau data\n  \n\n\n\n  \n    \n      \n    \n  \n  \n    \n      PL94171: Tabulate P.L. 94-171 Redistricting Data Summary Files\n        \n          (with Cory McCartan)\n        \n        Tools to process legacy format summary redistricting data files produced by the United States Census Bureau pursuant to P.L. 94-171. These files are generally available earlier but are difficult to work with as-is.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      censable: Making Census Data More Usable\n        \n        Creates a common framework for organizing, naming, and gathering population, age, race, and ethnicity data from the Census Bureau. Accesses the API https://www.census.gov/data/developers/data-sets.html. Provides tools for adding information to existing data to line up with Census data.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      tinytiger: Lightweight Interface to TIGER/Line Shapefiles\n        \n          (with Cory McCartan)\n        \n        Download geographic shapes from the United States Census Bureau TIGER/Line Shapefiles https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html. Functions support downloading and reading in geographic boundary data. All downloads can be set up with a cache to avoid multiple downloads. Data is available back to 2000 for most geographies.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      cvap: Citizen Voting Age Population\n        \n        Works with the Citizen Voting Age Population special tabulation from the US Census Bureau https://www.census.gov/programs-surveys/decennial-census/about/voting-rights/cvap.html. Provides tools to download and process raw data. Also provides a downloading interface to processed data. Implements a very basic approach to estimate block level citizen voting age population from block group data.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      ppmf: Read Census Privacy Protected Microdata Files\n        \n        Implements data processing described in to align modern differentially private data with formatting of older US Census data releases. The primary goal is to read in Census Privacy Protected Microdata Files data in a reproducible way. This includes tools for aggregating to relevant levels of geography by creating geographic identifiers which match the US Census Bureau's numbering. Additionally, there are tools for grouping race numeric identifiers into categories, consistent with OMB (Office of Management and Budget) classifications. Functions exist for downloading and linking to existing sources of privacy protected microdata.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      apportion: Apportion Seats\n        \n        Convert populations into integer number of seats for legislative bodies. Implements apportionment methods used historically and currently in the United States for reapportionment after the Census, as described in https://www.census.gov/history/www/reference/apportionment/methods_of_apportionment.html.\n    \n  \n\n\n\n  \n      Packages for plotting data\n  \n\n\n\n  \n    \n      \n    \n  \n  \n    \n      dots: Dot Density Maps\n        \n        Generate point data for representing people within spatial data. This collects a suite of tools for creating simple dot density maps. Several functions from different spatial packages are standardized to take the same arguments so that they can be easily substituted for each other.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      ggredist: Scales, Palettes, and Extensions of ggplot2 for Redistricting\n        \n          (with Cory McCartan)\n        \n        Provides 'ggplot2' extensions for political map making. Implements new geometries for groups of simple feature geometries. Adds palettes and scales for red to blue color mapping and for discrete maps. Implements tools for easy label generation and placement, automatic map coloring, and themes.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      crayons: Color Palettes from Crayon Boxes\n        \n        Provides color palettes based on crayon colors since the early 1900s. Colors are based on various crayon colors, sets, and promotional palettes, most of which can be found at https://en.wikipedia.org/wiki/List_of_Crayola_crayon_colors. All palettes are discrete palettes and are not necessarily color-blind friendly. Provides scales for 'ggplot2' for discrete coloring.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      palette: Color Scheme Helpers\n        \n        Hexadecimal codes are typically used to represent colors in R. Connecting these codes to their colors requires practice or memorization. 'palette' provides a 'vctrs' class for working with color palettes, including printing and plotting functions. The goal of the class is to place visual representations of color palettes directly on or, at least, next to their corresponding character representations. Palette extensions also are provided for data frames using 'pillar'.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      flexoki: Inky Color Schemes\n        \n        Provides color palettes designed to be reminiscent of text on paper. The color schemes were taken from https://stephango.com/flexoki. Includes discrete, continuous, and binned scales that are not necessarily color-blind friendly. Simple scale and theme functions are available for use with 'ggplot2'.\n    \n  \n\n\n\n  \n      Packages interfacing with API services\n  \n\n\n\n  \n    \n      \n    \n  \n  \n    \n      congress: Access the Congress.gov API\n        \n        Download and read data on United States congressional proceedings. Data is read from the Library of Congress's Congress.gov Application Programming Interface (https://github.com/LibraryOfCongress/api.congress.gov/). Functions exist for all version 3 endpoints, including for bills, amendments, congresses, summaries, members, reports, communications, nominations, and treaties.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      feltr: Access the Felt API\n        \n        Upload, download, and edit internet maps with the Felt API (https://feltmaps.notion.site/Felt-Public-API-reference-c01e0e6b0d954a678c608131b894e8e1). Allows users to create new maps, edit existing maps, and extract data. Provides tools for working with layers, which represent geographic data, and elements, which are interactive annotations. Spatial data accessed from the API is transformed to work with 'sf'.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      planscorer: Score Redistricting Plans with PlanScore\n        \n        Provides access to the 'PlanScore' Application Programming Interface (https://github.com/PlanScore/PlanScore/blob/main/API.md) for scoring redistricting plans. Allows for upload of plans from block assignment files and shape files. For shapes in memory, such as from 'sf' or 'redist', it processes them to save and upload. Includes tools for tidying responses and saving output from the website.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      gptzeror: Identify Text Written by Large Language Models using GPTZero\n        \n        An R interface to the 'GPTZero' API (https://gptzero.me/docs). Allows users to classify text into human and computer written with probabilities. Formats the data into data frames where each sentence is an observation. Paragraph-level and document-level predictions are organized to align with the sentences.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      bskyr: Interact with Bluesky Social\n        \n        Collect data from and make posts on 'Bluesky' Social via the Hypertext Transfer Protocol (HTTP) Application Programming Interface (API), as documented at https://atproto.com/specs/xrpc. This further supports broader queries to the Authenticated Transfer (AT) Protocol https://atproto.com/ which 'Bluesky' Social relies on. Data is returned in a tidy format and posts can be made using a simple interface.\n    \n  \n\n\n\n  \n      Other R packages\n  \n\n\n\n  \n    \n      \n    \n  \n  \n    \n      divseg: Compute Diversity and Segregation Indices\n        \n        Implements common measures of diversity and spatial segregation. This package has tools to compute the majority of measures are reviewed in Massey and Denton (1988). Multiple common measures of within-geography diversity are implemented as well. All functions operate on data frames with a 'tidyselect' based workflow.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      name: Tools for Working with Names\n        \n        A system for organizing column names in data. Aimed at supporting a prefix-based and suffix-based column naming scheme. Extends 'dplyr' functionality to add ordering by function and more explicit renaming.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      jot: Jot Down Notes for Later\n        \n        Reproducible work requires a record of where every statistic originated. When writing reports, some data is too big to load in the same environment and some statistics take a while to compute. This package offers a way to keep notes on statistics, simple functions, and small objects. Notepads can be locked to avoid accidental updates. Notepads keep track of who added the notes and when the notes were added. A simple text representation is used to allow for clear version histories.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      opengraph: Process Metadata from the Open Graph Protocol\n        \n        Social media sites often embed cards when links are shared, based on metadata in the Open Graph Protocol (https://ogp.me/). This supports extracting that metadata from a website. It further allows for the creation of tags to add to a website to support the Open Graph Protocol and provides a list of the standard tags and their required properties.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      typr: Write and Render Typst Documents\n        \n        Compile Typst files using the typst-cli (https://typst.app) command line tool. Automatically falls back to rendering via embedded Typst from Quarto (https://quarto.org) if Typst is not installed. Includes utilities to check for typst-cli availability and run Typst commands.\n    \n  \n\n\n\nPackages on GitHub\n\n  \n  \n\n\n\n  \n    \n      \n    \n  \n  \n    \n      ei: Ecological Inference\n        \n          (with Shusei Eshima, Gary King, and Molly Roberts)\n        \n        Software accompanying Gary King's book: A Solution to the Ecological Inference Problem. (1997). Princeton University Press. ISBN 978-0691012407.\n    \n  \n\n  \n    \n      \n    \n  \n  \n    \n      redistio: Interactive Redistricting\n        \n        A point and click editor for districts built on 'shiny' and 'Leaflet'. Users can draw districts and calculate standard redistricting metrics, like compactness or the number of administrative splits. Maps can be exported as assignment files or shapefiles, readable by most other redistricting software.\n    \n  \n\n\n\n  \n\n\n\n  \n    \n      \n    \n  \n  \n    \n      ThemePark: Themes for 'ggplot2' from Popular Culture\n        \n          (with Matthew B. Jané and Luke C. Pilling)\n        \n        Provides 'ggplot2' themes that mirror works from popular culture, such as Barbie, Star Wars, Game of Thrones, and others. The package currently holds 14 themes and a number of corresponding discrete color scales, palettes, and fonts. Each theme (e.g., 'theme_barbie') generates a unique color scheme and font for a 'ggplot2' object that matches the color scheme and font found in the movie, TV show, or video game.\n    \n  \n\n\n\n  \n\n\n\n  \n    \n      \n    \n  \n  \n    \n      causaltbl: Tidy Causal Data Frames and Tools\n        \n          (with Cory McCartan)\n        \n        Provides a 'causal_tbl' class for causal inference. A 'causal_tbl' keeps track of information on the roles of variables like treatment and outcome, and provides functionality to store models and their fitted values as columns in a data frame.\n    \n  \n\n\n\nNo matching items"
  }
]