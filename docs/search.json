[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Harvard College, Spring 2023 and Spring 2024.\nInstructor and course designer.\nA sophomore tutorial on redistricting and elections designed as an introduction to producing original research. A public copy of my syllabus is available here.\n\n\nFull Course Description\n\nFederal, state, and local governments are often built from geographic districts. In redistricting, the districts are drawn for a decade at a time. Commissions, legislatures, and courts draw those geographic districts for future elections. The process of redistricting can be done in as little as a few days, but the maps define constituencies for the next decade. We will study the fundamental building blocks of American democracy. Congress is entirely dependent on how its districts are drawn. Who wins when lines are the drawn? Do district-based systems empower or weaken minority communities? How do we distinguish between effects on race and party? Should we protect incumbents when there are different benefits to accountability and experience? Is it better to have non-partisan groups draw the lines? At the end of the day, how can we say if a map is good or bad?\nIn this course, we’ll explore the many aspects of how districts are drawn in America and their effects on our many geographic democracies. We’ll start by building a baseline understanding of how laws and rules from local, state, and federal entities come together to regulate mapmaking. We’ll explore the ways that we can describe maps quantitatively and qualitatively. We’ll learn from court decisions and recent research into redistricting. All the while, we will draw maps for local, state, and federal districts to better understand the tradeoffs and the high stakes in mapmaking. As new members of the Government department, we will develop your research skills, with an emphasis on how to measure and think about measuring some of the many, often-fuzzy concepts in political science.\nCourse readings draw on legal authorities, law review articles, expert reports, and empirical political science to explore the field. With a diverse set of readings it is important to realize that redistricting is partisan in many ways. We will engage with writers (and researchers) from the left and right. Disagreement is encouraged, as it is vital to advancement of knowledge, but we will make all efforts to ensure that disagreement is both substantive and polite.\n\n\n\n\n\n\n\nHarvard University and Harvard Extension School, Fall 2021.\nTeaching Fellow with Gary King.\nA first graduate course in political methodology.\n\n\nFull Course Description\n\nThis is a first graduate course in political methodology, the methodological subfield of the discipline of political science, akin to econometrics within economics, psychometrics within psychology, sociological methodology within sociology, biostatistics within public health and medicine, and dozens of others. These methodological subfields are increasingly interconnected across disciplines and are often known together under broader monikers, such as data science, applied statistics, or computational social science. Political science is an unusually diverse discipline, welcoming of an exceptionally broad array of approaches, substantive questions, theories, and scholars. As such, learning political methodology gives you experience with a broader array of specific methods and a focus on deeper, more unifying perspectives even when originating in many other areas.\nThe goal of political methodology and this course is to give you the tools necessary to do high quality scholarly research. This involves (1) learning statistical inference, using facts you know to learn about facts you don’t know, so that you feel completely comfortable using these methods in your own scholarship. With this knowledge, you will be able to easily digest articles about new methods invented after this class ends, implement the methods, apply them to your data, interpret the results, and explain them to others. You will also learn (2) how to write and publish novel substantive contributions in scholarly journals. This sounds hard, but almost everyone gets there and numerous graduate and undergraduate students in this class in previous years have published revised versions of their class papers in scholarly journals as their first professional publication. Large numbers of class papers have also turned into books, senior theses, dissertations, and conference presentations, and many have won awards and have been reported in the media.\n\nReceived the Certificate of Distinction in Teaching from the Derek Bok Center for Teaching and Learning."
  },
  {
    "objectID": "teaching.html#full-courses",
    "href": "teaching.html#full-courses",
    "title": "Teaching",
    "section": "",
    "text": "Harvard College, Spring 2023 and Spring 2024.\nInstructor and course designer.\nA sophomore tutorial on redistricting and elections designed as an introduction to producing original research. A public copy of my syllabus is available here.\n\n\nFull Course Description\n\nFederal, state, and local governments are often built from geographic districts. In redistricting, the districts are drawn for a decade at a time. Commissions, legislatures, and courts draw those geographic districts for future elections. The process of redistricting can be done in as little as a few days, but the maps define constituencies for the next decade. We will study the fundamental building blocks of American democracy. Congress is entirely dependent on how its districts are drawn. Who wins when lines are the drawn? Do district-based systems empower or weaken minority communities? How do we distinguish between effects on race and party? Should we protect incumbents when there are different benefits to accountability and experience? Is it better to have non-partisan groups draw the lines? At the end of the day, how can we say if a map is good or bad?\nIn this course, we’ll explore the many aspects of how districts are drawn in America and their effects on our many geographic democracies. We’ll start by building a baseline understanding of how laws and rules from local, state, and federal entities come together to regulate mapmaking. We’ll explore the ways that we can describe maps quantitatively and qualitatively. We’ll learn from court decisions and recent research into redistricting. All the while, we will draw maps for local, state, and federal districts to better understand the tradeoffs and the high stakes in mapmaking. As new members of the Government department, we will develop your research skills, with an emphasis on how to measure and think about measuring some of the many, often-fuzzy concepts in political science.\nCourse readings draw on legal authorities, law review articles, expert reports, and empirical political science to explore the field. With a diverse set of readings it is important to realize that redistricting is partisan in many ways. We will engage with writers (and researchers) from the left and right. Disagreement is encouraged, as it is vital to advancement of knowledge, but we will make all efforts to ensure that disagreement is both substantive and polite.\n\n\n\n\n\n\n\nHarvard University and Harvard Extension School, Fall 2021.\nTeaching Fellow with Gary King.\nA first graduate course in political methodology.\n\n\nFull Course Description\n\nThis is a first graduate course in political methodology, the methodological subfield of the discipline of political science, akin to econometrics within economics, psychometrics within psychology, sociological methodology within sociology, biostatistics within public health and medicine, and dozens of others. These methodological subfields are increasingly interconnected across disciplines and are often known together under broader monikers, such as data science, applied statistics, or computational social science. Political science is an unusually diverse discipline, welcoming of an exceptionally broad array of approaches, substantive questions, theories, and scholars. As such, learning political methodology gives you experience with a broader array of specific methods and a focus on deeper, more unifying perspectives even when originating in many other areas.\nThe goal of political methodology and this course is to give you the tools necessary to do high quality scholarly research. This involves (1) learning statistical inference, using facts you know to learn about facts you don’t know, so that you feel completely comfortable using these methods in your own scholarship. With this knowledge, you will be able to easily digest articles about new methods invented after this class ends, implement the methods, apply them to your data, interpret the results, and explain them to others. You will also learn (2) how to write and publish novel substantive contributions in scholarly journals. This sounds hard, but almost everyone gets there and numerous graduate and undergraduate students in this class in previous years have published revised versions of their class papers in scholarly journals as their first professional publication. Large numbers of class papers have also turned into books, senior theses, dissertations, and conference presentations, and many have won awards and have been reported in the media.\n\nReceived the Certificate of Distinction in Teaching from the Derek Bok Center for Teaching and Learning."
  },
  {
    "objectID": "teaching.html#short-courses-and-workshops",
    "href": "teaching.html#short-courses-and-workshops",
    "title": "Teaching",
    "section": "Short courses and workshops",
    "text": "Short courses and workshops\n\nlawyR: R Basics for Lawyers\n\n\n\nElection Law Clinic at Harvard Law School, Fall 2022.\nInstructor.\nAn 8 week workshop on basics of R for the social sciences, focusing on relevant issues to election lawyers.\n\n\nMath Prefresher for Political Scientists\n\n\n\nHarvard University, Summer 2022.\nInstructor.\nA PhD-level refresher (often called “math camp”) on math (linear algebra, calculus, and probability theory) and introduction to R for incoming graduate students in the Department of Government.\n\n\nFull Course Description\n\nMath prefresher (or “math camp”) programs in political science invite newly admitted PhD students to graduate school a week or two before their official start date to attend classes on math, statistics, computer science, and related technical material designed specially for them. At Harvard’s Department of Government, we have welcomed students to the prefresher since 1995. No grades are assigned. No individual attendance records are kept. The program is entirely voluntary, but almost all students usually choose to attend the entire program, regardless of background or interests. A faculty advisor organizes and guides the program and senior graduate students serve as instructors.\n\n\n\nThe Data Science of Redistricting\nHarvard College, Spring 2022.\nInstructor with Tyler Simko.\nAn undergraduate workshop on the data science tools used for studying redistricting. Materials for the workshop are available on GitHub.\n\n\nFull Course Description\n\nEvery decade following the US Census, states redraw their district maps for offices up and down the ballot like Congress and state legislatures. This process assigns particular geographic areas to political districts, where candidates then run for office. For example, most of Cambridge is assigned to Massachusetts Congressional Districts #5 (Katherine Clark) and #7 (Ayanna Pressley). Decisions about how to draw these maps are intensely political, as you may have heard about from ongoing lawsuits on racial and partisan “gerrymandering” and “vote dilution” in states like Alabama, New York, Ohio, and Pennsylvania.\nWhat is required when drawing a map? How do we evaluate the “fairness” of a particular map? How can we use data to evaluate these claims? In this workshop, we will cover the data science of redistricting. Starting with a discussion of the legal context and requirements in redistricting, we will think about how to evaluate maps in measurable ways. Along the way, we’ll cover tools for creating maps in R. Then, we will cover creating our own maps using algorithmic “simulation” methods, which create alternative plans according to particular criteria that statutes specify. These techniques are being used in cases across the country right now, and this workshop will provide you with all the background information you need to understand, evaluate, and maybe even contribute your own analyses."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "(with Cory McCartan, Shiro Kuriwaki, Tyler Simko, and Kosuke Imai). 2024. Science Advances.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nThe United States Census Bureau faces a difficult trade-off between the accuracy of Census statistics and the protection of individual information. We conduct the first independent evaluation of bias and noise induced by the Bureau’s two main disclosure avoidance systems: the TopDown algorithm employed for the 2020 Census and the swapping algorithm implemented for the 1990, 2000, and 2010 Censuses. Our evaluation leverages the recent release of the Noisy Measure File (NMF) as well as the availability of two independent runs of the TopDown algorithm applied to the 2010 decennial Census. We find that the NMF contains too much noise to be directly useful alone, especially for Hispanic and multiracial populations. TopDown’s post-processing dramatically reduces the NMF noise and produces similarly accurate data to swapping in terms of bias and noise. These patterns hold across census geographies with varying population sizes and racial diversity. While the estimated errors for both TopDown and swapping are generally no larger than other sources of Census error, they can be relatively substantial for geographies with small total populations.\n\n\n\n\n@misc{kenn:etal:24b,\n  author = {Christopher T. Kenny and Cory McCartan and Shiro Kuriwaki and Tyler Simko and Kosuke Imai},\n  title = {Evaluating bias and noise induced by the U.S. Census Bureau’s privacy protection methods},\n  journal = {Science Advances},\n  volume = {10},\n  number = {18},\n  pages = {eadl2524},\n  year = {2024},\n  doi = {10.1126/sciadv.adl2524},\n  URL = {https://www.science.org/doi/abs/10.1126/sciadv.adl2524},\n eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.adl2524},\n}\n\n\n\n\n\n(with Cory McCartan, Tyler Simko, and Kosuke Imai). 2024. PNAS.\n\n\nFirst paragraph\n\n\nBibTeX\n\n\n\n\nCurrent and former Census Bureau officials Jarmin et al. argue that differential privacy, which underlies the 2020 Census’s Disclosure Avoidance System (DAS), satisfies more desirable theoretical criteria than alternatives. They provide detailed criticisms of many published evaluations of the 2020 DAS, including our work. In this letter, we show that their criticisms are unfounded, grossly mischaracterize our research, and ignore critical issues that merit public discussion.\n\n\n\n\n@article{kenn:etal:24a,\n  title={Census officials must constructively engage with independent evaluations},\n  author={Kenny, Christopher T. and McCartan, Cory and Simko, Tyler and Imai, Kosuke},\n  journal={Proceedings of the National Academy of Sciences},\n  volume={121},\n  number={11},\n  pages={e2321196121},\n  year={2024},\n  publisher={National Acad Sciences}\n}\n\n\n\n\n\n(with Cory McCartan, Tyler Simko, Shiro Kuriwaki, and Kosuke Imai). 2023. PNAS.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nCongressional district lines in many U.S. states are drawn by partisan actors, raising concerns about gerrymandering. To isolate the electoral impact of gerrymandering from the effects of other factors including geography and redistricting rules, we compare predicted election outcomes under the enacted plan with those under a large sample of non-partisan, simulated alternative plans for all states. We find that partisan gerrymandering is widespread in the 2020 redistricting cycle, but most of the bias it creates cancels at the national level, giving Republicans two additional seats, on average. In contrast, moderate pro-Republican bias due to geography and redistricting rules remains. Finally, we find that partisan gerrymandering reduces electoral competition and makes the House’s partisan composition less responsive to shifts in the national vote.\n\n\n\n\n@article{kenn:etal:23b,\nauthor = {Christopher T. Kenny and Cory McCartan and Tyler Simko and Shiro Kuriwaki and Kosuke Imai},\ntitle = {Widespread partisan gerrymandering mostly cancels nationally, but reduces electoral competition},\njournal = {Proceedings of the National Academy of Sciences},\nvolume = {120},\nnumber = {25},\npages = {e2217322120},\nyear = {2023},\ndoi = {10.1073/pnas.2217322120},\nURL = {https://www.pnas.org/doi/abs/10.1073/pnas.2217322120},\neprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2217322120},\n}\n\n\n\n\n\n(with Shiro Kuriwaki, Cory McCartan, Evan T. R. Rosenman, and Tyler Simko). 2023. Harvard Data Science Review.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nIn “Differential Perspectives: Epistemic Disconnects Surrounding the US Census Bureau’s Use of Differential Privacy,” boyd and Sarathy argue that empirical evaluations of the Census Disclosure Avoidance System (DAS), including our published analysis, failed to recognize how the benchmark data against which the 2020 DAS was evaluated is never a ground truth of population counts. In this commentary, we explain why policy evaluation, which was the main goal of our analysis, is still meaningful without access to a perfect ground truth. We also point out that our evaluation leveraged features specific to the decennial Census and redistricting data, such as block-level population invariance under swapping and voter file racial identification, better approximating a comparison with the ground truth. Lastly, we show that accurate statistical predictions of individual race based on the Bayesian Improved Surname Geocoding, while not a violation of differential privacy, substantially increases the disclosure risk of private information the Census Bureau sought to protect. We conclude by arguing that policy makers must confront a key trade-off between data utility and privacy protection, and an epistemic disconnect alone is insufficient to explain disagreements between policy choices.\n\n\n\n\n@article{kenn:etal:23,\n    author = {Kenny, Christopher T. and Kuriwaki, Shiro and McCartan, Cory and Rosenman, Evan T. R. and Simko, Tyler and Imai, Kosuke},\n    journal = {Harvard Data Science Review},\n    number = {Special Issue 2},\n    year = {2023},\n    month = {jan 31},\n    note = {https://hdsr.mitpress.mit.edu/pub/6ffzuq19},\n    publisher = {},\n    title = {Comment: The {Essential} {Role} of {Policy} {Evaluation} for the 2020 {Census} {DisclosureAvoidance} {System}},\n    volume = { },\n}\n\n\n\n\n\n(with Cory McCartan, Tyler Simko, George Garcia III, Kevin Wang, Melissa Wu, Shiro Kuriwaki, and Kosuke Imai). 2022. Scientific Data.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nThis article introduces the 50stateSimulations, a collection of simulated congressional districting plans and underlying code developed by the Algorithm-Assisted Redistricting Methodology (ALARM) Project. The 50stateSimulations allow for the evaluation of enacted and other congressional redistricting plans in the United States. While the use of redistricting simulation algorithms has become standard in academic research and court cases, any simulation analysis requires non-trivial efforts to combine multiple data sets, identify state-specific redistricting criteria, implement complex simulation algorithms, and summarize and visualize simulation outputs. We have developed a complete workflow that facilitates this entire process of simulation-based redistricting analysis for the congressional districts of all 50 states. The resulting 50stateSimulations include ensembles of simulated 2020 congressional redistricting plans and necessary replication data. We also provide the underlying code, which serves as a template for customized analyses. All data and code are free and publicly available. This article details the design, creation, and validation of the data.\n\n\n\n\n@article{50statesSimulations,\n  title = {Simulated Redistricting Plans for the Analysis and Evaluation of Redistricting in the {{United States}}},\n  author = {McCartan, Cory and Kenny, Christopher T. and Simko, Tyler and Garcia, George and Wang, Kevin and Wu, Melissa and Kuriwaki, Shiro and Imai, Kosuke},\n  year = {2022},\n  month = nov,\n  journal = {Scientific Data},\n  volume = {9},\n  number = {1},\n  pages = {689},\n  issn = {2052-4463},\n  doi = {10.1038/s41597-022-01808-2},\n}\n\n\n\n\n\n(with Shiro Kuriwaki, Cory McCartan, Evan T. R. Rosenman, and Tyler Simko). 2021. Science Advances.\nCovered by The Washington Post, Associated Press, NC Policy Watch, and The Harvard Crimson.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nThe US Census Bureau plans to protect the privacy of 2020 Census respondents through its Disclosure Avoidance System (DAS), which attempts to achieve differential privacy guarantees by adding noise to the Census microdata. By applying redistricting simulation and analysis methods to DAS-protected 2010 Census data, we find that the protected data are not of sufficient quality for redistricting purposes. We demonstrate that the injected noise makes it impossible for states to accurately comply with the One Person, One Vote principle. Our analysis finds that the DAS-protected data are biased against certain areas, depending on voter turnout and partisan and racial composition, and that these biases lead to large and unpredictable errors in the analysis of partisan and racial gerrymanders. Finally, we show that the DAS algorithm does not universally protect respondent privacy. Based on the names and addresses of registered voters, we are able to predict their race as accurately using the DAS-protected data as when using the 2010 Census data. Despite this, the DAS-protected data can still inaccurately estimate the number of majority-minority districts. We conclude with recommendations for how the Census Bureau should proceed with privacy protection for the 2020 Census.\n\n\n\n\n@article{kenn:etal:21,\nauthor = {Christopher T. Kenny  and Shiro Kuriwaki  and Cory McCartan  and Evan T. R. Rosenman  and Tyler Simko  and Kosuke Imai },\ntitle = {The Use of Differential Privacy for Census Data and its Impact on Redistricting: The Case of the 2020 U.S. Census},\njournal = {Science Advances},\nvolume = {7},\nnumber = {41},\npages = {eabk3283},\nyear = {2021},\ndoi = {10.1126/sciadv.abk3283},\nURL = {https://www.science.org/doi/abs/10.1126/sciadv.abk3283},\neprint = {https://www.science.org/doi/pdf/10.1126/sciadv.abk3283},\n}\n\n\n\n\n\n(with Benjamin Fifield, Kosuke Imai, and Jun Kawahara). 2020. Statistics and Public Policy.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nAs granular data about elections and voters become available, redistricting simulation methods are playing an increasingly important role when legislatures adopt redistricting plans and courts determine their legality. These simulation methods are designed to yield a representative sample of all redistricting plans that satisfy statutory guidelines and requirements such as contiguity, population parity, and compactness. A proposed redistricting plan can be considered gerrymandered if it constitutes an outlier relative to this sample according to partisan fairness metrics. Despite their growing use, an insufficient effort has been made to empirically validate the accuracy of the simulation methods. We apply a recently developed computational method that can efficiently enumerate all possible redistricting plans and yield an independent sample from this population. We show that this algorithm scales to a state with a couple of hundred geographical units. Finally, we empirically examine how existing simulation methods perform on realistic validation datasets.\n\n\n\n\n@article{fife:etal:20,\n  author = {Benjamin Fifield and Kosuke Imai and Jun Kawahara and Christopher T. Kenny},\n  title = {The Essential Role of Empirical Validation in Legislative Redistricting Simulation},\n  journal = {Statistics and Public Policy},\n  volume = {7},\n  number = {1},\n  pages = {52-68},\n  year  = {2020},\n  publisher = {Taylor & Francis},\n  doi = {10.1080/2330443X.2020.1791773},\n  URL = {https://doi.org/10.1080/2330443X.2020.1791773},\n  eprint = {https://doi.org/10.1080/2330443X.2020.1791773},\n}"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "",
    "text": "(with Cory McCartan, Shiro Kuriwaki, Tyler Simko, and Kosuke Imai). 2024. Science Advances.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nThe United States Census Bureau faces a difficult trade-off between the accuracy of Census statistics and the protection of individual information. We conduct the first independent evaluation of bias and noise induced by the Bureau’s two main disclosure avoidance systems: the TopDown algorithm employed for the 2020 Census and the swapping algorithm implemented for the 1990, 2000, and 2010 Censuses. Our evaluation leverages the recent release of the Noisy Measure File (NMF) as well as the availability of two independent runs of the TopDown algorithm applied to the 2010 decennial Census. We find that the NMF contains too much noise to be directly useful alone, especially for Hispanic and multiracial populations. TopDown’s post-processing dramatically reduces the NMF noise and produces similarly accurate data to swapping in terms of bias and noise. These patterns hold across census geographies with varying population sizes and racial diversity. While the estimated errors for both TopDown and swapping are generally no larger than other sources of Census error, they can be relatively substantial for geographies with small total populations.\n\n\n\n\n@misc{kenn:etal:24b,\n  author = {Christopher T. Kenny and Cory McCartan and Shiro Kuriwaki and Tyler Simko and Kosuke Imai},\n  title = {Evaluating bias and noise induced by the U.S. Census Bureau’s privacy protection methods},\n  journal = {Science Advances},\n  volume = {10},\n  number = {18},\n  pages = {eadl2524},\n  year = {2024},\n  doi = {10.1126/sciadv.adl2524},\n  URL = {https://www.science.org/doi/abs/10.1126/sciadv.adl2524},\n eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.adl2524},\n}\n\n\n\n\n\n(with Cory McCartan, Tyler Simko, and Kosuke Imai). 2024. PNAS.\n\n\nFirst paragraph\n\n\nBibTeX\n\n\n\n\nCurrent and former Census Bureau officials Jarmin et al. argue that differential privacy, which underlies the 2020 Census’s Disclosure Avoidance System (DAS), satisfies more desirable theoretical criteria than alternatives. They provide detailed criticisms of many published evaluations of the 2020 DAS, including our work. In this letter, we show that their criticisms are unfounded, grossly mischaracterize our research, and ignore critical issues that merit public discussion.\n\n\n\n\n@article{kenn:etal:24a,\n  title={Census officials must constructively engage with independent evaluations},\n  author={Kenny, Christopher T. and McCartan, Cory and Simko, Tyler and Imai, Kosuke},\n  journal={Proceedings of the National Academy of Sciences},\n  volume={121},\n  number={11},\n  pages={e2321196121},\n  year={2024},\n  publisher={National Acad Sciences}\n}\n\n\n\n\n\n(with Cory McCartan, Tyler Simko, Shiro Kuriwaki, and Kosuke Imai). 2023. PNAS.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nCongressional district lines in many U.S. states are drawn by partisan actors, raising concerns about gerrymandering. To isolate the electoral impact of gerrymandering from the effects of other factors including geography and redistricting rules, we compare predicted election outcomes under the enacted plan with those under a large sample of non-partisan, simulated alternative plans for all states. We find that partisan gerrymandering is widespread in the 2020 redistricting cycle, but most of the bias it creates cancels at the national level, giving Republicans two additional seats, on average. In contrast, moderate pro-Republican bias due to geography and redistricting rules remains. Finally, we find that partisan gerrymandering reduces electoral competition and makes the House’s partisan composition less responsive to shifts in the national vote.\n\n\n\n\n@article{kenn:etal:23b,\nauthor = {Christopher T. Kenny and Cory McCartan and Tyler Simko and Shiro Kuriwaki and Kosuke Imai},\ntitle = {Widespread partisan gerrymandering mostly cancels nationally, but reduces electoral competition},\njournal = {Proceedings of the National Academy of Sciences},\nvolume = {120},\nnumber = {25},\npages = {e2217322120},\nyear = {2023},\ndoi = {10.1073/pnas.2217322120},\nURL = {https://www.pnas.org/doi/abs/10.1073/pnas.2217322120},\neprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2217322120},\n}\n\n\n\n\n\n(with Shiro Kuriwaki, Cory McCartan, Evan T. R. Rosenman, and Tyler Simko). 2023. Harvard Data Science Review.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nIn “Differential Perspectives: Epistemic Disconnects Surrounding the US Census Bureau’s Use of Differential Privacy,” boyd and Sarathy argue that empirical evaluations of the Census Disclosure Avoidance System (DAS), including our published analysis, failed to recognize how the benchmark data against which the 2020 DAS was evaluated is never a ground truth of population counts. In this commentary, we explain why policy evaluation, which was the main goal of our analysis, is still meaningful without access to a perfect ground truth. We also point out that our evaluation leveraged features specific to the decennial Census and redistricting data, such as block-level population invariance under swapping and voter file racial identification, better approximating a comparison with the ground truth. Lastly, we show that accurate statistical predictions of individual race based on the Bayesian Improved Surname Geocoding, while not a violation of differential privacy, substantially increases the disclosure risk of private information the Census Bureau sought to protect. We conclude by arguing that policy makers must confront a key trade-off between data utility and privacy protection, and an epistemic disconnect alone is insufficient to explain disagreements between policy choices.\n\n\n\n\n@article{kenn:etal:23,\n    author = {Kenny, Christopher T. and Kuriwaki, Shiro and McCartan, Cory and Rosenman, Evan T. R. and Simko, Tyler and Imai, Kosuke},\n    journal = {Harvard Data Science Review},\n    number = {Special Issue 2},\n    year = {2023},\n    month = {jan 31},\n    note = {https://hdsr.mitpress.mit.edu/pub/6ffzuq19},\n    publisher = {},\n    title = {Comment: The {Essential} {Role} of {Policy} {Evaluation} for the 2020 {Census} {DisclosureAvoidance} {System}},\n    volume = { },\n}\n\n\n\n\n\n(with Cory McCartan, Tyler Simko, George Garcia III, Kevin Wang, Melissa Wu, Shiro Kuriwaki, and Kosuke Imai). 2022. Scientific Data.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nThis article introduces the 50stateSimulations, a collection of simulated congressional districting plans and underlying code developed by the Algorithm-Assisted Redistricting Methodology (ALARM) Project. The 50stateSimulations allow for the evaluation of enacted and other congressional redistricting plans in the United States. While the use of redistricting simulation algorithms has become standard in academic research and court cases, any simulation analysis requires non-trivial efforts to combine multiple data sets, identify state-specific redistricting criteria, implement complex simulation algorithms, and summarize and visualize simulation outputs. We have developed a complete workflow that facilitates this entire process of simulation-based redistricting analysis for the congressional districts of all 50 states. The resulting 50stateSimulations include ensembles of simulated 2020 congressional redistricting plans and necessary replication data. We also provide the underlying code, which serves as a template for customized analyses. All data and code are free and publicly available. This article details the design, creation, and validation of the data.\n\n\n\n\n@article{50statesSimulations,\n  title = {Simulated Redistricting Plans for the Analysis and Evaluation of Redistricting in the {{United States}}},\n  author = {McCartan, Cory and Kenny, Christopher T. and Simko, Tyler and Garcia, George and Wang, Kevin and Wu, Melissa and Kuriwaki, Shiro and Imai, Kosuke},\n  year = {2022},\n  month = nov,\n  journal = {Scientific Data},\n  volume = {9},\n  number = {1},\n  pages = {689},\n  issn = {2052-4463},\n  doi = {10.1038/s41597-022-01808-2},\n}\n\n\n\n\n\n(with Shiro Kuriwaki, Cory McCartan, Evan T. R. Rosenman, and Tyler Simko). 2021. Science Advances.\nCovered by The Washington Post, Associated Press, NC Policy Watch, and The Harvard Crimson.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nThe US Census Bureau plans to protect the privacy of 2020 Census respondents through its Disclosure Avoidance System (DAS), which attempts to achieve differential privacy guarantees by adding noise to the Census microdata. By applying redistricting simulation and analysis methods to DAS-protected 2010 Census data, we find that the protected data are not of sufficient quality for redistricting purposes. We demonstrate that the injected noise makes it impossible for states to accurately comply with the One Person, One Vote principle. Our analysis finds that the DAS-protected data are biased against certain areas, depending on voter turnout and partisan and racial composition, and that these biases lead to large and unpredictable errors in the analysis of partisan and racial gerrymanders. Finally, we show that the DAS algorithm does not universally protect respondent privacy. Based on the names and addresses of registered voters, we are able to predict their race as accurately using the DAS-protected data as when using the 2010 Census data. Despite this, the DAS-protected data can still inaccurately estimate the number of majority-minority districts. We conclude with recommendations for how the Census Bureau should proceed with privacy protection for the 2020 Census.\n\n\n\n\n@article{kenn:etal:21,\nauthor = {Christopher T. Kenny  and Shiro Kuriwaki  and Cory McCartan  and Evan T. R. Rosenman  and Tyler Simko  and Kosuke Imai },\ntitle = {The Use of Differential Privacy for Census Data and its Impact on Redistricting: The Case of the 2020 U.S. Census},\njournal = {Science Advances},\nvolume = {7},\nnumber = {41},\npages = {eabk3283},\nyear = {2021},\ndoi = {10.1126/sciadv.abk3283},\nURL = {https://www.science.org/doi/abs/10.1126/sciadv.abk3283},\neprint = {https://www.science.org/doi/pdf/10.1126/sciadv.abk3283},\n}\n\n\n\n\n\n(with Benjamin Fifield, Kosuke Imai, and Jun Kawahara). 2020. Statistics and Public Policy.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nAs granular data about elections and voters become available, redistricting simulation methods are playing an increasingly important role when legislatures adopt redistricting plans and courts determine their legality. These simulation methods are designed to yield a representative sample of all redistricting plans that satisfy statutory guidelines and requirements such as contiguity, population parity, and compactness. A proposed redistricting plan can be considered gerrymandered if it constitutes an outlier relative to this sample according to partisan fairness metrics. Despite their growing use, an insufficient effort has been made to empirically validate the accuracy of the simulation methods. We apply a recently developed computational method that can efficiently enumerate all possible redistricting plans and yield an independent sample from this population. We show that this algorithm scales to a state with a couple of hundred geographical units. Finally, we empirically examine how existing simulation methods perform on realistic validation datasets.\n\n\n\n\n@article{fife:etal:20,\n  author = {Benjamin Fifield and Kosuke Imai and Jun Kawahara and Christopher T. Kenny},\n  title = {The Essential Role of Empirical Validation in Legislative Redistricting Simulation},\n  journal = {Statistics and Public Policy},\n  volume = {7},\n  number = {1},\n  pages = {52-68},\n  year  = {2020},\n  publisher = {Taylor & Francis},\n  doi = {10.1080/2330443X.2020.1791773},\n  URL = {https://doi.org/10.1080/2330443X.2020.1791773},\n  eprint = {https://doi.org/10.1080/2330443X.2020.1791773},\n}"
  },
  {
    "objectID": "research.html#working-papers",
    "href": "research.html#working-papers",
    "title": "Research",
    "section": "Working Papers",
    "text": "Working Papers\n\nIndividual and Differential Harm in Redistricting\n(with Cory McCartan). Current version: 2022-06-24.\n\n\nAbstract\n\n\nBibTeX\n\n\n\n\nSocial scientists have developed dozens of measures for assessing partisan bias in redistricting.But these measures cannot be easily adapted to other groups, including those defined by race, class, or geography. Nor are they applicable to single- or no-party contexts such as local redistricting. To overcome these limitations, we propose a unified framework of harm for evaluating the impacts of a districting plan on individual voters and the groups to which they belong. We consider a voter harmed if their chosen candidate is not elected under the current plan, but would be under a different plan. Harm improves on existing measures by both focusing on the choices of individual voters and directly incorporating counterfactual plans. We discuss strategies for estimating harm, and demonstrate the utility of our framework through analyses of partisan gerrymandering in New Jersey, voting rights litigation in Alabama, and racial dynamics of Boston City Council elections.\n\n\n\n\n@misc{mcca:kenn:22,\n  doi = {10.31235/osf.io/nc2x7},\n  url = {https://osf.io/preprints/socarxiv/nc2x7/},\n  author = {McCartan, Cory and Kenny, Christopher T.},\n  keywords = {representation, redistricting, voting rights, individual harm},\n  title = {Individual and Differential Harm in Redistricting},\n  publisher = {SocArXiv},\n  year = {2022}\n}\n\n\n\n\nInequality in Administrative Democracy: Large-Sample Evidence from American Financial Regulation\n(with Daniel P. Carpenter, Angelo Dagonel, Devin Judge-Lord, Brian Libgober, Steven Rashin, Jacob Waggoner, and Susan Webb Yackee)\nAwarded the 2021 Herbert Kaufman Award.\n\n\nAbstract\n\n\n\n\nResearch on inequality overlooks administrative policymaking, where most U.S. law is currently made, under pressure from vast flows of money, lobbying, and political mobilization. Analyzing a new database of over 260,000 comments on agency rules implementing the Dodd-Frank Act, we identify the lobbying activities of over 6,000 organizations. Leveraging measures of organizations’ wealth, participation in administrative politics, sophistication, and lobbying success, we provide the first large-scale assessment of wealth-based inequality in agency rulemaking. We find that wealthier organizations are more likely to participate in rulemaking and enjoy more success in shifting the content of federal agency rules. These patterns are not explained by membership differentials. More profit-driven organizations are also more likely to participate and enjoy more success in shifting the content of federal agency rules. Wealthier organizations’ ability to marshal legal and technical expertise appears to be a key mechanism by which wealth leads to lobbying success."
  },
  {
    "objectID": "research.html#works-in-progress",
    "href": "research.html#works-in-progress",
    "title": "Research",
    "section": "Works-in-Progress",
    "text": "Works-in-Progress\n\nAlgorithm-Assisted Redistricting Methodology\n(with Kosuke Imai, Cory McCartan, and Tyler Simko). Book project."
  },
  {
    "objectID": "research.html#public-writing",
    "href": "research.html#public-writing",
    "title": "Research",
    "section": "Public Writing",
    "text": "Public Writing\n\nExpert Report in Sakhnovsky, et al v. City of Daytona Beach\nCase No. 2024 10140 CICI. Expert report on census data and map drawing for Daytona Beach. 2024.\n\n\nRedistricting Process Reform\nThe University of Chicago Center for Effective Government’s Democracy Reform Primer Series. 2024. With Steve Ansolabehere.\n\n\nAmici Curiae Brief of Fair Districts Georgia and Election Law Clinic in Support of Plaintiffs\nAlpha Phi Alpha Fraternity, Inc. et al. v. Brad Raffensperger. 2021. With the Election Law Clinic at Harvard Law School.\n\n\nMaryland Congressional District Memo\nMemo to the Maryland Redistricting Commission. 2021. With Jonathan Rodden."
  },
  {
    "objectID": "posts/2023-12-21-cran-wrapped/index.html",
    "href": "posts/2023-12-21-cran-wrapped/index.html",
    "title": "Packages 2023 Wrapped",
    "section": "",
    "text": "With CRAN closing its submission queue for the holiday break tomorrow, it seems the right time to look over the past year. I maintain and contribute to quite a few packages on CRAN, primarily focused on social science data and methods. This year included adding 5 new packages to CRAN, with a minor shift towards tidy interfaces for web-based APIs. Below, I talk about some of the new packages, updates to existing packages, and then look at the downloads for all of my packages."
  },
  {
    "objectID": "posts/2023-12-21-cran-wrapped/index.html#new-packages",
    "href": "posts/2023-12-21-cran-wrapped/index.html#new-packages",
    "title": "Packages 2023 Wrapped",
    "section": "New packages",
    "text": "New packages\nI’ve added five new packages to CRAN this year.\n\napportion\n\n\n\napportion is a relatively simple package. It calculates apportionments, the allocation of seats to states based on population. It includes functions for the most common apportionment methods:\n\nthe Adams Method (app_adams())\nthe Balinski Young Method (app_balinski_young())\nthe Dean Method (app_dean())\nthe D’Hondt Method (app_dhondt())\nthe Hamilton-Vinton Method (app_hamilton_vinton())\nthe Huntington-Hill Method (app_huntington_hill())\nthe Jefferson Method (app_jefferson())\nthe Webster Method (app_webster())\n\n\n\ncrayons\n\n\n\ncrayons takes a few dozen packs of crayons and turns them into color palettes. The package itself is pretty thin, relying on scale_color_crayons() and scale_fill_crayons() to create the palettes.\n\nlibrary(ggplot2)\nlibrary(crayons)\n\nmpg |&gt;\n  ggplot() + \n  geom_point(aes(displ, hwy, colour = class)) + \n  scale_color_crayons(palette = 'original') + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\ngptzeror\n\n\n\nIn the peak of worries about students using ChatGPT, GPTZero offered an API for estimating if text was human or AI generated. I wrapped an R interface to this API in gptzeror. It seems to work somewhat well, but is far from perfect. The risk of false positives is really high, so I’m not using this without other substantial evidence.     \n\n\nfeltr\n\n\n\nfeltr is a package for working with the Felt. It covers all of the API endpoints, which lets you upload shapes from R directly to Felt. You can delete them, update them, extract comments, and more. I covered an application of this package in a prior blog post on loosely focused on Dunkin Donuts in Cambridge.      \n\n\nbskyr\n\n\n\nbskyr is a package for working with the Bluesky Social API. It’s focused largely on collecting tidy data from Bluesky. Given the decentralized nature of Bluesky, it seems like it has immense opportunity for social science research. You can design entire feeds as treatments, letting people push further in treatment arms without the same need for industry-academy partnerships as with Facebook or X/Twitter.\nOf course, it also contains all of the tools for posting and otherwise interacting with Bluesky. I even have a small bot going which tracks CRAN Updates that I’ll cover soon in a holiday-times blog post. It’s run entirely through bskyr, which has been working even better than expected."
  },
  {
    "objectID": "posts/2023-12-21-cran-wrapped/index.html#updates-to-existing-packages",
    "href": "posts/2023-12-21-cran-wrapped/index.html#updates-to-existing-packages",
    "title": "Packages 2023 Wrapped",
    "section": "Updates to existing packages",
    "text": "Updates to existing packages\nMy CRAN updates have not all been new packages, I also maintain a handful of packages. This year, I’ve made 14 submissions across 10 packages. 7 of these are related to the 5 new packages above (5 first submission + 2 updates).\n\n\n\n\n\n\n\n\nName\nVersion\nDate\nTitle\n\n\n\n\ncvap\n0.1.3\n2023-03-17\nCitizen Voting Age Population\n\n\nredist\n4.1.0\n2023-03-19\nSimulation Methods for Legislative Redistricting\n\n\nredist\n4.1.1\n2023-04-03\nSimulation Methods for Legislative Redistricting\n\n\ngeomander\n2.2.1\n2023-04-16\nGeographic Tools for Studying Gerrymandering\n\n\ncvap\n0.1.4\n2023-07-01\nCitizen Voting Age Population\n\n\ntinytiger\n0.0.8\n2023-10-17\nLightweight Interface to TIGER/Line Shapefiles\n\n\nredistmetrics\n1.0.7\n2023-12-12\nRedistricting Metrics\n\n\n\n\n\n\n\nUpdates to cvap and tinytiger added support for new years of Census Bureau data. redist and redistmetrics each saw primarily bug fixes and performance improvements, without any major changes. geomander similar saw mostly bug fixes. Its one update also drastically cleaned up the dependencies to make the package easier to install."
  },
  {
    "objectID": "posts/2023-12-21-cran-wrapped/index.html#footnotes",
    "href": "posts/2023-12-21-cran-wrapped/index.html#footnotes",
    "title": "Packages 2023 Wrapped",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDear Spotify, please do something like this next year to account late releases like 1989 in October.↩︎\nContributions welcome!↩︎"
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html",
    "title": "Creating Quarto Journal Article Templates",
    "section": "",
    "text": "This post is largely written for my future self. If you’re not me and you’ve ended up here, I do hope that this is helpful for you, but it may fall below or above your current experience. I’ll include informative links as I think of them to help with the latter. For the former, this post may be too specific to my process to be helpful.\n\n\nAs with all blog posts, I’m assuming you both know everything and nothing. You have every clue how Quarto, Markdown, LaTeX, pandoc, and Lua work and simultaneously have never used anything but Microsoft Word.\nMore realistically, you need some experience with one of Quarto, RMarkdown, or Markdown and definitely some experience with LaTeX.\nYou needn’t be an expert on the first part, but if you’re new to LaTeX, take some time to write a few documents before trying to build templates that rely on it for pdf-output.\n\n\n\nI’ve made a handful of templates for Quarto and RMarkdown. A few of them are public, but many of them are hacky one-off solutions to some problem. I’m focused here on the more-focused public template, where you want it to work in general for a journal’s set of problems.\nPerhaps most importantly, I’ve got about eight years of self-taught LaTeX experience. That’s good for me and bad for you (or maybe good for you and bad for me). My solutions to problems often (1) produce correct output and (2) are sub-optimal. That’s not the best, but if you care about (1) more, then, hey, we’ve got some work to do.\nFor full context: This post was mostly written while I was making a PNAS template for Quarto. Previously, I used the rticles::pnas_template() but (at the time of writing) I was trying to get a better sense of how Quarto built things. So, I have a sense of what it should look like, but no guarantee of understanding how the details work."
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#background",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#background",
    "title": "Creating Quarto Journal Article Templates",
    "section": "",
    "text": "This post is largely written for my future self. If you’re not me and you’ve ended up here, I do hope that this is helpful for you, but it may fall below or above your current experience. I’ll include informative links as I think of them to help with the latter. For the former, this post may be too specific to my process to be helpful.\n\n\nAs with all blog posts, I’m assuming you both know everything and nothing. You have every clue how Quarto, Markdown, LaTeX, pandoc, and Lua work and simultaneously have never used anything but Microsoft Word.\nMore realistically, you need some experience with one of Quarto, RMarkdown, or Markdown and definitely some experience with LaTeX.\nYou needn’t be an expert on the first part, but if you’re new to LaTeX, take some time to write a few documents before trying to build templates that rely on it for pdf-output.\n\n\n\nI’ve made a handful of templates for Quarto and RMarkdown. A few of them are public, but many of them are hacky one-off solutions to some problem. I’m focused here on the more-focused public template, where you want it to work in general for a journal’s set of problems.\nPerhaps most importantly, I’ve got about eight years of self-taught LaTeX experience. That’s good for me and bad for you (or maybe good for you and bad for me). My solutions to problems often (1) produce correct output and (2) are sub-optimal. That’s not the best, but if you care about (1) more, then, hey, we’ve got some work to do.\nFor full context: This post was mostly written while I was making a PNAS template for Quarto. Previously, I used the rticles::pnas_template() but (at the time of writing) I was trying to get a better sense of how Quarto built things. So, I have a sense of what it should look like, but no guarantee of understanding how the details work."
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#making-the-template",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#making-the-template",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Making the Template",
    "text": "Making the Template\nFirst things first, we need to initialize the template. We can do that in the terminal with:\nquarto create extension journal\nIt’ll prompt you to give it a name. Names tend to be short and normally match a common journal abbrevation. Quarto has a list of templates, as does Awesome Quarto. Some names I’ve used:\n\n\n\njournal\nname\n\n\n\n\nAPSR\napsr\n\n\nPNAS\npnas\n\n\nPA, PSRM, BJPS, …\ncambridge-medium\n\n\nScientific Data\nscientific-data\n\n\n\nAs you can see, some are obvious. The Cambridge medium one tripped me up a bit, as it was intended for at least five journals. As such, I just named it for the template and added text to make it clear what journals it was for.\nNow, once you have a name, it creates a new R project named for the name you provided. You can open that and give the structure a look.\n\n\n\n\n\n\nNote\n\n\n\nFor the rest of this post, I’ll use building a PNAS template as the example. pnas can be replaced with the name you chose above.\n\n\nOn creation, the file structure should look something like this:\n.\n├── bibliography.bib\n├── pnas.Rproj\n├── README.md\n├── template.qmd\n└── _extensions\n    └── pnas\n        ├── header.tex\n        ├── pnas.lua\n        ├── styles.css\n        └── _extension.yml\nFor a journal template, we’re going to have to augment the _extensions/pnas folder. For now some quick bits of information on included files:\n\nREADME.md is the general readme for the whole template. It’ll hold installation instructions, option documentation, and anything else you need users to know.\ntemplate.qmds is where our example template goes.\nbibliography.bib is a placeholder bibtex file.\n_extensions/pnas/_extension.yml will hold the metadata for the template. This tells Quarto which files to use and such.\n\n_extensions/pnas/header.tex is included in the default template and will be inserted in the header of the LaTeX template.\n\n_extensions/pnas/styles.css can probably be deleted at this point, unless you want to make a website version of the template, but you probably aren’t if you’re making a journal template.\n_extensions/pnas/pnas.lua is the default Lua filter. Lua is powerful, but only necessary for more complicated features.\n\n\nThe README\nThe first thing I edit is the README. It can’t be finished yet, but I find it helpful to organize right away.\nUsing RStudio’s find+replace, I change the github organization placeholder to my GitHub account name (christopherkenny). Then, using the same tool, I replace the file name and title with template name from before. (If you now feeling anxious about the name, this is the right time to fix it before doing anything.) Then, I remove the (now complete) TODOs for those.\nIn the “Example” section, I then start to add more info. First, I add some template code to include a screenshot.\n&lt;!-- pdftools::pdf_convert('template.pdf',pages = 1) \n![[template.qmd](template.qmd)](template_1.png) --&gt;\nOnce the template is finished, I’ll run the first line in R and uncomment the second. Including a screenshot is inspired by Cory McCartan’s pre-print template. I think it’s a nice touch.\nThe options section is helpful for the end user, but may not be super clear right now. I think it is best to hold off on that and figure it out later.\nFor now, the last thing I’ll do is add a bit of information on licensing. Odds are good that I’m working off of some official or officially endorsed template. Even if it isn’t official, you want to give credit to the LaTeX wizardry going on behind the scenes. My license section typically takes the form, using {glue} syntax for placeholders:\n## License\nThis modifies the {Owner} {What} Template, available at &lt;{some_link}&gt;.\nThe original template is licensed under the [{license_name}]({license_link}).\n{any_other_required_notes_about_modifications}"
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#adding-latex-files-into-the-template",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#adding-latex-files-into-the-template",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Adding LaTeX Files into the Template",
    "text": "Adding LaTeX Files into the Template\nBefore you dive into building the Quarto part of the template, I’ll next download any relevant files for the LaTeX part of the template. For the PNAS template, I downloaded the source files from Overleaf.\nFrom the template, I extract all of the LaTeX-related files (.bst, .sty, .cls, and in this case .ldf files). Those files go into the _extensions/pnas folder. Any similar files that we want to be available to the template itself should go there.\nAny files that are necessary for demonstrating the template can go into a higher-level folder. For example, in the PNAS template, they use a frog as the placeholder image. I make a folder called figs/ that can hold such things.\nAt this point, my file structure looks like:\n.\n├── bibliography.bib\n├── figs\n│   └── frog.pdf\n├── pnas.Rproj\n├── README.md\n├── template.qmd\n└── _extensions\n    └── pnas\n        ├── header.tex\n        ├── jabbrv-ltwa-all.ldf\n        ├── jabbrv-ltwa-en.ldf\n        ├── jabbrv.sty\n        ├── pnas-new.bst\n        ├── pnas-new.cls\n        ├── pnas.lua\n        ├── pnasresearcharticle.sty\n        ├── styles.css\n        └── _extension.yml\nPutting the files here only does so much. We have to let the template know that we have them. This all happens in _extensions/pnas/_extension.yml\nThere are a pair of lines in the file telling it what to use for PDFs.\n    pdf:\n      include-in-header: header.tex\nWe’ll add some new instructions for the format-resources listing out the new files that we have.\n    pdf:\n      include-in-header: header.tex\n      format-resources:\n        - pnas-new.cls\n        - pnas-new.bst\n        - pnasresearcharticle.sty\n        - jabbrv.sty\n        - jabbrv-ltwa-all.ldf\n        - jabbrv-ltwa-en.ldf\n\nEditing template.qmd (Part 1: The YAML)\nOnce we’ve got all the files in place, it’s time to make some edits to template.qmd so that we can render it and see how we’re doing.\nThe first big part is setting up the Quarto YAML. This is where all of the information that we will later use in the template will go.\nThe first few lines of the document are standard, but we’ll make two changes to the below:\ntitle: Pnas Template\nformat:\n  pnas-pdf:\n    keep-tex: true  \n  pnas-html: default\nFirst, I’ll give it a more fun title, here “Quarto Template for PNAS Submissions”. Second, I’m focused on making the pnas-pdf class, so I’ll remove the pnas-html: default line from line 6 (or so) of the YAML. For journals that need PDFs for submission, the extra type doesn’t seem worth the effort.\n\nAuthors and Affiliations YAML\nNot all templates use all possible information about authors. But, there is a pretty extensive set of possible pieces of information.\nWe need to include every relevant piece of information that the journal template will use. If we don’t include everything, there might be weird empty spaces. If we over include, that’s okay, but some things will get ignored.\nUnlike the rticles templates that RMarkdown used to use, Quarto has tried to build an exhaustive schema for people’s names and their affiliations. The schema are detailed extensively at https://quarto.org/docs/journals/authors.html.\nIf we peek at some recent PNAS articles, we can see what author and affiliation information are used.\nAuthors look something like\n\nAuthor One\\(^{a, c, 1}\\), Author Two\\(^{b, 1, 2}\\), and Author Three^\\(a\\)\n\nwhich is kind of a mess.\n1 indicates that the authors contributed equally. 2 indicates the corresponding author. a,b,c are all affiliations.\nAffiliations themselves are pretty standard here. They should get printed to look like\nUniversity, Department, City, State ZIP\nWith that much information, we can assemble the YAML for an author. To make sure we can see the difference in these, I’ll do set up the affiliations as:\n\na: Harvard University, Department of Government, Cambridge, MA 02138\nb: Yale University, Department of Political Science, New Haven, CT 06511\nc: Harvard University, Department of Statistics, Cambridge, MA 02138\n\nauthor:\n  - name: Author One\n    affiliations:\n      - name: Harvard University\n        id: a\n        department: Department of Government\n        city: Cambridge\n        state: MA\n        postal-code: 02138\n      - name: Harvard University\n        id: c\n        department: Department of Statistics\n        city: Cambridge\n        state: MA\n        postal-code: 02138\n    attributes:\n      equal-contributor: true\nNote here that Author One and Author Two contributed equally and Author Two is the corresponding author.\nWith that, we can build out the other authors.\n  - name: Author Two\n    affiliations:\n      - name: Yale University\n        id: b\n        department: Department of Political Science\n        city: New Haven\n        state: CT\n        postal-code: 06511\n    attributes:\n      equal-contributor: true\n      corresponding: true\n  - name: Author Three\n    affiliations:\n      - ref: a\nBuilding out Author Two’s affiliation is very similar, we just add a corresponding: true under attributes along with their information. For Author Three, we can do a cool thing and reference affiliation a, since it’s the same.\nNow at this point, I know I’ll also need the email for the corresponding author, so I’ll add an email: corresponding@email.com line to the Author Two chunk.\n\n\nCustom YAML Components\nThe rest of the YAML varies immensely by journal. To figure out what we need, we have to look back at the LaTeX template. We want to identify anything where we either need to tell the template about options to use or replace filler text. To that point, I’ve pulled out each thing that I see as something we need to be controllable in the YAML.\n\\templatetype{pnasresearcharticle} % Choose template\n% {pnasresearcharticle} = Template for a two-column research article\n% {pnasmathematics} %= Template for a one-column mathematics article\n% {pnasinvited} %= Template for a PNAS invited submission\nIt looks like we need a template_type argument to the YAML. Later, we can then list the options in the README.\n\npnasresearcharticle: Template for a two-column research article\npnasmathematics: Template for a one-column mathematics article\npnasinvited: Template for a PNAS invited submission\n\nWe should be aware though that these actually have different official templates. This gives an important choice:\n\ntry to support it\nlet it be set but make it clear that the template is optimized for pnasresearcharticle types\ndon’t make it settable and just always use pnasresearcharticle\n\nI tend to go for (2) since it might work really easily without changing anything. Aiming for (1) is commendable, but may open a new can of worms. Option (3) is really safe and might actually be the best option, but if it at least mostly works, then someone else trying to do the other type now has to start from scratch when they didn’t have to. Part of how I’ll approach this is to make it the default in LaTeX if nothing is specified.\n\\leadauthor{Lead author last name}\nThis is a pretty common request, as it’s used in the header of pages. To be consistent with other journal types, such as apsr, I’ll call this field runningauthor. It’s always better to be consistent, since it makes switching easier, but if I didn’t know about the other name, I probably would have called it lead_author.\n\\significancestatement{Authors must submit a 120-word maximum statement about the significance of their research paper written at a level understandable to an undergraduate educated scientist outside their field of speciality. The primary goal of the significance statement is to explain the relevance of the work in broad context to a broad readership. The significance statement appears in the paper itself and is required for all research papers.}\nFor this, we can just give a new argument called significance, since it is just a bunch of text.\n\\authorcontributions{Please provide details of author contributions here.}\nFor this, we can also make a new argument called author_contributions. It’s a bunch of text and readable is best.\n\\authordeclaration{Please declare any competing interests here.}\nI tend to call this conflict_of_interest, as I’ve seen it that way in the old rticles days.\n\\keywords{Keyword 1 $|$ Keyword 2 $|$ Keyword 3 $|$ ...}\nKeywords come up in many templates and are included by default in the automatically generated YAML as keywords.\n\\doi{\\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}\nThis one looked like something that we might want to change, but is probably best to leave for now. With how publications and submissions work for PNAS, you won’t know the doi until after you’re done using the template.\n\\firstpage{12}\n% Use \\firstpage to indicate which paragraph and line will start the second page and subsequent formatting. In this example, there are a total of 11 paragraphs on the first page, counting the first level heading as a paragraph. The value {12} represents the number of the paragraph starting the second page. If a paragraph runs over onto the second page, include a bracket with the paragraph line number starting the second page, followed by the paragraph number in curly brackets, e.g. \"\\firstpage[4]{11}\".\nNow this one presents an interesting question. We could include a first_page argument. Though, the formatting is kinda tricky. We would need to support both \\firstpage{12} and \\firstpage[4]{11}. It seems to me that this would be a good thing to leave out of the YAML but use within the template.qmd, so that it’s obvious it should be used but doesn’t require multi-option handling. That is my preference and I can’t say for sure if this is best practice.\n\\acknow{Please include your acknowledgments here, set in a single paragraph. Please do not include any acknowledgments in the Supporting Information, or anywhere else in the manuscript.}\nWe can give this a name like acknowledgements, again looking to other templates to see what they use as names, since this is a pretty common thing to have.\nOkay now that gives us pretty set of YAML option to add. They’ll look like:\nrunningauthor: \"One, Two, and Three\"\nsignificance: |\n  Authors must submit a 120-word maximum statement about the significance of their research paper written at a level understandable to an undergraduate educated scientist outside their field of speciality. The primary goal of the significance statement is to explain the relevance of the work in broad context to a broad readership. The significance statement appears in the paper itself and is required for all research papers.\nauthor_contributions: \"Please provide details of author contributions here.\"\nconflict_of_interest: \"Please declare any competing interests here.\"\nkeywords: [template, demo]\nacknowledgements: | \n  Please include your acknowledgments here, set in a single paragraph. Please do not include any acknowledgments in the Supporting Information, or anywhere else in the manuscript.\nAs you can see, where applicable, I’m using the instructions as the placeholder text where applicable. This has the benefit of keeping the end-user instructions near the end user without them having to deep dive into the original LaTeX template.\nThe bad news: that was the easy part. The good news: the next part walks you through the hard part.\n\n\n\n\n\n\nUse Git\n\n\n\nIf you aren’t already using Git at this point, now is a good time to run usethis::use_git(), initialize a repo, and commit what you’ve done. This is a good save point so that if you start playing with things for the next part, you can easily remove them if you break anything.\n\n\n\n\n\nUsing the YAML in LaTeX\nAdding the arguments to the YAML doesn’t automatically change anything about the output. It makes variables available to Pandoc’s processing as the name of the YAML set. So, for example, if our YAML looked like:\ntitle: Quarto Template for PNAS Submissions\nthen a variable called title would be available for us to use. We can extract its value with $title$.\nPossibly the most common is to check if a value is set and then use it if it is. For something like title, where we want to pass it to the LaTeX function \\title{}, we can do this with the following pattern.\n$if(title)$\n\\title{$title$}\n$endif$\nThe first and third lines here are how if statements are done. So, if the title is set then in your YAML, then the outputted .tex file will say:\n\\title{Quarto Template for PNAS Submissions}\nFor more details on how these types of variables work, take a look at Template syntax section of the Pandoc manual.\nMost of the templating from here is going to be finding the correct place to evaluate a variable. To find those places, we have to introduce the idea of partials, which are short files that get injected into the bigger template.\n\nLaTeX Partials\nPartials represent little snippets of LaTeX that get joined in a specific order. Quarto is built off of a series of default partials. If you’re rendering a regular document without any of this templating, it passing through those. The defaults are very good: they cover lots of cases and do them in a smart, efficient way. The ideal is that you only have to change a small number of partials. Journals have all sorts of format choices that you want to match, so we will have to replace some.\nAs assumed before, you know some LaTeX. A familiar, stylized LaTeX document then looks something like:\n% first line is the *doc-class*\n\\documentclass{article}\n\n% then we have the header\n% like where we load packages with \\usepackage{}\n\n% This next group of lines are the *title*\n\\title{Some Latex Document}\n\\author{Christopher Kenny}\n\\date{July 2023}\n\n\\begin{document}\n\n% then we have *before-body*\n\\maketitle\n\nThe body of the document\n\n% then we have *after-body*\n\n% then we have the *before-bib*\n\nThe references\n% then we have *biblio* which makes the bibliography\n\n\\end{document}\nEach of the things in *s is a partial and the general place where it goes. There are more partials, like toc, a table of contents partial. There’s also a partial for pandoc, for some things that Pandoc needs for rendering from LaTeX. Odds are good that you don’t have to touch the Pandoc one!\nA full description of Quarto partials is available in the Quarto documentation. This lists out all of the partials and a short description of what they do. The source files for the partials are on GitHub.\nAny partials that we need will live in a partials folder, below where the _extension.yml folder lives. Each partial will be a .tex file. So, they will all be something like: _extensions/pnas/partials/*.tex. Each file that we add to this folder has to be listed in _extension.yml, which I’ll show below.\nWith the general idea of partials down, we can go in order from the top down and see what we need.\n\ndoc-class.tex\nOkay, so first things first is the document class. In the template, it looks like this:\n\\documentclass[9pt,twocolumn,twoside]{pnas-new}\nArguably, we don’t need to change this one out. This whole partial will evaluate to one line. The default looks like this:\n\\documentclass[\n$if(fontsize)$\n  $fontsize$,\n$endif$\n$if(papersize)$\n  $papersize$paper,\n$endif$\n$if(beamer)$\n  ignorenonframetext,\n$if(handout)$\n  handout,\n$endif$\n$if(aspectratio)$\n  aspectratio=$aspectratio$,\n$endif$\n$endif$\n$for(classoption)$\n  $classoption$$sep$,\n$endfor$\n]{$documentclass$}\nI say that we don’t need to necessarily change this one because we could pass the following to the YAML.\nfontsize: \"9pt\"\nclassoption:\n - twocolumn\n - twoside\ndocumentclass: \"pnas-new\"\nThis would do the following:\n\n$if(fontsize)$ is true, so return the fontsize (9pt) and include the ,.\n$if(papersize)$ isn’t specified, so it’s false and nothing happens.\n$if(beamer)$ isn’t specified, so it’s false and nothing happens.\n$if(handout)$ isn’t specified, so it’s false and nothing happens.\n$if(aspectratio)$ isn’t specified, so it’s false and nothing happens.\n\nThen we get a for loop. This works like the if syntax from before. It takes each element of classoption (a length two vector with twocolumn and twoside), returns them followed by the seperator. The $sep$, $endfor$ syntax just says “hey this is the separator ,, a comma followed by a space.”\nFinally, document class would be pnas-new. So that completes it as\n\\documentclass[9pt,twocolumn,twoside]{pnas-new}\nThere are at least three advantages to implementing this simple partial as a custom partial. First, we can hard code the documentclass to be just the class we want to support. This avoids weird errors related to incorrect class specifications by downstream users. Second, if we decide that something is important enough to elevate to an additional YAML argument, we can then do so and add it with a little if syntax. For example, the APSR has a special nonblind argument that is really important for the submission. Knowing that, I could then make it a YAML option here. Third, we can remove some of the arguments that aren’t relevant to our template, like aspectratio.\nWith that in mind, I’ll first trim the options:\n\\documentclass[\n$if(fontsize)$\n  $fontsize$,\n$endif$\n$for(classoption)$\n  $classoption$$sep$,\n$endfor$\n]{$documentclass$}\nThen I’ll hard code the documentclass\n\\documentclass[\n$if(fontsize)$\n  $fontsize$,\n$endif$\n$for(classoption)$\n  $classoption$$sep$,\n$endfor$\n]{pnas-new}\nAt this point, I’ll reopen the template.qmd file and add to the YAML the relevant options for what we were just looking at.\nfontsize: \"9pt\"\nclassoption:\n - twocolumn\n - twoside\nThat’s it for that file. Now we just have to let the _extension.yml file know we have it, like we did for the class files above.\nLook for the lines we edited before and below it we’ll add a template-partials section to indicate that we have this file.\n    pdf:\n      include-in-header: header.tex\n      format-resources:\n        - pnas-new.cls\n        - pnas-new.bst\n        - pnasresearcharticle.sty\n        - jabbrv.sty\n        - jabbrv-ltwa-all.ldf\n        - jabbrv-ltwa-en.ldf\n      template-partials:\n        - \"partials/doc-class.tex\"\n\n\ntitle.tex\nNow, the title block definitely needs work for this example and probably for the majority of cases. The title partial will tell the document how to create the title, identify the authors, and identify anything else important that needs to be specified at the start of the document. For the PNAS template, we need to tell it how to:\n\nSpecify the \\templatetype{}\nCall \\title{}\nIdentify the authors (this one is the hardest!)\nCall \\leadauthor{}\nCall \\significancestatement{}\nCall \\authorcontributions{}\nCall \\authordeclaration{}\nIdentify equal authors\nIdentify corresponding authors\nSpecify the keywords\n\nFor the template type, we want to use an if-else statement. This lets us specify the default option for the case where no type was set. So, since there is an else, we can put it all directly inside the call to \\templatetype{}. This will look something like:\n\\templatetype{\n$if(template_type)$\n$template_type$\n$else$\npnasresearcharticle\n$endif$}\nwhere $else$ is how we specify the else part of the if-else.\nThe next part of this is going to be familiar. If there’s a title, we want to write the title.\n$if(title)$\n\\title{$title$}\n$endif$\nIf we hold onto the author pieces for a second, we can repeat that pattern for each of \\leadauthor{},\\significancestatement{},\\authorcontributions{}, and \\authordeclaration{}.\n$if(runningauthor)$\n\\leadauthor{$runningauthor$}\n$endif$\n\n$if(significance)$\n\\significancestatement{$significance$}\n$endif$\n\n$if(author_contributions)$\n\\authorcontributions{$author_contributions$}\n$endif$\n\n$if(conflict_of_interest)$\n\\authordeclaration{$conflict_of_interest$}\n$endif$\nNow back to the authors and affiliations. This is often the hardest part of making a Quarto template for journals. As a reminder, we want the output to look like:\n\\author[a,c,1]{Author One}\n\\author[b,1,2]{Author Two}\n\\author[a]{Author Three}\n\n\\affil[a]{Affiliation One}\n\\affil[b]{Affiliation Two}\n\\affil[c]{Affiliation Three}\nLet’s break apart what we need for Author One.\n\nA name\nAffiliation with a\n\nNeed to make the \\affil for a\n\nAffiliation with b\n\nNeed to make that \\affil for b too\n\na 1 to indicate joint first authorship\n\nWe’ll fill that part out in a minute.\n\n\nIf we look at the normalized schema for author first, we can figure out how to access it. The relevant part of the YAML looks like:\nauthor:\n  - id: string\n    number: number\n    name:\n      given: string\n      family: string\n      literal: string\nHow do we get the value? We can build it like so\n\nit’s part of the author: $author.\n\nit’s part of the name: name.\n\nwe want the full thing and that’s it literal$\n\n\n\nSo going down the little tree, we get that it is \\(author.name.literal\\). If we needed something like the first name, it would be \\(author.name.given\\).\nSo, we can throw that into:\n\\author[...]{$author.name.literal$}\nleaving the ... for this next step.\nWe need to extract the IDs for the affiliations for each person, so we’ll need to iterate. To iterate over the affiliations of each author, we can use the by-author iterator. This will let us loop over each author and grab the relevant values.\nIf we just needed the authors, then this would look like:\n$for(by-author)$\n\\author{$by-author.name.literal$}\n$endfor$\nBut, we need to fill in the [...] from above. To start, let’s do the affiliation ids. If they have affiliations, we want to loop over them.\n$for(by-author)$\n\\author[\n$if(by-author.affiliations)$\n$for(by-author.affiliations)$\n$it.id$\n$sep$,\n$endfor$\n$endif$\n]{$by-author.name.literal$}\n$endfor$\nAs you can see, I’ve introduced the special keyword it. It’s essentially a placeholder for the current iterator. Here, that’s each entry of by-author.affiliations and we are accessing the .id from them. We’re separating them with ,s so that they evaluate nicely.\nThe spacing that this outputs is a little funny, so we can collapse it to be\n$for(by-author)$\n\\author[$if(by-author.affiliations)$$for(by-author.affiliations)$$it.id$$sep$,$endfor$$endif$%\n]{$by-author.name.literal$}\n$endfor$\nWe can use the % just as in LaTeX for the normal end of line behavior.\nNow this all works beautifully for the affiliations, but we also need to think about the \\(1\\) and \\(2\\). \\(1\\) means equal first authorship if there are multiple first authors. \\(2\\) means corresponding author assuming that there are multiple first authors. So, \\(1\\) isn’t always there but \\(2\\) should be. Now, we can back out that if there are multiple first authors, then the author listed first will be a first author.\nThis is good news! This means we can check if there is an equal contribution note. We can do this with our good friend LaTeX, by defining a new command for this. If there’s an equal contributor, it’ll always be 1. We can make the corresponding variable vary based on the equal contributor variable.\n\\newcommand{\\equalcont}{1}\n\n$if(equal-contributor)$\n\\newcommand{\\correspond}{2}\n$else$\n\\newcommand{\\correspond}{1}\n$endif$\nWe can then use that in our templating. If an author is listed as an equal author, we want to add a ,\\equalcont after the affiliations. So, we can do just that by adding in another line.\n$for(by-author)$\n\\author[$if(by-author.affiliations)$$for(by-author.affiliations)$$it.id$$sep$,$endfor$$endif$%\n$if(by-author.attributes.equal-contributor)$,\\equalcont$endif$%\n]{$by-author.name.literal$}\n$endfor$\nIn the same way, we can add information about corresponding authors. If an author is listed as the corresponding author, we want to add a ,\\correspond after the affiliations.\n$for(by-author)$\n\\author[$if(by-author.affiliations)$$for(by-author.affiliations)$$it.id$$sep$,$endfor$$endif$%\n$if(by-author.attributes.equal-contributor)$,\\equalcont$endif$%\n$if(by-author.attributes.corresponding)$,\\correspond$endif$%\n]{$by-author.name.literal$}\n$endfor$\nNow, we can turn to setting up the affiliation lines, with \\affil. As with authors, there is a by-affiliation keyword that will let us iterate over the affiliations. For each affilition, we want something of the form:\n\\affil[a]{Affiliation One}\nAt the most basic level, if it was just a name (like “Harvard University”), we could use a loop and the it syntax again to make something like this:\n$for(by-affiliation)$\n\\affil[$it.id$]{$it.name$}\n$endfor$\nWe need it to do a teeny bit more though. We want something instead that uses all of the relevant pieces of the YAML to say:\nHarvard University, Department of Government, Cambridge, MA 02138\nThe components of this are then:\nname, department, city, state, postal-code\nFor each affiliation, we would want something like:\n$if(it.name)$$it.name$$endif$ \n$if(it.department)$, $it.department$$endif$ \n$if(it.city)$, $it.city$$endif$ \n$if(it.state)$, $it.state$$endif$ \n$if(it.postal-code)$, $it.postal-code$$endif$ \n$if(it.country)$, $it.country$$endif$\nNote that we’re separating them with , at the start of each additional argument. This avoids weird spacing in the output. And we should probably add in country at the end, for if not all authors work in the US.\n$if(it.country)$, $it.country$$endif$\nWe can put this all in its own file, called _affiliation.tex in the partials/ directory. We don’t have to, but it seems to be common practice in existing templates. Then, we can call this chunk using $_affiliation.tex()$. The benefit of doing that is it keeps the template cleaner and easier to debug later.\n$for(by-affiliation)$\n\\affil[$it.id$]{$_affiliation.tex()$}\n$endfor$\nDon’t forget to add the partials/_affiliation.tex line to the _extension file. That will now have a template-partials chunk like so:\n      template-partials:\n        - \"partials/doc-class.tex\"\n        - \"partials/_affiliation.tex\"\n        - \"partials/title.tex\"\nNow, a minute ago, we did the hard work for the equal contributors, so we can add in some code to indicate the equal authors too in the template:\n$if(equal-contributor)$\n\\equalauthors{\\textsuperscript{\\equalcont} $equal-contributor$}\n$endif$\nWe add the superscript because we had just a raw number above.\nNow, we can do something similar for the corresponding author. This time we’ll iterate over the authors, again with by-author to find the corresponding one. If they’re the corresponding author, we’ll fill in the ... below:\n$for(by-author)$\n$if(by-author.attributes.corresponding)$\n\\correspondingauthor{...}\n$endif$\n$endfor$\nJust as with the equal contributors, we want to start it with the identifier, \\textsuperscript{\\correspond}. Then we need the PNAS template text: “To whom correspondence should be addressed. E-mail:”. And last, we can get the email from the author with $by-author.email$.\nPut together, that looks like:\n$for(by-author)$\n$if(by-author.attributes.corresponding)$\n\\correspondingauthor{\\textsuperscript{\\correspond}To whom correspondence should be addressed. E-mail: $by-author.email$}\n$endif$\n$endfor$\nFinally, we can build out the keywords. Since there can be multiple, we’ll use a for loop with a separator, like in Section 3.2.1.1. We want to iterate over each one and print them separated by a pipe, |.\n$if(keywords)$\n\\keywords{$for(keywords)$$keywords$$sep$ | $endfor$}\n$endif$\nAll together, the file looks something like:\n\\templatetype{$if(template_type)$$template_type$$else$pnasresearcharticle$endif$}\n\n$if(title)$\n\\title{$title$}\n$endif$\n\n\\newcommand{\\equalcont}{1}\n\n$if(equal-contributor)$\n\\newcommand{\\correspond}{2}\n$else$\n\\newcommand{\\correspond}{1}\n$endif$\n\n$for(by-author)$\n\\author[$if(by-author.affiliations)$$for(by-author.affiliations)$$it.id$$sep$,$endfor$$endif$%\n$if(by-author.attributes.equal-contributor)$,\\equalcont$endif$%\n$if(by-author.attributes.corresponding)$,\\correspond$endif$%\n]{$by-author.name.literal$}\n$endfor$\n\n$for(by-affiliation)$\n\\affil[$it.id$]{$_affiliation.tex()$}\n$endfor$\n\n$if(runningauthor)$\n\\leadauthor{$runningauthor$}\n$endif$\n\n$if(significance)$\n\\significancestatement{$significance$}\n$endif$\n\n$if(author_contributions)$\n\\authorcontributions{$author_contributions$}\n$endif$\n\n$if(conflict_of_interest)$\n\\authordeclaration{$conflict_of_interest$}\n$endif$\n\n$if(equal-contributor)$\n\\equalauthors{\\textsuperscript{\\equalcont} $equal-contributor$}\n$endif$\n\n$for(by-author)$\n$if(by-author.attributes.corresponding)$\n\\correspondingauthor{\\textsuperscript{\\correspond}To whom correspondence should be addressed. E-mail: $by-author.email$}\n$endif$\n$endfor$\n\n$if(keywords)$\n\\keywords{$for(keywords)$$keywords$$sep$ | $endfor$}\n$endif$\nNote that it will eventually be processeed from top to bottom, so we have to define the LaTeX commands before we use them.\n\n\nbefore-body.tex\nOkay, with the hardest part done, we can do this one without too much crazy stuff. This partial is all the stuff that comes after \\begin{document} but before the writing. So, that normally includes making the title and the abstract. As you’ll see in a minute, there’s also some generic LaTeX from the template that we want here.\nFirst, if there’s a title we have to tell it to make it. This looks like much of how we built out the title section.\n$if(title)$\n\\maketitle\n$endif$\nNext, if there’s an abstract, we need to make that. We also need it to be in an abstract environment, so we can toss the whole thing into one if statement. For what it’s worth, this chunk goes in pretty much every template. I just copy the same chunk from old Quarto template to new Quarto template whenever I need it.\n$if(abstract)$\n\\begin{abstract}\n$abstract$\n\\end{abstract}\n$endif$\nAnd really both of these two things are handled by the default partial. I would have preferred to not change it, but we need some defaults to be set here that we see in the template itself. We can just straight up copy those into it. No edits necessary. If you go the route to allow setting the doi, perhaps with a YAML doi:, this is where you would insert that.\n\\dates{This manuscript was compiled on \\today}\n\\doi{\\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}\n\n\\thispagestyle{firststyle}\n\\ifthenelse{\\boolean{shortarticle}}{\\ifthenelse{\\boolean{singlecolumn}}{\\abscontentformatted}{\\abscontent}}{}\nAll together, the before-body partial looks something like:\n$if(title)$\n\\maketitle\n$endif$\n\n$if(abstract)$\n\\begin{abstract}\n$abstract$\n\\end{abstract}\n$endif$\n\n\\dates{This manuscript was compiled on \\today}\n\\doi{\\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}\n\n\\thispagestyle{firststyle}\n\\ifthenelse{\\boolean{shortarticle}}{\\ifthenelse{\\boolean{singlecolumn}}{\\abscontentformatted}{\\abscontent}}{}\nThis is about the “simplest” partial to make, once you’re comfortable with partials, since it is so close to the default. If it weren’t for setting the first page style, we could have omitted it. Don’t forget to add it to _extension.yml.\n\n\nbefore-bib.tex\nIn the PNAS template, just before the bibliography, we need to include the acknowledgements. We also need to make a call to a function, \\showacknow{} to make them appear.\nThe first part is the same as our usual pattern. Check if there are acknowledgements, and if there are, set them.\n$if(acknowledgements)$\n\\acknow{$acknowledgements$}\n\n\\showacknow{} % Display the acknowledgments section\n$endif$\nWithin the check, we’ll add the quick call to make sure they appear.\nAs before, we to add it to _extension.yml. But now we’re done with most of the work!\n\n\n\n\nEditing template.qmd (Part 2: The Body)\nOkay, now we can test and run with the file. To do so, we want to add some text back from the other template that instructs the user how to fill out the template. I’ll start by copying in big chunks from the template and then converting things from LaTeX to the friendlier Quarto (or sometimes R) syntax.\nFor example, if the LaTeX template has:\n\\subsection*{Author Affiliations}\nI’ll make that into:\n## Author Affiliations {.unnumbered}\nWe can replace things like sections or links with some find+replace regex in RStudio:\n\nReplace (\\\\subsection\\*{)(.+?)(}) with ## \\2 {.unnumbered} to fix subsection titles.\nReplace (\\\\subsubsection\\*{)(.+?)(}) with ### \\2 {.unnumbered} to fix subsubsection titles.\nReplace (\\\\href{)(.+?)(})({)(.+?)(}) with [\\5](\\2) to move links from LaTeX to Quarto.\nReplace (\\\\ref{fig:)(.+?)(}) with @fig-\\2 to move cross references to Quarto syntax.\nReplace (\\\\verb\\|)(.+?)(\\|) with \\\\2`` to move verbatim environments to Quarto ones.\n\nAs I was doing this, I got a really weird error.\ncompilation failed- error\nLaTeX Error: Not in outer par mode.\n\nSee the LaTeX manual or LaTeX Companion for explanation.\nType  H &lt;return&gt;  for immediate help.\n ...                                              \n                                                  \nl.206 \\end{document}\n                     \n\nsee template.log for more information.\nThe obvious thing was that I probably had a mismatch in } or { somewhere. After a few minutes, that clearly wasn’t the case. They all seemed to match and match correctly.\nAt this point, I looked at the template again and thought about the first page special formatting. The first page has a different column width, so it’s a little special. I added some more paragraphs from the LaTeX template and it rendered fine. Playing around and if there was anything short of a page, it would fail. Otherwise, it worked beautifully. As such, I added a warning to the template in the text, so any overzealous compilers would have a clue what broke.\nA word of warning: This template will fail with a \"Not in outer par mode\" warning if you try to compile it with less than one page of text.\nThe template relies on having a full first page which is styled separately.\nIf you see a warning to the tune of \"LaTeX Error: Not in outer par mode.\" or referencing `\\end{document}`, try writing more or adding filler text and recompiling.\nNext, I’ll replace figures. For the most basic figures, I’ll just use raw Quarto figure syntax.\nFor example:\n\\begin{figure}%[tbhp]\n\\centering\n\\includegraphics[width=.8\\linewidth]{figs/frog.pdf}\n\\caption{Placeholder image of a frog with a long example legend to show justification setting.}\n\\label{fig:frog}\n\\end{figure}\ncan be pretty well replicated with the shorter:\n![Placeholder image of a frog with a long example legend to show justification setting.](figs/frog.pdf){#fig-frog}\nAdditional settings can be set within an R chunk (or other code chunk). I find it very helpful to mention the fig-env argument, such as below:\n\nIn Quarto, we can do these by setting the fig-env command to figure* or SCfigure*\n\n#| label: fig-side\n#| fig-cap: \"This legend would be placed at the side of the figure, rather than below it.\"\n#| fig-env: \"SCfigure*\"\n#| echo: false\n# tell it the options as comments with a | and a space, as above.\n# set echo: false to avoid printing this text\nknitr::include_graphics('figs/frog.pdf')\nTwo column journals need the figure* (often called a “star figure”, “figure star”, or “star” environment) for page wide columns. PNAS also has a side-caption version, which is included in the template as the above example. This gives a clear demo of a simpler approach to including figures, at least than with LaTeX.\nAs for tables, I tend to leave them as-is in LaTeX. Many table environments need a little something else that doesn’t seem to translate super well into Markdown syntax. Of course, you could translate them into Markdown, especially if the LaTeX doesn’t need special options.\nFinally, I like to include anything about the bibliography in a References section, like below. I include the special \\bibsplit command that needs to be set manually at the end with a comment explaining how to use it.\n# References\n\\bibsplit[2]\n&lt;!-- Use \\bibsplit to split the references from the body of the text. Value \"[2]\" represents the number of reference in the left column (Note: Please avoid single column figures & tables on this page.) --&gt;\n\n:::{#refs}\n:::\nThis includes the special reference div (the ::: things) to help make it easier to understand where those will be printed. Details on that div are available in the Quarto documentation.\n\n\n\n\n\n\nNote\n\n\n\nDon’t forget to update the bibliography.bib default file to include any example references you use!"
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#cleaning-up",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#cleaning-up",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Cleaning up",
    "text": "Cleaning up\n\nRevising the README\nWith the Quarto and LaTeX stuff pretty much done, we want to include information for future users.\nFirst, we’ll fill out the Options section. In general, this should describe anything that isn’t default in the YAML or that should generally be set.\nFor example, I want to explain a few things:\n\nclassoption defaults\nsetting a corresponding author\naffiliation IDs should be letters\n\nWords are useful here to refer\nFor the PNAS template, I included the following:\n\nThe default setting for class option generates a two column layout with:\n\nclassoption:\n - twocolumn\n - twoside\nTo set a corresponding author, ensure that the attribute “corresponding” is true and that they have an email listed. For proper formatting, each affiliation should be given a letter id (like a, b, …, z). This template is designed for template_type: pnasresearcharticle (the default). It can also take options pnasmathematics or pnasinvited, but these are not formally supported, as they have official alternative formats available on Overleaf.\nNow, we can run the png code from before.\npdftools::pdf_convert('template.pdf', pages = 1)\nwill generate a file template_1.png.\nTo include that in the readme, we can use Markdown, like:\n![[template.qmd](template.qmd)](template_1.png)\nThat gives people enough information to get started with your template.\n\n\nAdding a .quartoignore\nNow, one last thing we want to do is make sure that people using the template won’t install extra files. Things like template_1.png from the last section are useful in the repo, but not to the end user. We can add a file .quartoignore in the root directory that functions like a .gitignore file.\nMine looks like:\n*.pdf\n*.png\n*.rproj\n*.Rproj\n!figs/*.png\nThis ignores all pdfs, pngs, and R project files. It then un-ignores the figs/ folder where useful example files for the template live. It’s important to include those in the template because you want the file that gets downloaded to run when someone runs\nquarto use template christopherkenny/pnas\nBut, they can be deleted once people have made sure the template works. As such, they don’t need to be included in the _extension.yml file, but shouldn’t be .quartoignored.\n\n\nDeleting unused files\nIt’s probably over. There may be other changes, but it’s probably over. But it’s not an official template, so no one will contact you when they update it. But, one day, you might notice that it is different or someone will comment on GitHub. But for now, you’re free! Time to make another template for that other journal you were thinking about submitting to.\nMore importantly, at this point, you should delete any files or code that you didn’t use. For this template, that means removing styles.css and pnas.lua. Also, remove them from the _extension.yml file. That is, delete:\n      filters:\n        - pnas.lua\nand\n    html:\n      css: styles.css"
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#finishing-up",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#finishing-up",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Finishing up",
    "text": "Finishing up\nLast things last.\nIf your template now works, then it’s time to make it public. A seemingly ridiculous portion of research time is spent recreating resources that surely someone else has somewhere. A semi-functional template is better than starting from scratch.\nAdd a topic to the GitHub repo for quarto-template and anything else relevant! Congrats, you have a template. Share it on Twitter or whatever is still useful. If it doesn’t work for someone, they’ll email you or open an issue. As a wise person once said &gt; all (feedback) is good (feedback)\nEven if it didn’t work for someone, a little information about why it didn’t work can help you a ton for when you have the same issue some day."
  },
  {
    "objectID": "posts/2023-07-01-creating-quarto-journal-articles/index.html#resources",
    "href": "posts/2023-07-01-creating-quarto-journal-articles/index.html#resources",
    "title": "Creating Quarto Journal Article Templates",
    "section": "Resources",
    "text": "Resources\nThe template created within this post is available at christopherkenny/pnas."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christopher T. Kenny",
    "section": "",
    "text": "Christopher T. Kenny\nPh.D. Candidate, Department of Government, Harvard University\n    \n\n\n\n\nAbout Me\nI am a PhD candidate in the Department of Government, studying American Politics and Political Methodology. My substantive focus is on redistricting and gerrymandering. I work on open source R tools for analyzing redistricting and voting rights in context. I am affiliated with the Center for American Political Studies at Harvard University, the Institute for Quantitative Social Science, and the Algorithm-Assisted Redistricting Methodology (ALARM) Project. In 2022, I was a fellow at the Election Law Clinic at Harvard Law School.\n\n\nCurrent Affiliations\nALARM Project\nInstitute for Quantitative Social Science\nCenter for American Political Studies\n\n\nEducation\nHarvard University | Cambridge, MA\nPh.D. in Government | Expected May 2025\nM.A. in Government | August 2019 - May 2021\nCornell University | Ithaca, NY\nB.A. in Mathematics and Government | August 2015 - May 2019"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Making a Scheduled Bot for Bluesky Social in R\n\n\n\n\n\n\nr-pkg\n\n\nbskyr\n\n\n\nExplaining the mechanics behind my CRAN Updates bot for Bluesky Social. \n\n\n\n\n\nJan 3, 2024\n\n\nChristopher T. Kenny\n\n\n\n\n\n\n\n\n\n\n\n\nPackages 2023 Wrapped\n\n\n\n\n\n\nr-pkg\n\n\n\nA quick look at my R package updates this year. \n\n\n\n\n\nDec 21, 2023\n\n\nChristopher T. Kenny\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Maps with feltr\n\n\n\n\n\n\nmaps\n\n\nr-pkg\n\n\n\nA brief introduction to the feltr package. \n\n\n\n\n\nJul 7, 2023\n\n\nChristopher T. Kenny\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Quarto Journal Article Templates\n\n\n\n\n\n\nquarto\n\n\n\nA long walk through creating Quarto templates for journal articles. \n\n\n\n\n\nJul 1, 2023\n\n\nChristopher T. Kenny\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "package-relationships.html",
    "href": "package-relationships.html",
    "title": "Package relationships",
    "section": "",
    "text": "flowchart LR\n  redist{{redist}}\n  redistmetrics{{redistmetrics}}\n  geomander{{geomander}}\n  PL94171{{PL94171}}\n  censable{{censable}}\n  tinytiger{{tinytiger}}\n  alarmdata{{alarmdata}}\n  ggredist{{ggredist}}\n  redistverse{{redistverse}}\n  dots{{dots}}\n  cvap{{cvap}}\n  ppmf{{ppmf}}\n  divseg{{divseg}}\n  name{{name}}\n  jot{{jot}}\n  ei{{ei}}\n  congress{{congress}}\n  feltr{{feltr}}\n  planscorer{{planscorer}}\n  crayons{{crayons}}\n  apportion{{apportion}}\n  gptzeror{{gptzeror}}\n  bskyr{{bskyr}}\n  redistio((redistio))\n  palette{{palette}}\n  ThemePark((ThemePark))\n  baf{{baf}}\n  redistmetrics --&gt; redist\n  censable --&gt; geomander\n  tinytiger --&gt; geomander\n  tinytiger --&gt; PL94171\n  tinytiger --&gt; censable\n  censable --&gt; cvap\n  censable --&gt; ppmf\n  censable --&gt; alarmdata\n  geomander --&gt; alarmdata\n  redist --&gt; alarmdata\n  redistmetrics --&gt; alarmdata\n  tinytiger --&gt; alarmdata\n  palette --&gt; ggredist\n  redist --&gt; redistverse\n  redistmetrics --&gt; redistverse\n  geomander --&gt; redistverse\n  ggredist --&gt; redistverse\n  censable --&gt; redistverse\n  tinytiger --&gt; redistverse\n  PL94171 --&gt; redistverse\n  alarmdata --&gt; redistverse\n  palette --&gt; crayons\n  geomander --&gt; redistio\n  ggredist --&gt; redistio\n  redistmetrics --&gt; redistio"
  },
  {
    "objectID": "posts/2023-07-07-making-maps-with-feltr/index.html",
    "href": "posts/2023-07-07-making-maps-with-feltr/index.html",
    "title": "Making Maps with feltr",
    "section": "",
    "text": "I make a lot of maps, almost always in R. Recently, I was introduced to felt.com. It’s a clean interface for web maps, including some great features, like drawing directly on a map or adding text annotations.\nThe new feltr package offers an interface to the Felt API, so you can upload data to Felt directly from R. It also includes tools for reading data from Felt into R as sf objects.\nYou can install feltr with:\nBelow, I’ll demo making a map with point locations of Dunkins in Cambridge, MA, from a csv file of Dunkin addresses."
  },
  {
    "objectID": "posts/2023-07-07-making-maps-with-feltr/index.html#dunkins-in-cambridge-ma",
    "href": "posts/2023-07-07-making-maps-with-feltr/index.html#dunkins-in-cambridge-ma",
    "title": "Making Maps with feltr",
    "section": "Dunkins in Cambridge, MA",
    "text": "Dunkins in Cambridge, MA\nFirst, we’ll load a few packages.\n\nlibrary(feltr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at /Users/chris/Documents/GitHub/christopherkenny.github.io\n\n\nOne of the cool things with Felt is its “Upload Anything” feature, where we can upload anything. Here, we have a csv file of addresses for every Dunkin in Cambridge. It is simple, just text addresses separated into appropriate fields.\n\npath_dunkin_ma &lt;- here('posts/2023-07-07-making-maps-with-feltr/dunkin_ma.csv')\nread_csv(path_dunkin_ma, show_col_types = FALSE)\n\n# A tibble: 1,062 × 5\n   address           city     state zipcode    id\n   &lt;chr&gt;             &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 147 N Quincy St   Abington MA    02351       1\n 2 259 Brockton Ave  Abington MA    02351       2\n 3 323 Centre Ave    Abington MA    02351       3\n 4 937 Bedford St    Abington MA    02351       4\n 5 100 Powdermill Rd Acton    MA    01720       5\n 6 182 Great Rd      Acton    MA    01720       6\n 7 212 Main St       Acton    MA    01720       7\n 8 315 Main St       Acton    MA    01720       8\n 9 44 Great Rd       Acton    MA    01720       9\n10 150 S Main St     Acushnet MA    02743      10\n# ℹ 1,052 more rows\n\n\nTo share this data with Felt, we first have to make a new map. We don’t have to give it any information, it’ll just make an empty map. We can pass it a title and some starting information, like where to center the map and how far to zoom.\n\ndunk &lt;- felt_create_map(\n  title = 'Cambridge Dunkin Desert', \n  zoom = 14.5, lat = 42.3799, lon = -71.10668\n)\n\nThen once we have the map, we can upload the csv file directly to Felt. No local geocoding necessary, it’ll handle that. We can label the layer with name or supply colors, like fill_color and stroke_color.\n\nlayer_id &lt;- felt_add_map_layers(\n  map_id = dunk$id, name = 'Dunkin', file_names = path_dunkin_ma, \n  fill_color = '#FF671F', stroke_color = '#DA1884'\n)\n\nOnce we do that, after a couple of minutes, we have a map. Normally it’s a few seconds if we uploaded a geojson or shp file, but geocoding takes a small bit of time.\n\n\n\nDefault Felt Layout\n\n\nWhat I find great about this is that I can handle all of the data work in R and then adjust the map as needed after. For example, I can annotate where the Department of Government buildings are with a green star or highlight where Darwin’s was (until recently) with a blue x.\n\n\n\nAnnotated Map\n\n\nClearly, Darwin’s old location would be a great place for a new Dunkin, near the middle of an existing Dunkin desert.\nfeltr has additional features, including:\n\ndeleting maps with felt_delete_map()\nlisting details of existing maps with felt_get_map() and felt_get_map_layers()\ndownloading shapes with felt_get_map_sf(), felt_get_map_geojson(), and felt_get_map_elements()\nretrieving user details with felt_get_user().\n\nAll current features of the Felt API are supported in the CRAN version of feltr, as of July 2023. To offer feedback on feltr or ask questions, open an issue on GitHub."
  },
  {
    "objectID": "posts/2024-01-03-bskyr-bot/index.html",
    "href": "posts/2024-01-03-bskyr-bot/index.html",
    "title": "Making a Scheduled Bot for Bluesky Social in R",
    "section": "",
    "text": "This post walks through how I set up a simple bot in Bluesky Social. The bot, @cranupdates.bsky.social, posts every 2 hours with details about packages that have been updated, added, or removed from CRAN. Everything is run in R, primarily using the bskyr package. It’s run for free on GitHub Actions and data is stored between runs using Google Sheets.\nThe basic mechanics of the bot are:\nThis bot is entirely schedule based, so it doesn’t need to interact with other Bluesky users. Below, I detail how I set up the bot, including how to authenticate with Google Sheets (using googlesheets4) and GitHub Actions."
  },
  {
    "objectID": "posts/2024-01-03-bskyr-bot/index.html#sec-schedule",
    "href": "posts/2024-01-03-bskyr-bot/index.html#sec-schedule",
    "title": "Making a Scheduled Bot for Bluesky Social in R",
    "section": "Scheduling the run",
    "text": "Scheduling the run\nTo schedule the run, we need to tell it a few things:\n\nwhen to run it\nwhat to run\n\nwhat environment variables it needs\nwhat R version to use\nwhat R packages to install\nwhat script to run\n\n\nBelow, I explain these steps. But first, the completed workflow looks like:\non:\n  push:\n    branches: main\n  schedule:\n    - cron: '0 1,5,9,13,17,21 * * *'\n\nname: Post\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      BLUESKY_APP_USER: ${{ secrets.BLUESKY_APP_USER }}\n      BLUESKY_APP_PASS: ${{ secrets.BLUESKY_APP_PASS }}\n      GARGLE_KEY: ${{ secrets.GARGLE_KEY }}\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\n\n      - uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: 'release'\n\n      - uses: r-lib/actions/setup-r-dependencies@v2\n        with:\n          packages:\n            any::here\n            any::dplyr\n            any::stringr\n            any::googlesheets4\n            any::bskyr\n\n      - run: Rscript 'post.R'\nThis whole file lives in .github/workflows/post.yml in the repo.\nFirst, let’s break down the when part:\non:\n  push:\n    branches: main\n  schedule:\n    - cron: '0 1,5,9,13,17,21 * * *'\nThis tells GitHub Actions to run the workflow when there’s a push to the main branch. It also says to schedule the workflow to run every 4 hours. CRON entries are minute hour day month weekday, so 0 1,5,9,13,17,21 * * * means to run at 1am, 5am, 9am, 1pm, 5pm, and 9pm every day. As implied, setting the star says to run it every day, every month, and every weekday. More documentation for the schedule part can be found here.\nThen we give the job a name, “Post”, with name: Post.\nThe next section simply indicates we want the job to run on the latest version of Ubuntu:\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\nNext, we provide environment variables, that we set in the repo settings. See {#sec-auth} for an explanation of the env GARGLE_KEY variable. This step is like setting an .Rprofile file locally, but for GitHub Actions.\n    env:\n      BLUESKY_APP_USER: ${{ secrets.BLUESKY_APP_USER }}\n      BLUESKY_APP_PASS: ${{ secrets.BLUESKY_APP_PASS }}\n      GARGLE_KEY: ${{ secrets.GARGLE_KEY }}\nNow, we can give it the steps to use.\nFirst, it needs to download the contents of the repo, so that it can access our post.R script.\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v3\nThen, we have to let it know to install R. We can use one of the r-lib/actions actions, setup-r@v2. I’m using the released R version (4.3.2) at the time of writing this. You could pin a specific version, but for bots, I plan to do the minor maintenance necessary as R versions increment.\n      - uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: 'release'\nAnd then we list the packages we need. This uses the setup-r-dependencies@v2 action, again from r-lib/actions. Below, I make use of the prefixes: any:: and github::. any:: is used for packages on CRAN, generally, but will also run if you’ve installed it elsewhere through a prior dependency.\n\n\n\n\n\n\nUsing GitHub Packages\n\n\n\n\n\nYou can use github:: in place of any:: for packages on GitHub. You just need to specify the user and repo. So, if you want to use the dev version of a package, like bskyr, you could use the syntax: github::christopherkenny/bskyr.\n\n\n\n      - uses: r-lib/actions/setup-r-dependencies@v2\n        with:\n          packages:\n            any::here\n            any::dplyr\n            any::stringr\n            any::googlesheets4\n            any::bskyr\nIf you already use renv, you could instead use that with the setup-renv action.\nFinally, we tell it to run post.R with:\n      - run: Rscript 'post.R'\nThat’s sufficient to tell GitHub Actions everything it needs to know to run the bot. Below, I describe how to set up the authentication with Google Sheets. This is only necessary if you’re using it, like I am, to store data about the prior bot run."
  },
  {
    "objectID": "posts/2024-01-03-bskyr-bot/index.html#footnotes",
    "href": "posts/2024-01-03-bskyr-bot/index.html#footnotes",
    "title": "Making a Scheduled Bot for Bluesky Social in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou could use something more private, but for a generic bot, public means GitHub Actions is free and can get the job done.↩︎\nThis created a small issue in the bot: Several of the first set of posts were about new packages that weren’t actually new. This was because the first run of the bot was on a Windows laptop, while the second was on a Linux server. By default, available.packages() returns packages available for the current OS, hence the setting of filters = c('CRAN', 'duplicates'), which overwrites the default that filters to OS-available too.↩︎"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Packages on CRAN:\n\n\n\n\n\n\n\n\n\nredist: Simulation Methods for Legislative Redistricting (with Cory McCartan, Ben Fifield, and Kosuke Imai)\n\n\n\nredistmetrics: Redistricting metrics (with Cory McCartan, Ben Fifield, and Kosuke Imai)\n\n\n\ngeomander: Geographic Tools for Studying Gerrymandering\n\n\n\nPL94171: Tabulate P.L. 94-171 Redistricting Data Summary Files (with Cory McCartan)\n\n\n\ncensable: Making Census Data More Usable\n\n\n\ntinytiger: Lightweight Interface to TIGER/Line Shapefiles (with Cory McCartan)\n\n\n\ndots: Dot Density Maps\n\n\n\ncvap: Citizen Voting Age Population\n\n\n\nppmf: Read Census Privacy Protected Microdata Files\n\n\n\ndivseg: Compute Diversity and Segregation Indices\n\n\n\nname: Tools for Working with Names\n\n\n\njot: Jot Down Notes for Later\n\n\n\nalarmdata: Download, Merge, and Process Redistricting Data (with Cory McCartan, Tyler Simko, Michael Zhao, and Kosuke Imai)\n\n\n\nggredist: Scales, Palettes, and Extensions of ggplot2 for Redistricting (with Cory McCartan)\n\n\n\ncongress: Access the Congress.gov API\n\n\n\nfeltr: Access the Felt API\n\n\n\nplanscorer: Score Redistricting Plans with PlanScore\n\n\n\nredistverse: Easily Install and Load Redistricting Software (with Cory McCartan)\n\n\n\ncrayons: Color Palettes from Crayon Boxes\n\n\n\napportion: Apportion Seats\n\n\n\ngptzeror: Identify Text Written by Large Language Models using GPTZero\n\n\n\nbskyr: Interact with Bluesky Social\n\n\n\npalette: Color Scheme Helpers\n\n\n\nbaf: Block Assignment Files\n\n\n\n\n\n\n\n\n\nPackages on GitHub:\n\n\n\n\n\n\n\n\n\nei: Ecological Inference (with Shusei Eshima, Gary King, and Molly Roberts)\n\n\n\nredistio: Interactive Redistricting\n\n\n\nThemePark: Themes for 'ggplot2' from Popular Culture (with Matthew B. Jané and Luke C. Pilling)"
  }
]