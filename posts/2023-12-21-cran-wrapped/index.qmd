---
title: "Packages 2023 Wrapped"
date: "2023-12-21"
description: |
  A quick look at my R package updates this year.
categories: [r-pkg]
image: "hexes_2023-12-20.png"
image-alt: "An array of hexagonal logos for discussed R packages."
editor: 
  markdown: 
    wrap: sentence
knitr:
  opts_chunk: 
    echo: false
---

![](hexes_2023-12-20.png)

With CRAN closing its submission queue for the holiday break tomorrow, it seems the right time to look over the past year.
I maintain and contribute to quite a few packages on CRAN, primarily focused on social science data and methods.
This year included adding 5 new packages to CRAN, with a minor shift towards tidy interfaces for web-based APIs.
Below, I talk about some of the new packages, updates to existing packages, and then look at the downloads for all of my packages.


```{r}
#| label: libs
suppressPackageStartupMessages(library(tidyverse))
library(gt)
library(geomtextpath)
```


```{r}
#| label: data
pkgs <- read_csv('pkgs.csv', show_col_types = FALSE) |> 
  mutate(logo = paste0('../../', logo)) # set to higher level directory

counts_daily <- pkgs %>%
  filter(status == 'CRAN') %>%
  pull(name) %>%
  cranlogs::cran_downloads(from = '2023-01-01') |> 
  rename(name = package)
counts_lst_yr <- counts_daily %>%
  group_by(name) %>%
  summarize(count = sum(count)) %>%
  arrange(desc(count)) |> 
  left_join(pkgs, by = 'name')

hists <- pkgs |> 
  filter(status == 'CRAN') |> 
  pull(name) |> 
  map(pkgsearch::cran_package_history) |> 
  list_rbind() |> 
  mutate(
    year = year(date),
    date2 = date(date)
  ) |> 
  group_by(Package) |> 
  mutate(first_year = min(year)) |> 
  ungroup()

first_release <- hists |> 
  group_by(name = Package) |> 
  summarize(first_release = min(date2))
```

# Updates on CRAN


## New packages

I've added five new packages to CRAN this year. 

### `apportion`

::: {.column-margin}
![](../../hexes/hex_apportion.png)
:::

[`apportion`](https://christophertkenny.com/apportion/) is a relatively simple package.
It calculates apportionments, the allocation of seats to states based on population.
It includes functions for the most common apportionment methods:

- the Adams Method (`app_adams()`)
- the Balinski Young Method (`app_balinski_young()`)
- the Dean Method (`app_dean()`)
- the Dâ€™Hondt Method (`app_dhondt()`)
- the Hamilton-Vinton Method (`app_hamilton_vinton()`)
- the Huntington-Hill Method (`app_huntington_hill()`)
- the Jefferson Method (`app_jefferson()`)
- the Webster Method (`app_webster()`)

### `crayons`

::: {.column-margin}
![](../../hexes/hex_crayons.png)
:::

[`crayons`](https://christophertkenny.com/crayons/) takes a few dozen packs of crayons and turns them into color palettes.
The package itself is pretty thin, relying on `scale_color_crayons()` and `scale_fill_crayons()` to create the palettes. 

```{r, echo = TRUE}
library(ggplot2)
library(crayons)

mpg |>
  ggplot() + 
  geom_point(aes(displ, hwy, colour = class)) + 
  scale_color_crayons(palette = 'original') + 
  theme_bw()
```

### `gptzeror`
::: {.column-margin}
![](../../hexes/hex_gptzeror.png)
:::

In the peak of worries about students using ChatGPT, [GPTZero](https://gptzero.me/) offered an API for estimating if text was human or AI generated.
I wrapped an R interface to this API in [`gptzeror`](https://christophertkenny.com/gptzeror/).
It seems to work somewhat well, but is far from perfect.
The risk of false positives is really high, so I'm not using this without other substantial evidence.
<br>
<br>
<br>
<br>
<br>

### `feltr`

::: {.column-margin}
![](../../hexes/hex_feltr.png)
:::

[`feltr`](https://christophertkenny.com/feltr/) is a package for working with the [Felt](https://felt.com/).
It covers all of the API endpoints, which lets you upload shapes from R directly to Felt.
You can delete them, update them, extract comments, and more.
I covered an application of this package in a prior blog post on loosely focused on [Dunkin Donuts in Cambridge](https://christophertkenny.com/posts/2023-07-07-making-maps-with-feltr/).
<br>
<br>
<br>
<br>
<br>
<br>

### `bskyr`

::: {.column-margin}
![](../../hexes/hex_bskyr.png)
:::

[`bskyr`](https://christophertkenny.com/bskyr/) is a package for working with the [Bluesky Social](https://bsky.app/) API.
It's focused largely on collecting tidy data from Bluesky.
Given the decentralized nature of Bluesky, it seems like it has immense opportunity for social science research.
You can design entire feeds as treatments, letting people push further in treatment arms without the same need for industry-academy partnerships as with Facebook or X/Twitter.

Of course, it also contains all of the tools for posting and otherwise interacting with Bluesky.
I even have a small bot going which tracks [CRAN Updates](https://bsky.app/profile/did:plc:qnzaaisqa3lafzmtjxt5nt52) that I'll cover soon in a holiday-times blog post.
It's run entirely through `bskyr`, which has been working even better than expected.


## Updates to existing packages

My CRAN updates have not all been *new* packages, I also maintain a handful of packages.
This year, I've made 14 submissions across 10 packages.
7 of these are related to the 5 new packages above (5 first submission + 2 updates).

```{r}
hists |> 
  filter(year == 2023) |> 
  filter(!Package %in% c('apportion', 'crayons', 'gptzeror', 'feltr', 'bskyr')) |> 
  select(Name = Package, Version, Date = date2, Title) |> 
  arrange(Date) |> 
  gt()
```

Updates to [`cvap`](https://christophertkenny.com/cvap/news/index.html) and [`tinytiger`](https://alarm-redist.org/tinytiger/news/index.html) added support for new years of Census Bureau data.
[`redist`](https://alarm-redist.org/redist/news/index.html) and [`redistmetrics`](https://alarm-redist.org/redistmetrics/news/index.html) each 
saw primarily bug fixes and performance improvements, without any major changes.
[`geomander`](https://christophertkenny.com/geomander/news/index.html) similar saw mostly bug fixes.
Its one update also drastically cleaned up the dependencies to make the package easier to install.


# Package Downloads

By downloads, my most popular package was [`tinytiger`](https://alarm-redist.org/tinytiger/), a project with [Cory McCartan](https://corymccartan.com/).
This was designed as a low dependency alternative to `tigris` for accessing Census data, primarily as a dependency for other `redist`verse packages.

[`congress`](https://christophertkenny.com/congress/) surprisingly comes in at number 2.
I didn't realize people were actually using this, but glad to see it.
`congress` provides a tidy interface to Congress.gov's API.
You can get bill text, member info, committee documents, and more.

[`redist`](https://alarm-redist.org/redist/) had a good year, taking the bronze medal.
After being used for the winning side in [`Allen v. Milligan`](https://www.supremecourt.gov/opinions/22pdf/21-1086_1co6.pdf), it has another chance to make a difference in the [ongoing South Carolina redistricting case](https://www.oyez.org/cases/2023/22-807).

The rest of the download counts are below, with `bskyr` coming in last with a mere 509 downloads.

```{r} 
#| label: ranks

make_url <- function(x, y) {
  paste0('<a href="', pkgs %>% filter(logo %in% x) %>% slice(match(logo, x)) %>% pull(link), '">', y, '</a>')
}

counts_lst_yr  |> 
  select(logo, info, count) |>
  mutate(rank = row_number(), .before = everything()) |> 
  gt(id = 'pkgs') |> 
  text_transform(
    locations = cells_body(columns = logo),
    fn = function(x) {
      make_url(x, y = 
                 local_image(
                   filename = x,
                   height = 100
                 )
      )
    }
  ) |> 
  fmt_number(columns = count, decimals = 0) |> 
  cols_width(
    rank ~ px(100),
    logo ~ px(100),
    count ~ px(150),
  ) |> 
  cols_label(
    rank = 'Rank',
    logo = '',
    info = 'Package',
    count = 'Downloads'
  ) |> 
  opt_interactive()
```

We can also look at their trajectories over time. 
Below are the cumulative downloads for each package over the last year by day.

```{r}
#| label: "daily"
hj <- tribble(
  ~name, ~hj,
         "redist", 1.0,
  "redistmetrics", 1.0,
      "geomander", 1.0,
        "PL94171", 1.0,
       "censable", 0.85,
      "tinytiger", 1.0,
           "dots", 1.0,
           "cvap", 1.0,
           "ppmf", 0.85,
         "divseg", 1.0,
           "name", 0.8,
            "jot", 0.75,
       "ggredist", 1.0,
       "congress", 0.95,
          "feltr", 1.0,
        "crayons", 1.0,
      "apportion", 0.9,
       "gptzeror", 0.7,
          "bskyr", 1.0,
) |> 
  mutate(hj = hj - 0.03)
counts_daily |> 
  left_join(pkgs, by = 'name') |> 
  group_by(name) |> 
  arrange(date) |> 
  mutate(
    cumulative_count = cumsum(count),
    pkg_max = max(cumulative_count)
  ) |> 
  ungroup() |> 
  left_join(first_release, by = 'name') |>
  filter(date >= first_release) |>
  left_join(hj, by = 'name') |>
  ggplot() +
  geom_textpath(aes(x = date, y = cumulative_count, label = name, hjust = hj, color = color)) + 
  scale_x_date(name = 'Date', date_breaks = '2 month', date_labels = '%b %Y') +
  scale_y_continuous(name = 'Count', labels = scales::label_comma(), breaks = seq(0, 6000, by = 1000)) +
  scale_color_identity() +
  theme_bw()
```

Similar to Spotify's Wrapped, this only counts the number of packages downloaded in the past year.
It's interesting to see the count, but some of the ordering is time dependent.
For example, `bskyr` really never had a chance, only being up for about a month.
`feltr`, despite being on CRAN only since the summer, passed several packages.
We can do some back of the envelope calculations to see what those packages might have done given the same 355 days as the other packages.^[Dear Spotify, please do something like this next year to account late releases like [1989 in October.](https://en.wikipedia.org/wiki/1989_(Taylor's_Version))]

```{r}
counts_daily |> 
  left_join(first_release, by = 'name') |> 
  filter(
    name %in% c('apportion', 'crayons', 'gptzeror', 'feltr', 'bskyr'),
    date >= first_release
  ) |>
  group_by(name) |>
  summarize(
        days = n(),
    downloads = sum(count)
  ) |>
  mutate(
    downloads_adjusted = downloads * (355 / days)
  ) |> 
  arrange(desc(downloads_adjusted)) |>
  gt() |> 
  cols_label(
    name = 'Package',
    days = 'Days',
    downloads = 'Raw',
    downloads_adjusted = 'Adjusted'
  ) |> 
  tab_spanner(
    label = 'Downloads',
    columns = c(downloads, downloads_adjusted)
  ) |> 
  fmt_number(
    columns = c(downloads, downloads_adjusted),
    decimals = 0
  )
```

With these, we can see that `feltr` and `gptzeror` would have been in the running for the top few packages.


# Packages goals for 2024

The first package I contributed to was [`redist`](https://alarm-redist.org/redist/), some years ago, under the watchful eyes of [Ben Fifield](https://www.benfifield.com/). 
At that time, `redist` was a massive package, pushing the boundaries of what CRAN would accept, for size, compilation time, and number of direct dependencies.
It handled everything all at once, from the algorithms to organizing outputs.
After expanding some of the features in 2022, we had to split that out into `redistmetrics`.
That's grown further now, with `geomander` and `censable` to handle data inputs.
`ggredist` extends the visualization side of things.
Cory and I put together [`redistverse`](https://github.com/alarm-redist/redistverse) to tie all of these back together.
At some point this year, we should put that on CRAN.

I've also been organizing a package for unified reading of voter files, [`vf`](https://github.com/christopherkenny/vf).
The goals of that package are to (1) efficiently read voter files and (2) standardize column names across states.
A few states are done, but there's a lot more to go.
Once I have about 10 states covered, I'd like to put that on CRAN as well.^[Contributions welcome!]


Merry Christmas and a Happy New Year!

